Operation name: stablehlo.constant
Cost: 0

Operation name: stablehlo.constant
Cost: 0

Operation name: stablehlo.constant
Cost: 0

Operation name: stablehlo.constant
Cost: 0

Operation name: stablehlo.constant
Cost: 0

Operation name: stablehlo.constant
Cost: 0

Operation name: stablehlo.constant
Cost: 0

Operation name: stablehlo.constant
Cost: 0

Operation name: stablehlo.constant
Cost: 0

Operation name: stablehlo.constant
Cost: 0

Operation name: stablehlo.slice
cache miss
2887246636
%0 = stablehlo.slice %arg1 [0:1, 0:288] : (tensor<6x288xf32>) -> tensor<1x288xf32>
Cost: 619

Operation name: stablehlo.reshape
cache miss
1377738304
%1 = stablehlo.reshape %0 : (tensor<1x288xf32>) -> tensor<288xf32>
Cost: 345

Operation name: stablehlo.dot_general
cache miss
2778462547
%2 = stablehlo.dot_general %arg0, %arg0, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288xf32>) -> tensor<f32>
Cost: 387

Operation name: stablehlo.divide
cache miss
719102371
%3 = stablehlo.divide %2, %cst_5 : tensor<f32>
Cost: 279

Operation name: stablehlo.add
cache miss
1937139428
%4 = stablehlo.add %3, %cst_4 : tensor<f32>
Cost: 282

Operation name: stablehlo.sqrt
cache miss
1884932271
%5 = stablehlo.sqrt %4 : tensor<f32>
Cost: 278

Operation name: stablehlo.divide
cache miss
719102371
%6 = stablehlo.divide %cst_6, %5 : tensor<f32>
Cost: 377

Operation name: stablehlo.divide
cache miss
719102371
%7 = stablehlo.divide %cst_7, %5 : tensor<f32>
Cost: 273

Operation name: stablehlo.multiply
cache miss
1284973678
%8 = stablehlo.multiply %5, %5 : tensor<f32>
Cost: 172

Operation name: stablehlo.divide
cache miss
719102371
%9 = stablehlo.divide %cst_7, %8 : tensor<f32>
Cost: 377

Operation name: stablehlo.multiply
cache miss
2511610444
%10 = stablehlo.multiply %1, %arg0 : tensor<288xf32>
Cost: 484

Operation name: stablehlo.broadcast_in_dim
cache miss
2616490109
%11 = stablehlo.broadcast_in_dim %7, dims = [] : (tensor<f32>) -> tensor<288xf32>
Cost: 344

Operation name: stablehlo.multiply
cache miss
2511610444
%12 = stablehlo.multiply %10, %11 : tensor<288xf32>
Cost: 215

Operation name: stablehlo.slice
cache miss
1602837588
%13 = stablehlo.slice %arg9 [0:1, 0:288, 0:288] : (tensor<6x288x288xf32>) -> tensor<1x288x288xf32>
Cost: 36475

Operation name: stablehlo.reshape
cache miss
3219454195
%14 = stablehlo.reshape %13 : (tensor<1x288x288xf32>) -> tensor<288x288xf32>
Cost: 46273

Operation name: stablehlo.dot_general
cache miss
1061109412
%15 = stablehlo.dot_general %14, %12, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288x288xf32>, tensor<288xf32>) -> tensor<288xf32>
Cost: 436

Operation name: stablehlo.slice
cache miss
1602837588
%16 = stablehlo.slice %arg7 [0:1, 0:288, 0:288] : (tensor<6x288x288xf32>) -> tensor<1x288x288xf32>
Cost: 63631

Operation name: stablehlo.reshape
cache miss
3219454195
%17 = stablehlo.reshape %16 : (tensor<1x288x288xf32>) -> tensor<288x288xf32>
Cost: 42731

Operation name: stablehlo.dot_general
cache miss
1061109412
%18 = stablehlo.dot_general %17, %12, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288x288xf32>, tensor<288xf32>) -> tensor<288xf32>
Cost: 323

Operation name: stablehlo.slice
cache miss
1602837588
%19 = stablehlo.slice %arg10 [0:1, 0:288, 0:288] : (tensor<6x288x288xf32>) -> tensor<1x288x288xf32>
Cost: 21736

Operation name: stablehlo.reshape
cache miss
3219454195
%20 = stablehlo.reshape %19 : (tensor<1x288x288xf32>) -> tensor<288x288xf32>
Cost: 18491

Operation name: stablehlo.dot_general
cache miss
1061109412
%21 = stablehlo.dot_general %20, %12, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288x288xf32>, tensor<288xf32>) -> tensor<288xf32>
Cost: 212

Operation name: stablehlo.reshape
cache miss
2392077026
%22 = stablehlo.reshape %15 : (tensor<288xf32>) -> tensor<144x2xf32>
Cost: 437

Operation name: stablehlo.reshape
cache miss
2392077026
%23 = stablehlo.reshape %18 : (tensor<288xf32>) -> tensor<144x2xf32>
Cost: 340

Operation name: stablehlo.dot_general
cache miss
865228849
%24 = stablehlo.dot_general %cst_8, %23, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<144x2x2xf32>, tensor<144x2xf32>) -> tensor<144x2xf32>
Cost: 560

Operation name: stablehlo.dot_general
cache miss
865228849
%25 = stablehlo.dot_general %cst_8, %22, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<144x2x2xf32>, tensor<144x2xf32>) -> tensor<144x2xf32>
Cost: 557

Operation name: stablehlo.reshape
cache miss
1377738304
%26 = stablehlo.reshape %25 : (tensor<144x2xf32>) -> tensor<288xf32>
Cost: 212

Operation name: stablehlo.slice
cache miss
4163773663
%27 = stablehlo.slice %arg11 [0:1, 0:1, 0:288] : (tensor<6x1x288xf32>) -> tensor<1x1x288xf32>
Cost: 353

Operation name: stablehlo.reshape
cache miss
2960881163
%28 = stablehlo.reshape %27 : (tensor<1x1x288xf32>) -> tensor<1x288xf32>
Cost: 546

Operation name: stablehlo.reshape
cache miss
2960881163
%29 = stablehlo.reshape %24 : (tensor<144x2xf32>) -> tensor<1x288xf32>
Cost: 344

Operation name: stablehlo.slice
cache miss
4163773663
%30 = stablehlo.slice %arg12 [0:1, 0:1, 0:288] : (tensor<6x1x288xf32>) -> tensor<1x1x288xf32>
Cost: 467

Operation name: stablehlo.reshape
cache miss
2960881163
%31 = stablehlo.reshape %30 : (tensor<1x1x288xf32>) -> tensor<1x288xf32>
Cost: 348

Operation name: stablehlo.reshape
cache miss
2960881163
%32 = stablehlo.reshape %21 : (tensor<288xf32>) -> tensor<1x288xf32>
Cost: 469

Operation name: stablehlo.slice
cache miss
2839717066
%33 = stablehlo.slice %26 [0:48] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 369

Operation name: stablehlo.slice
cache miss
3729170049
%34 = stablehlo.slice %28 [0:1, 0:48] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 174

Operation name: stablehlo.slice
cache miss
3729170049
%35 = stablehlo.slice %29 [0:1, 0:48] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 274

Operation name: stablehlo.concatenate
cache miss
895440948
%36 = stablehlo.concatenate %34, %35, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 297

Operation name: stablehlo.dot_general
cache miss
1097263507
%37 = stablehlo.dot_general %36, %33, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<48xf32>) -> tensor<2xf32>
Cost: 272

Operation name: stablehlo.divide
cache miss
1531341356
%38 = stablehlo.divide %37, %cst_1 : tensor<2xf32>
Cost: 285

Operation name: stablehlo.maximum
cache miss
641216813
%2733 = stablehlo.maximum %arg14, %arg15 : tensor<f32>
Cost: 282

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%39 = stablehlo.reduce(%38 init: %cst_3) applies stablehlo.maximum across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 281

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%40 = stablehlo.broadcast_in_dim %39, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 284

Operation name: stablehlo.compare
Cost: 0

Operation name: stablehlo.convert
cache miss
1629337212
%42 = stablehlo.convert %41 : (tensor<2xi1>) -> tensor<2xf32>
Cost: 372

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 393

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%43 = stablehlo.reduce(%42 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 283

Operation name: stablehlo.subtract
cache miss
3818527485
%44 = stablehlo.subtract %38, %40 : tensor<2xf32>
Cost: 286

Operation name: stablehlo.exponential
cache miss
3699945286
%45 = stablehlo.exponential %44 : tensor<2xf32>
Cost: 452

Operation name: stablehlo.slice
cache miss
2078395662
%46 = stablehlo.slice %45 [0:1] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 280

Operation name: stablehlo.reshape
cache miss
1900484018
%47 = stablehlo.reshape %46 : (tensor<1xf32>) -> tensor<f32>
Cost: 394

Operation name: stablehlo.slice
cache miss
4032994822
%48 = stablehlo.slice %45 [1:2] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 252

Operation name: stablehlo.reshape
cache miss
1900484018
%49 = stablehlo.reshape %48 : (tensor<1xf32>) -> tensor<f32>
Cost: 277

Operation name: stablehlo.add
cache miss
1937139428
%50 = stablehlo.add %47, %49 : tensor<f32>
Cost: 247

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%51 = stablehlo.broadcast_in_dim %50, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 383

Operation name: stablehlo.divide
cache miss
1531341356
%52 = stablehlo.divide %45, %51 : tensor<2xf32>
Cost: 173

Operation name: stablehlo.multiply
cache miss
1284973678
%53 = stablehlo.multiply %50, %50 : tensor<f32>
Cost: 172

Operation name: stablehlo.divide
cache miss
719102371
%54 = stablehlo.divide %cst_7, %53 : tensor<f32>
Cost: 213

Operation name: stablehlo.slice
cache miss
3729170049
%55 = stablehlo.slice %31 [0:1, 0:48] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 300

Operation name: stablehlo.slice
cache miss
3729170049
%56 = stablehlo.slice %32 [0:1, 0:48] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 173

Operation name: stablehlo.concatenate
cache miss
895440948
%57 = stablehlo.concatenate %55, %56, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 374

Operation name: stablehlo.dot_general
cache miss
2902940993
%58 = stablehlo.dot_general %57, %52, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<2xf32>) -> tensor<48xf32>
Cost: 351

Operation name: stablehlo.slice
cache miss
4032389970
%59 = stablehlo.slice %26 [48:96] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 176

Operation name: stablehlo.slice
cache miss
3581210888
%60 = stablehlo.slice %28 [0:1, 48:96] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 174

Operation name: stablehlo.slice
cache miss
3581210888
%61 = stablehlo.slice %29 [0:1, 48:96] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 372

Operation name: stablehlo.concatenate
cache miss
895440948
%62 = stablehlo.concatenate %60, %61, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 372

Operation name: stablehlo.dot_general
cache miss
1097263507
%63 = stablehlo.dot_general %62, %59, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<48xf32>) -> tensor<2xf32>
Cost: 277

Operation name: stablehlo.divide
cache miss
1531341356
%64 = stablehlo.divide %63, %cst_1 : tensor<2xf32>
Cost: 369

Operation name: stablehlo.maximum
cache miss
641216813
%2733 = stablehlo.maximum %arg14, %arg15 : tensor<f32>
Cost: 275

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%65 = stablehlo.reduce(%64 init: %cst_3) applies stablehlo.maximum across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 277

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%66 = stablehlo.broadcast_in_dim %65, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 173

Operation name: stablehlo.compare
Cost: 0

Operation name: stablehlo.convert
cache miss
1629337212
%68 = stablehlo.convert %67 : (tensor<2xi1>) -> tensor<2xf32>
Cost: 170

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 393

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%69 = stablehlo.reduce(%68 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 272

Operation name: stablehlo.subtract
cache miss
3818527485
%70 = stablehlo.subtract %64, %66 : tensor<2xf32>
Cost: 274

Operation name: stablehlo.exponential
cache miss
3699945286
%71 = stablehlo.exponential %70 : tensor<2xf32>
Cost: 274

Operation name: stablehlo.slice
cache miss
2078395662
%72 = stablehlo.slice %71 [0:1] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 364

Operation name: stablehlo.reshape
cache miss
1900484018
%73 = stablehlo.reshape %72 : (tensor<1xf32>) -> tensor<f32>
Cost: 305

Operation name: stablehlo.slice
cache miss
4032994822
%74 = stablehlo.slice %71 [1:2] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 300

Operation name: stablehlo.reshape
cache miss
1900484018
%75 = stablehlo.reshape %74 : (tensor<1xf32>) -> tensor<f32>
Cost: 301

Operation name: stablehlo.add
cache miss
1937139428
%76 = stablehlo.add %73, %75 : tensor<f32>
Cost: 170

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%77 = stablehlo.broadcast_in_dim %76, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 394

Operation name: stablehlo.divide
cache miss
1531341356
%78 = stablehlo.divide %71, %77 : tensor<2xf32>
Cost: 173

Operation name: stablehlo.multiply
cache miss
1284973678
%79 = stablehlo.multiply %76, %76 : tensor<f32>
Cost: 459

Operation name: stablehlo.divide
cache miss
719102371
%80 = stablehlo.divide %cst_7, %79 : tensor<f32>
Cost: 386

Operation name: stablehlo.slice
cache miss
3581210888
%81 = stablehlo.slice %31 [0:1, 48:96] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 391

Operation name: stablehlo.slice
cache miss
3581210888
%82 = stablehlo.slice %32 [0:1, 48:96] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 387

Operation name: stablehlo.concatenate
cache miss
895440948
%83 = stablehlo.concatenate %81, %82, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 378

Operation name: stablehlo.dot_general
cache miss
2902940993
%84 = stablehlo.dot_general %83, %78, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<2xf32>) -> tensor<48xf32>
Cost: 266

Operation name: stablehlo.slice
cache miss
1375222232
%85 = stablehlo.slice %26 [96:144] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 483

Operation name: stablehlo.slice
cache miss
1957506058
%86 = stablehlo.slice %28 [0:1, 96:144] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 541

Operation name: stablehlo.slice
cache miss
1957506058
%87 = stablehlo.slice %29 [0:1, 96:144] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 309

Operation name: stablehlo.concatenate
cache miss
895440948
%88 = stablehlo.concatenate %86, %87, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 393

Operation name: stablehlo.dot_general
cache miss
1097263507
%89 = stablehlo.dot_general %88, %85, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<48xf32>) -> tensor<2xf32>
Cost: 276

Operation name: stablehlo.divide
cache miss
1531341356
%90 = stablehlo.divide %89, %cst_1 : tensor<2xf32>
Cost: 332

Operation name: stablehlo.maximum
cache miss
641216813
%2733 = stablehlo.maximum %arg14, %arg15 : tensor<f32>
Cost: 278

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%91 = stablehlo.reduce(%90 init: %cst_3) applies stablehlo.maximum across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 271

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%92 = stablehlo.broadcast_in_dim %91, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 602

Operation name: stablehlo.compare
Cost: 0

Operation name: stablehlo.convert
cache miss
1629337212
%94 = stablehlo.convert %93 : (tensor<2xi1>) -> tensor<2xf32>
Cost: 269

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 277

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%95 = stablehlo.reduce(%94 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 364

Operation name: stablehlo.subtract
cache miss
3818527485
%96 = stablehlo.subtract %90, %92 : tensor<2xf32>
Cost: 368

Operation name: stablehlo.exponential
cache miss
3699945286
%97 = stablehlo.exponential %96 : tensor<2xf32>
Cost: 273

Operation name: stablehlo.slice
cache miss
2078395662
%98 = stablehlo.slice %97 [0:1] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 360

Operation name: stablehlo.reshape
cache miss
1900484018
%99 = stablehlo.reshape %98 : (tensor<1xf32>) -> tensor<f32>
Cost: 274

Operation name: stablehlo.slice
cache miss
4032994822
%100 = stablehlo.slice %97 [1:2] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 364

Operation name: stablehlo.reshape
cache miss
1900484018
%101 = stablehlo.reshape %100 : (tensor<1xf32>) -> tensor<f32>
Cost: 383

Operation name: stablehlo.add
cache miss
1937139428
%102 = stablehlo.add %99, %101 : tensor<f32>
Cost: 171

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%103 = stablehlo.broadcast_in_dim %102, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 395

Operation name: stablehlo.divide
cache miss
1531341356
%104 = stablehlo.divide %97, %103 : tensor<2xf32>
Cost: 363

Operation name: stablehlo.multiply
cache miss
1284973678
%105 = stablehlo.multiply %102, %102 : tensor<f32>
Cost: 272

Operation name: stablehlo.divide
cache miss
719102371
%106 = stablehlo.divide %cst_7, %105 : tensor<f32>
Cost: 272

Operation name: stablehlo.slice
cache miss
1957506058
%107 = stablehlo.slice %31 [0:1, 96:144] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 175

Operation name: stablehlo.slice
cache miss
1957506058
%108 = stablehlo.slice %32 [0:1, 96:144] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 175

Operation name: stablehlo.concatenate
cache miss
895440948
%109 = stablehlo.concatenate %107, %108, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 372

Operation name: stablehlo.dot_general
cache miss
2902940993
%110 = stablehlo.dot_general %109, %104, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<2xf32>) -> tensor<48xf32>
Cost: 264

Operation name: stablehlo.slice
cache miss
3469783879
%111 = stablehlo.slice %26 [144:192] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 370

Operation name: stablehlo.slice
cache miss
3339692416
%112 = stablehlo.slice %28 [0:1, 144:192] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 277

Operation name: stablehlo.slice
cache miss
3339692416
%113 = stablehlo.slice %29 [0:1, 144:192] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 379

Operation name: stablehlo.concatenate
cache miss
895440948
%114 = stablehlo.concatenate %112, %113, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 399

Operation name: stablehlo.dot_general
cache miss
1097263507
%115 = stablehlo.dot_general %114, %111, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<48xf32>) -> tensor<2xf32>
Cost: 279

Operation name: stablehlo.divide
cache miss
1531341356
%116 = stablehlo.divide %115, %cst_1 : tensor<2xf32>
Cost: 285

Operation name: stablehlo.maximum
cache miss
641216813
%2733 = stablehlo.maximum %arg14, %arg15 : tensor<f32>
Cost: 370

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%117 = stablehlo.reduce(%116 init: %cst_3) applies stablehlo.maximum across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 275

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%118 = stablehlo.broadcast_in_dim %117, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 451

Operation name: stablehlo.compare
Cost: 0

Operation name: stablehlo.convert
cache miss
1629337212
%120 = stablehlo.convert %119 : (tensor<2xi1>) -> tensor<2xf32>
Cost: 364

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 275

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%121 = stablehlo.reduce(%120 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 400

Operation name: stablehlo.subtract
cache miss
3818527485
%122 = stablehlo.subtract %116, %118 : tensor<2xf32>
Cost: 410

Operation name: stablehlo.exponential
cache miss
3699945286
%123 = stablehlo.exponential %122 : tensor<2xf32>
Cost: 548

Operation name: stablehlo.slice
cache miss
2078395662
%124 = stablehlo.slice %123 [0:1] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 364

Operation name: stablehlo.reshape
cache miss
1900484018
%125 = stablehlo.reshape %124 : (tensor<1xf32>) -> tensor<f32>
Cost: 464

Operation name: stablehlo.slice
cache miss
4032994822
%126 = stablehlo.slice %123 [1:2] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 277

Operation name: stablehlo.reshape
cache miss
1900484018
%127 = stablehlo.reshape %126 : (tensor<1xf32>) -> tensor<f32>
Cost: 417

Operation name: stablehlo.add
cache miss
1937139428
%128 = stablehlo.add %125, %127 : tensor<f32>
Cost: 350

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%129 = stablehlo.broadcast_in_dim %128, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 406

Operation name: stablehlo.divide
cache miss
1531341356
%130 = stablehlo.divide %123, %129 : tensor<2xf32>
Cost: 361

Operation name: stablehlo.multiply
cache miss
1284973678
%131 = stablehlo.multiply %128, %128 : tensor<f32>
Cost: 397

Operation name: stablehlo.divide
cache miss
719102371
%132 = stablehlo.divide %cst_7, %131 : tensor<f32>
Cost: 273

Operation name: stablehlo.slice
cache miss
3339692416
%133 = stablehlo.slice %31 [0:1, 144:192] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 364

Operation name: stablehlo.slice
cache miss
3339692416
%134 = stablehlo.slice %32 [0:1, 144:192] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 578

Operation name: stablehlo.concatenate
cache miss
895440948
%135 = stablehlo.concatenate %133, %134, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 371

Operation name: stablehlo.dot_general
cache miss
2902940993
%136 = stablehlo.dot_general %135, %130, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<2xf32>) -> tensor<48xf32>
Cost: 167

Operation name: stablehlo.slice
cache miss
1801550340
%137 = stablehlo.slice %26 [192:240] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 274

Operation name: stablehlo.slice
cache miss
3411559083
%138 = stablehlo.slice %28 [0:1, 192:240] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 389

Operation name: stablehlo.slice
cache miss
3411559083
%139 = stablehlo.slice %29 [0:1, 192:240] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 302

Operation name: stablehlo.concatenate
cache miss
895440948
%140 = stablehlo.concatenate %138, %139, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 322

Operation name: stablehlo.dot_general
cache miss
1097263507
%141 = stablehlo.dot_general %140, %137, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<48xf32>) -> tensor<2xf32>
Cost: 392

Operation name: stablehlo.divide
cache miss
1531341356
%142 = stablehlo.divide %141, %cst_1 : tensor<2xf32>
Cost: 329

Operation name: stablehlo.maximum
cache miss
641216813
%2733 = stablehlo.maximum %arg14, %arg15 : tensor<f32>
Cost: 277

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%143 = stablehlo.reduce(%142 init: %cst_3) applies stablehlo.maximum across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 475

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%144 = stablehlo.broadcast_in_dim %143, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 358

Operation name: stablehlo.compare
Cost: 0

Operation name: stablehlo.convert
cache miss
1629337212
%146 = stablehlo.convert %145 : (tensor<2xi1>) -> tensor<2xf32>
Cost: 286

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 272

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%147 = stablehlo.reduce(%146 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 173

Operation name: stablehlo.subtract
cache miss
3818527485
%148 = stablehlo.subtract %142, %144 : tensor<2xf32>
Cost: 359

Operation name: stablehlo.exponential
cache miss
3699945286
%149 = stablehlo.exponential %148 : tensor<2xf32>
Cost: 351

Operation name: stablehlo.slice
cache miss
2078395662
%150 = stablehlo.slice %149 [0:1] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 483

Operation name: stablehlo.reshape
cache miss
1900484018
%151 = stablehlo.reshape %150 : (tensor<1xf32>) -> tensor<f32>
Cost: 172

Operation name: stablehlo.slice
cache miss
4032994822
%152 = stablehlo.slice %149 [1:2] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 188

Operation name: stablehlo.reshape
cache miss
1900484018
%153 = stablehlo.reshape %152 : (tensor<1xf32>) -> tensor<f32>
Cost: 370

Operation name: stablehlo.add
cache miss
1937139428
%154 = stablehlo.add %151, %153 : tensor<f32>
Cost: 298

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%155 = stablehlo.broadcast_in_dim %154, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 276

Operation name: stablehlo.divide
cache miss
1531341356
%156 = stablehlo.divide %149, %155 : tensor<2xf32>
Cost: 299

Operation name: stablehlo.multiply
cache miss
1284973678
%157 = stablehlo.multiply %154, %154 : tensor<f32>
Cost: 277

Operation name: stablehlo.divide
cache miss
719102371
%158 = stablehlo.divide %cst_7, %157 : tensor<f32>
Cost: 301

Operation name: stablehlo.slice
cache miss
3411559083
%159 = stablehlo.slice %31 [0:1, 192:240] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 176

Operation name: stablehlo.slice
cache miss
3411559083
%160 = stablehlo.slice %32 [0:1, 192:240] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 173

Operation name: stablehlo.concatenate
cache miss
895440948
%161 = stablehlo.concatenate %159, %160, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 176

Operation name: stablehlo.dot_general
cache miss
2902940993
%162 = stablehlo.dot_general %161, %156, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<2xf32>) -> tensor<48xf32>
Cost: 166

Operation name: stablehlo.slice
cache miss
2798019243
%163 = stablehlo.slice %26 [240:288] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 180

Operation name: stablehlo.slice
cache miss
1619767244
%164 = stablehlo.slice %28 [0:1, 240:288] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 305

Operation name: stablehlo.slice
cache miss
1619767244
%165 = stablehlo.slice %29 [0:1, 240:288] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 274

Operation name: stablehlo.concatenate
cache miss
895440948
%166 = stablehlo.concatenate %164, %165, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 177

Operation name: stablehlo.dot_general
cache miss
1097263507
%167 = stablehlo.dot_general %166, %163, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<48xf32>) -> tensor<2xf32>
Cost: 174

Operation name: stablehlo.divide
cache miss
1531341356
%168 = stablehlo.divide %167, %cst_1 : tensor<2xf32>
Cost: 174

Operation name: stablehlo.maximum
cache miss
641216813
%2733 = stablehlo.maximum %arg14, %arg15 : tensor<f32>
Cost: 187

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%169 = stablehlo.reduce(%168 init: %cst_3) applies stablehlo.maximum across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 273

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%170 = stablehlo.broadcast_in_dim %169, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 276

Operation name: stablehlo.compare
Cost: 0

Operation name: stablehlo.convert
cache miss
1629337212
%172 = stablehlo.convert %171 : (tensor<2xi1>) -> tensor<2xf32>
Cost: 170

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 174

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%173 = stablehlo.reduce(%172 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 174

Operation name: stablehlo.subtract
cache miss
3818527485
%174 = stablehlo.subtract %168, %170 : tensor<2xf32>
Cost: 384

Operation name: stablehlo.exponential
cache miss
3699945286
%175 = stablehlo.exponential %174 : tensor<2xf32>
Cost: 369

Operation name: stablehlo.slice
cache miss
2078395662
%176 = stablehlo.slice %175 [0:1] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 386

Operation name: stablehlo.reshape
cache miss
1900484018
%177 = stablehlo.reshape %176 : (tensor<1xf32>) -> tensor<f32>
Cost: 276

Operation name: stablehlo.slice
cache miss
4032994822
%178 = stablehlo.slice %175 [1:2] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 243

Operation name: stablehlo.reshape
cache miss
1900484018
%179 = stablehlo.reshape %178 : (tensor<1xf32>) -> tensor<f32>
Cost: 277

Operation name: stablehlo.add
cache miss
1937139428
%180 = stablehlo.add %177, %179 : tensor<f32>
Cost: 273

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%181 = stablehlo.broadcast_in_dim %180, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 270

Operation name: stablehlo.divide
cache miss
1531341356
%182 = stablehlo.divide %175, %181 : tensor<2xf32>
Cost: 373

Operation name: stablehlo.multiply
cache miss
1284973678
%183 = stablehlo.multiply %180, %180 : tensor<f32>
Cost: 367

Operation name: stablehlo.divide
cache miss
719102371
%184 = stablehlo.divide %cst_7, %183 : tensor<f32>
Cost: 384

Operation name: stablehlo.slice
cache miss
1619767244
%185 = stablehlo.slice %31 [0:1, 240:288] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 393

Operation name: stablehlo.slice
cache miss
1619767244
%186 = stablehlo.slice %32 [0:1, 240:288] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 173

Operation name: stablehlo.concatenate
cache miss
895440948
%187 = stablehlo.concatenate %185, %186, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 371

Operation name: stablehlo.dot_general
cache miss
2902940993
%188 = stablehlo.dot_general %187, %182, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<2xf32>) -> tensor<48xf32>
Cost: 360

Operation name: stablehlo.concatenate
cache miss
292037604
%189 = stablehlo.concatenate %58, %84, %110, %136, %162, %188, dim = 0 : (tensor<48xf32>, tensor<48xf32>, tensor<48xf32>, tensor<48xf32>, tensor<48xf32>, tensor<48xf32>) -> tensor<288xf32>
Cost: 254

Operation name: stablehlo.slice
cache miss
1602837588
%190 = stablehlo.slice %arg8 [0:1, 0:288, 0:288] : (tensor<6x288x288xf32>) -> tensor<1x288x288xf32>
Cost: 37512

Operation name: stablehlo.reshape
cache miss
3219454195
%191 = stablehlo.reshape %190 : (tensor<1x288x288xf32>) -> tensor<288x288xf32>
Cost: 55159

Operation name: stablehlo.dot_general
cache miss
1061109412
%192 = stablehlo.dot_general %191, %189, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288x288xf32>, tensor<288xf32>) -> tensor<288xf32>
Cost: 450

Operation name: stablehlo.add
cache miss
3847999983
%193 = stablehlo.add %arg0, %192 : tensor<288xf32>
Cost: 507

Operation name: stablehlo.slice
cache miss
2887246636
%194 = stablehlo.slice %arg2 [0:1, 0:288] : (tensor<6x288xf32>) -> tensor<1x288xf32>
Cost: 386

Operation name: stablehlo.reshape
cache miss
1377738304
%195 = stablehlo.reshape %194 : (tensor<1x288xf32>) -> tensor<288xf32>
Cost: 387

Operation name: stablehlo.dot_general
cache miss
2778462547
%196 = stablehlo.dot_general %193, %193, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288xf32>) -> tensor<f32>
Cost: 272

Operation name: stablehlo.divide
cache miss
719102371
%197 = stablehlo.divide %196, %cst_5 : tensor<f32>
Cost: 372

Operation name: stablehlo.add
cache miss
1937139428
%198 = stablehlo.add %197, %cst_4 : tensor<f32>
Cost: 280

Operation name: stablehlo.sqrt
cache miss
1884932271
%199 = stablehlo.sqrt %198 : tensor<f32>
Cost: 287

Operation name: stablehlo.divide
cache miss
719102371
%200 = stablehlo.divide %cst_6, %199 : tensor<f32>
Cost: 322

Operation name: stablehlo.divide
cache miss
719102371
%201 = stablehlo.divide %cst_7, %199 : tensor<f32>
Cost: 376

Operation name: stablehlo.multiply
cache miss
1284973678
%202 = stablehlo.multiply %199, %199 : tensor<f32>
Cost: 367

Operation name: stablehlo.divide
cache miss
719102371
%203 = stablehlo.divide %cst_7, %202 : tensor<f32>
Cost: 273

Operation name: stablehlo.multiply
cache miss
2511610444
%204 = stablehlo.multiply %195, %193 : tensor<288xf32>
Cost: 505

Operation name: stablehlo.broadcast_in_dim
cache miss
2616490109
%205 = stablehlo.broadcast_in_dim %201, dims = [] : (tensor<f32>) -> tensor<288xf32>
Cost: 391

Operation name: stablehlo.multiply
cache miss
2511610444
%206 = stablehlo.multiply %204, %205 : tensor<288xf32>
Cost: 508

Operation name: stablehlo.slice
cache miss
726165347
%207 = stablehlo.slice %arg4 [0:1, 0:768, 0:288] : (tensor<6x768x288xf32>) -> tensor<1x768x288xf32>
Cost: 93856

Operation name: stablehlo.reshape
cache miss
3265769957
%208 = stablehlo.reshape %207 : (tensor<1x768x288xf32>) -> tensor<768x288xf32>
Cost: 108036

Operation name: stablehlo.dot_general
cache miss
2529236298
%209 = stablehlo.dot_general %208, %206, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<768x288xf32>, tensor<288xf32>) -> tensor<768xf32>
Cost: 3443

Operation name: stablehlo.slice
cache miss
726165347
%210 = stablehlo.slice %arg6 [0:1, 0:768, 0:288] : (tensor<6x768x288xf32>) -> tensor<1x768x288xf32>
Cost: 78689

Operation name: stablehlo.reshape
cache miss
3265769957
%211 = stablehlo.reshape %210 : (tensor<1x768x288xf32>) -> tensor<768x288xf32>
Cost: 94200

Operation name: stablehlo.dot_general
cache miss
2529236298
%212 = stablehlo.dot_general %211, %206, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<768x288xf32>, tensor<288xf32>) -> tensor<768xf32>
Cost: 3452

Operation name: stablehlo.negate
cache miss
4256221255
%213 = stablehlo.negate %209 : tensor<768xf32>
Cost: 278

Operation name: stablehlo.exponential
cache miss
6921514
%214 = stablehlo.exponential %213 : tensor<768xf32>
Cost: 459

Operation name: stablehlo.add
cache miss
389450582
%215 = stablehlo.add %cst_0, %214 : tensor<768xf32>
Cost: 414

Operation name: stablehlo.divide
cache miss
2134516102
%216 = stablehlo.divide %cst_0, %215 : tensor<768xf32>
Cost: 400

Operation name: stablehlo.multiply
cache miss
4137946040
%217 = stablehlo.multiply %215, %215 : tensor<768xf32>
Cost: 479

Operation name: stablehlo.divide
cache miss
2134516102
%218 = stablehlo.divide %cst_0, %217 : tensor<768xf32>
Cost: 404

Operation name: stablehlo.multiply
cache miss
4137946040
%219 = stablehlo.multiply %209, %216 : tensor<768xf32>
Cost: 446

Operation name: stablehlo.multiply
cache miss
4137946040
%220 = stablehlo.multiply %219, %212 : tensor<768xf32>
Cost: 593

Operation name: stablehlo.slice
cache miss
3777828501
%221 = stablehlo.slice %arg5 [0:1, 0:288, 0:768] : (tensor<6x288x768xf32>) -> tensor<1x288x768xf32>
Cost: 99507

Operation name: stablehlo.reshape
cache miss
2162077039
%222 = stablehlo.reshape %221 : (tensor<1x288x768xf32>) -> tensor<288x768xf32>
Cost: 104648

Operation name: stablehlo.dot_general
cache miss
1061109412
%223 = stablehlo.dot_general %222, %220, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288x768xf32>, tensor<768xf32>) -> tensor<288xf32>
Cost: 3882

Operation name: stablehlo.add
cache miss
3847999983
%224 = stablehlo.add %193, %223 : tensor<288xf32>
Cost: 386

Operation name: stablehlo.slice
cache miss
2525166936
%225 = stablehlo.slice %arg1 [1:2, 0:288] : (tensor<6x288xf32>) -> tensor<1x288xf32>
Cost: 406

Operation name: stablehlo.reshape
cache miss
1377738304
%226 = stablehlo.reshape %225 : (tensor<1x288xf32>) -> tensor<288xf32>
Cost: 383

Operation name: stablehlo.dot_general
cache miss
2778462547
%227 = stablehlo.dot_general %224, %224, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288xf32>) -> tensor<f32>
Cost: 275

Operation name: stablehlo.divide
cache miss
719102371
%228 = stablehlo.divide %227, %cst_5 : tensor<f32>
Cost: 313

Operation name: stablehlo.add
cache miss
1937139428
%229 = stablehlo.add %228, %cst_4 : tensor<f32>
Cost: 279

Operation name: stablehlo.sqrt
cache miss
1884932271
%230 = stablehlo.sqrt %229 : tensor<f32>
Cost: 314

Operation name: stablehlo.divide
cache miss
719102371
%231 = stablehlo.divide %cst_6, %230 : tensor<f32>
Cost: 312

Operation name: stablehlo.divide
cache miss
719102371
%232 = stablehlo.divide %cst_7, %230 : tensor<f32>
Cost: 309

Operation name: stablehlo.multiply
cache miss
1284973678
%233 = stablehlo.multiply %230, %230 : tensor<f32>
Cost: 309

Operation name: stablehlo.divide
cache miss
719102371
%234 = stablehlo.divide %cst_7, %233 : tensor<f32>
Cost: 286

Operation name: stablehlo.multiply
cache miss
2511610444
%235 = stablehlo.multiply %226, %224 : tensor<288xf32>
Cost: 425

Operation name: stablehlo.broadcast_in_dim
cache miss
2616490109
%236 = stablehlo.broadcast_in_dim %232, dims = [] : (tensor<f32>) -> tensor<288xf32>
Cost: 521

Operation name: stablehlo.multiply
cache miss
2511610444
%237 = stablehlo.multiply %235, %236 : tensor<288xf32>
Cost: 265

Operation name: stablehlo.slice
cache miss
3584813111
%238 = stablehlo.slice %arg9 [1:2, 0:288, 0:288] : (tensor<6x288x288xf32>) -> tensor<1x288x288xf32>
Cost: 40601

Operation name: stablehlo.reshape
cache miss
3219454195
%239 = stablehlo.reshape %238 : (tensor<1x288x288xf32>) -> tensor<288x288xf32>
Cost: 50977

Operation name: stablehlo.dot_general
cache miss
1061109412
%240 = stablehlo.dot_general %239, %237, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288x288xf32>, tensor<288xf32>) -> tensor<288xf32>
Cost: 220

Operation name: stablehlo.slice
cache miss
3584813111
%241 = stablehlo.slice %arg7 [1:2, 0:288, 0:288] : (tensor<6x288x288xf32>) -> tensor<1x288x288xf32>
Cost: 41286

Operation name: stablehlo.reshape
cache miss
3219454195
%242 = stablehlo.reshape %241 : (tensor<1x288x288xf32>) -> tensor<288x288xf32>
Cost: 44375

Operation name: stablehlo.dot_general
cache miss
1061109412
%243 = stablehlo.dot_general %242, %237, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288x288xf32>, tensor<288xf32>) -> tensor<288xf32>
Cost: 225

Operation name: stablehlo.slice
cache miss
3584813111
%244 = stablehlo.slice %arg10 [1:2, 0:288, 0:288] : (tensor<6x288x288xf32>) -> tensor<1x288x288xf32>
Cost: 66121

Operation name: stablehlo.reshape
cache miss
3219454195
%245 = stablehlo.reshape %244 : (tensor<1x288x288xf32>) -> tensor<288x288xf32>
Cost: 52084

Operation name: stablehlo.dot_general
cache miss
1061109412
%246 = stablehlo.dot_general %245, %237, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288x288xf32>, tensor<288xf32>) -> tensor<288xf32>
Cost: 341

Operation name: stablehlo.reshape
cache miss
2392077026
%247 = stablehlo.reshape %240 : (tensor<288xf32>) -> tensor<144x2xf32>
Cost: 516

Operation name: stablehlo.reshape
cache miss
2392077026
%248 = stablehlo.reshape %243 : (tensor<288xf32>) -> tensor<144x2xf32>
Cost: 540

Operation name: stablehlo.dot_general
cache miss
865228849
%249 = stablehlo.dot_general %cst_8, %248, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<144x2x2xf32>, tensor<144x2xf32>) -> tensor<144x2xf32>
Cost: 678

Operation name: stablehlo.dot_general
cache miss
865228849
%250 = stablehlo.dot_general %cst_8, %247, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<144x2x2xf32>, tensor<144x2xf32>) -> tensor<144x2xf32>
Cost: 400

Operation name: stablehlo.reshape
cache miss
1377738304
%251 = stablehlo.reshape %250 : (tensor<144x2xf32>) -> tensor<288xf32>
Cost: 303

Operation name: stablehlo.slice
cache miss
1889752609
%252 = stablehlo.slice %arg11 [1:2, 0:1, 0:288] : (tensor<6x1x288xf32>) -> tensor<1x1x288xf32>
Cost: 494

Operation name: stablehlo.reshape
cache miss
2960881163
%253 = stablehlo.reshape %252 : (tensor<1x1x288xf32>) -> tensor<1x288xf32>
Cost: 253

Operation name: stablehlo.reshape
cache miss
2960881163
%254 = stablehlo.reshape %249 : (tensor<144x2xf32>) -> tensor<1x288xf32>
Cost: 255

Operation name: stablehlo.slice
cache miss
1889752609
%255 = stablehlo.slice %arg12 [1:2, 0:1, 0:288] : (tensor<6x1x288xf32>) -> tensor<1x1x288xf32>
Cost: 518

Operation name: stablehlo.reshape
cache miss
2960881163
%256 = stablehlo.reshape %255 : (tensor<1x1x288xf32>) -> tensor<1x288xf32>
Cost: 517

Operation name: stablehlo.reshape
cache miss
2960881163
%257 = stablehlo.reshape %246 : (tensor<288xf32>) -> tensor<1x288xf32>
Cost: 515

Operation name: stablehlo.slice
cache miss
2839717066
%258 = stablehlo.slice %251 [0:48] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 390

Operation name: stablehlo.slice
cache miss
3729170049
%259 = stablehlo.slice %253 [0:1, 0:48] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 365

Operation name: stablehlo.slice
cache miss
3729170049
%260 = stablehlo.slice %254 [0:1, 0:48] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 417

Operation name: stablehlo.concatenate
cache miss
895440948
%261 = stablehlo.concatenate %259, %260, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 407

Operation name: stablehlo.dot_general
cache miss
1097263507
%262 = stablehlo.dot_general %261, %258, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<48xf32>) -> tensor<2xf32>
Cost: 327

Operation name: stablehlo.divide
cache miss
1531341356
%263 = stablehlo.divide %262, %cst_1 : tensor<2xf32>
Cost: 400

Operation name: stablehlo.maximum
cache miss
641216813
%2733 = stablehlo.maximum %arg14, %arg15 : tensor<f32>
Cost: 323

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%264 = stablehlo.reduce(%263 init: %cst_3) applies stablehlo.maximum across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 248

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%265 = stablehlo.broadcast_in_dim %264, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 297

Operation name: stablehlo.compare
Cost: 0

Operation name: stablehlo.convert
cache miss
1629337212
%267 = stablehlo.convert %266 : (tensor<2xi1>) -> tensor<2xf32>
Cost: 384

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 389

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%268 = stablehlo.reduce(%267 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 388

Operation name: stablehlo.subtract
cache miss
3818527485
%269 = stablehlo.subtract %263, %265 : tensor<2xf32>
Cost: 396

Operation name: stablehlo.exponential
cache miss
3699945286
%270 = stablehlo.exponential %269 : tensor<2xf32>
Cost: 374

Operation name: stablehlo.slice
cache miss
2078395662
%271 = stablehlo.slice %270 [0:1] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 305

Operation name: stablehlo.reshape
cache miss
1900484018
%272 = stablehlo.reshape %271 : (tensor<1xf32>) -> tensor<f32>
Cost: 312

Operation name: stablehlo.slice
cache miss
4032994822
%273 = stablehlo.slice %270 [1:2] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 178

Operation name: stablehlo.reshape
cache miss
1900484018
%274 = stablehlo.reshape %273 : (tensor<1xf32>) -> tensor<f32>
Cost: 308

Operation name: stablehlo.add
cache miss
1937139428
%275 = stablehlo.add %272, %274 : tensor<f32>
Cost: 373

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%276 = stablehlo.broadcast_in_dim %275, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 281

Operation name: stablehlo.divide
cache miss
1531341356
%277 = stablehlo.divide %270, %276 : tensor<2xf32>
Cost: 568

Operation name: stablehlo.multiply
cache miss
1284973678
%278 = stablehlo.multiply %275, %275 : tensor<f32>
Cost: 405

Operation name: stablehlo.divide
cache miss
719102371
%279 = stablehlo.divide %cst_7, %278 : tensor<f32>
Cost: 279

Operation name: stablehlo.slice
cache miss
3729170049
%280 = stablehlo.slice %256 [0:1, 0:48] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 370

Operation name: stablehlo.slice
cache miss
3729170049
%281 = stablehlo.slice %257 [0:1, 0:48] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 174

Operation name: stablehlo.concatenate
cache miss
895440948
%282 = stablehlo.concatenate %280, %281, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 282

Operation name: stablehlo.dot_general
cache miss
2902940993
%283 = stablehlo.dot_general %282, %277, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<2xf32>) -> tensor<48xf32>
Cost: 291

Operation name: stablehlo.slice
cache miss
4032389970
%284 = stablehlo.slice %251 [48:96] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 302

Operation name: stablehlo.slice
cache miss
3581210888
%285 = stablehlo.slice %253 [0:1, 48:96] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 176

Operation name: stablehlo.slice
cache miss
3581210888
%286 = stablehlo.slice %254 [0:1, 48:96] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 278

Operation name: stablehlo.concatenate
cache miss
895440948
%287 = stablehlo.concatenate %285, %286, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 283

Operation name: stablehlo.dot_general
cache miss
1097263507
%288 = stablehlo.dot_general %287, %284, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<48xf32>) -> tensor<2xf32>
Cost: 274

Operation name: stablehlo.divide
cache miss
1531341356
%289 = stablehlo.divide %288, %cst_1 : tensor<2xf32>
Cost: 308

Operation name: stablehlo.maximum
cache miss
641216813
%2733 = stablehlo.maximum %arg14, %arg15 : tensor<f32>
Cost: 312

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%290 = stablehlo.reduce(%289 init: %cst_3) applies stablehlo.maximum across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 507

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%291 = stablehlo.broadcast_in_dim %290, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 522

Operation name: stablehlo.compare
Cost: 0

Operation name: stablehlo.convert
cache miss
1629337212
%293 = stablehlo.convert %292 : (tensor<2xi1>) -> tensor<2xf32>
Cost: 313

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 275

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%294 = stablehlo.reduce(%293 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 293

Operation name: stablehlo.subtract
cache miss
3818527485
%295 = stablehlo.subtract %289, %291 : tensor<2xf32>
Cost: 270

Operation name: stablehlo.exponential
cache miss
3699945286
%296 = stablehlo.exponential %295 : tensor<2xf32>
Cost: 269

Operation name: stablehlo.slice
cache miss
2078395662
%297 = stablehlo.slice %296 [0:1] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 380

Operation name: stablehlo.reshape
cache miss
1900484018
%298 = stablehlo.reshape %297 : (tensor<1xf32>) -> tensor<f32>
Cost: 384

Operation name: stablehlo.slice
cache miss
4032994822
%299 = stablehlo.slice %296 [1:2] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 382

Operation name: stablehlo.reshape
cache miss
1900484018
%300 = stablehlo.reshape %299 : (tensor<1xf32>) -> tensor<f32>
Cost: 386

Operation name: stablehlo.add
cache miss
1937139428
%301 = stablehlo.add %298, %300 : tensor<f32>
Cost: 338

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%302 = stablehlo.broadcast_in_dim %301, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 296

Operation name: stablehlo.divide
cache miss
1531341356
%303 = stablehlo.divide %296, %302 : tensor<2xf32>
Cost: 298

Operation name: stablehlo.multiply
cache miss
1284973678
%304 = stablehlo.multiply %301, %301 : tensor<f32>
Cost: 174

Operation name: stablehlo.divide
cache miss
719102371
%305 = stablehlo.divide %cst_7, %304 : tensor<f32>
Cost: 543

Operation name: stablehlo.slice
cache miss
3581210888
%306 = stablehlo.slice %256 [0:1, 48:96] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 366

Operation name: stablehlo.slice
cache miss
3581210888
%307 = stablehlo.slice %257 [0:1, 48:96] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 307

Operation name: stablehlo.concatenate
cache miss
895440948
%308 = stablehlo.concatenate %306, %307, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 396

Operation name: stablehlo.dot_general
cache miss
2902940993
%309 = stablehlo.dot_general %308, %303, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<2xf32>) -> tensor<48xf32>
Cost: 267

Operation name: stablehlo.slice
cache miss
1375222232
%310 = stablehlo.slice %251 [96:144] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 395

Operation name: stablehlo.slice
cache miss
1957506058
%311 = stablehlo.slice %253 [0:1, 96:144] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 368

Operation name: stablehlo.slice
cache miss
1957506058
%312 = stablehlo.slice %254 [0:1, 96:144] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 280

Operation name: stablehlo.concatenate
cache miss
895440948
%313 = stablehlo.concatenate %311, %312, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 309

Operation name: stablehlo.dot_general
cache miss
1097263507
%314 = stablehlo.dot_general %313, %310, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<48xf32>) -> tensor<2xf32>
Cost: 176

Operation name: stablehlo.divide
cache miss
1531341356
%315 = stablehlo.divide %314, %cst_1 : tensor<2xf32>
Cost: 177

Operation name: stablehlo.maximum
cache miss
641216813
%2733 = stablehlo.maximum %arg14, %arg15 : tensor<f32>
Cost: 309

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%316 = stablehlo.reduce(%315 init: %cst_3) applies stablehlo.maximum across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 303

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%317 = stablehlo.broadcast_in_dim %316, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 273

Operation name: stablehlo.compare
Cost: 0

Operation name: stablehlo.convert
cache miss
1629337212
%319 = stablehlo.convert %318 : (tensor<2xi1>) -> tensor<2xf32>
Cost: 390

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 296

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%320 = stablehlo.reduce(%319 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 173

Operation name: stablehlo.subtract
cache miss
3818527485
%321 = stablehlo.subtract %315, %317 : tensor<2xf32>
Cost: 171

Operation name: stablehlo.exponential
cache miss
3699945286
%322 = stablehlo.exponential %321 : tensor<2xf32>
Cost: 184

Operation name: stablehlo.slice
cache miss
2078395662
%323 = stablehlo.slice %322 [0:1] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 319

Operation name: stablehlo.reshape
cache miss
1900484018
%324 = stablehlo.reshape %323 : (tensor<1xf32>) -> tensor<f32>
Cost: 272

Operation name: stablehlo.slice
cache miss
4032994822
%325 = stablehlo.slice %322 [1:2] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 445

Operation name: stablehlo.reshape
cache miss
1900484018
%326 = stablehlo.reshape %325 : (tensor<1xf32>) -> tensor<f32>
Cost: 407

Operation name: stablehlo.add
cache miss
1937139428
%327 = stablehlo.add %324, %326 : tensor<f32>
Cost: 371

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%328 = stablehlo.broadcast_in_dim %327, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 360

Operation name: stablehlo.divide
cache miss
1531341356
%329 = stablehlo.divide %322, %328 : tensor<2xf32>
Cost: 172

Operation name: stablehlo.multiply
cache miss
1284973678
%330 = stablehlo.multiply %327, %327 : tensor<f32>
Cost: 300

Operation name: stablehlo.divide
cache miss
719102371
%331 = stablehlo.divide %cst_7, %330 : tensor<f32>
Cost: 381

Operation name: stablehlo.slice
cache miss
1957506058
%332 = stablehlo.slice %256 [0:1, 96:144] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 176

Operation name: stablehlo.slice
cache miss
1957506058
%333 = stablehlo.slice %257 [0:1, 96:144] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 478

Operation name: stablehlo.concatenate
cache miss
895440948
%334 = stablehlo.concatenate %332, %333, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 390

Operation name: stablehlo.dot_general
cache miss
2902940993
%335 = stablehlo.dot_general %334, %329, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<2xf32>) -> tensor<48xf32>
Cost: 355

Operation name: stablehlo.slice
cache miss
3469783879
%336 = stablehlo.slice %251 [144:192] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 368

Operation name: stablehlo.slice
cache miss
3339692416
%337 = stablehlo.slice %253 [0:1, 144:192] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 401

Operation name: stablehlo.slice
cache miss
3339692416
%338 = stablehlo.slice %254 [0:1, 144:192] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 175

Operation name: stablehlo.concatenate
cache miss
895440948
%339 = stablehlo.concatenate %337, %338, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 176

Operation name: stablehlo.dot_general
cache miss
1097263507
%340 = stablehlo.dot_general %339, %336, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<48xf32>) -> tensor<2xf32>
Cost: 496

Operation name: stablehlo.divide
cache miss
1531341356
%341 = stablehlo.divide %340, %cst_1 : tensor<2xf32>
Cost: 286

Operation name: stablehlo.maximum
cache miss
641216813
%2733 = stablehlo.maximum %arg14, %arg15 : tensor<f32>
Cost: 174

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%342 = stablehlo.reduce(%341 init: %cst_3) applies stablehlo.maximum across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 171

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%343 = stablehlo.broadcast_in_dim %342, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 270

Operation name: stablehlo.compare
Cost: 0

Operation name: stablehlo.convert
cache miss
1629337212
%345 = stablehlo.convert %344 : (tensor<2xi1>) -> tensor<2xf32>
Cost: 274

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 276

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%346 = stablehlo.reduce(%345 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 402

Operation name: stablehlo.subtract
cache miss
3818527485
%347 = stablehlo.subtract %341, %343 : tensor<2xf32>
Cost: 295

Operation name: stablehlo.exponential
cache miss
3699945286
%348 = stablehlo.exponential %347 : tensor<2xf32>
Cost: 384

Operation name: stablehlo.slice
cache miss
2078395662
%349 = stablehlo.slice %348 [0:1] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 272

Operation name: stablehlo.reshape
cache miss
1900484018
%350 = stablehlo.reshape %349 : (tensor<1xf32>) -> tensor<f32>
Cost: 385

Operation name: stablehlo.slice
cache miss
4032994822
%351 = stablehlo.slice %348 [1:2] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 272

Operation name: stablehlo.reshape
cache miss
1900484018
%352 = stablehlo.reshape %351 : (tensor<1xf32>) -> tensor<f32>
Cost: 276

Operation name: stablehlo.add
cache miss
1937139428
%353 = stablehlo.add %350, %352 : tensor<f32>
Cost: 389

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%354 = stablehlo.broadcast_in_dim %353, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 170

Operation name: stablehlo.divide
cache miss
1531341356
%355 = stablehlo.divide %348, %354 : tensor<2xf32>
Cost: 454

Operation name: stablehlo.multiply
cache miss
1284973678
%356 = stablehlo.multiply %353, %353 : tensor<f32>
Cost: 176

Operation name: stablehlo.divide
cache miss
719102371
%357 = stablehlo.divide %cst_7, %356 : tensor<f32>
Cost: 364

Operation name: stablehlo.slice
cache miss
3339692416
%358 = stablehlo.slice %256 [0:1, 144:192] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 306

Operation name: stablehlo.slice
cache miss
3339692416
%359 = stablehlo.slice %257 [0:1, 144:192] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 305

Operation name: stablehlo.concatenate
cache miss
895440948
%360 = stablehlo.concatenate %358, %359, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 529

Operation name: stablehlo.dot_general
cache miss
2902940993
%361 = stablehlo.dot_general %360, %355, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<2xf32>) -> tensor<48xf32>
Cost: 265

Operation name: stablehlo.slice
cache miss
1801550340
%362 = stablehlo.slice %251 [192:240] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 174

Operation name: stablehlo.slice
cache miss
3411559083
%363 = stablehlo.slice %253 [0:1, 192:240] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 366

Operation name: stablehlo.slice
cache miss
3411559083
%364 = stablehlo.slice %254 [0:1, 192:240] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 172

Operation name: stablehlo.concatenate
cache miss
895440948
%365 = stablehlo.concatenate %363, %364, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 280

Operation name: stablehlo.dot_general
cache miss
1097263507
%366 = stablehlo.dot_general %365, %362, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<48xf32>) -> tensor<2xf32>
Cost: 298

Operation name: stablehlo.divide
cache miss
1531341356
%367 = stablehlo.divide %366, %cst_1 : tensor<2xf32>
Cost: 393

Operation name: stablehlo.maximum
cache miss
641216813
%2733 = stablehlo.maximum %arg14, %arg15 : tensor<f32>
Cost: 566

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%368 = stablehlo.reduce(%367 init: %cst_3) applies stablehlo.maximum across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 360

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%369 = stablehlo.broadcast_in_dim %368, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 271

Operation name: stablehlo.compare
Cost: 0

Operation name: stablehlo.convert
cache miss
1629337212
%371 = stablehlo.convert %370 : (tensor<2xi1>) -> tensor<2xf32>
Cost: 361

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 362

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%372 = stablehlo.reduce(%371 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 275

Operation name: stablehlo.subtract
cache miss
3818527485
%373 = stablehlo.subtract %367, %369 : tensor<2xf32>
Cost: 540

Operation name: stablehlo.exponential
cache miss
3699945286
%374 = stablehlo.exponential %373 : tensor<2xf32>
Cost: 284

Operation name: stablehlo.slice
cache miss
2078395662
%375 = stablehlo.slice %374 [0:1] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 373

Operation name: stablehlo.reshape
cache miss
1900484018
%376 = stablehlo.reshape %375 : (tensor<1xf32>) -> tensor<f32>
Cost: 369

Operation name: stablehlo.slice
cache miss
4032994822
%377 = stablehlo.slice %374 [1:2] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 361

Operation name: stablehlo.reshape
cache miss
1900484018
%378 = stablehlo.reshape %377 : (tensor<1xf32>) -> tensor<f32>
Cost: 270

Operation name: stablehlo.add
cache miss
1937139428
%379 = stablehlo.add %376, %378 : tensor<f32>
Cost: 293

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%380 = stablehlo.broadcast_in_dim %379, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 277

Operation name: stablehlo.divide
cache miss
1531341356
%381 = stablehlo.divide %374, %380 : tensor<2xf32>
Cost: 388

Operation name: stablehlo.multiply
cache miss
1284973678
%382 = stablehlo.multiply %379, %379 : tensor<f32>
Cost: 364

Operation name: stablehlo.divide
cache miss
719102371
%383 = stablehlo.divide %cst_7, %382 : tensor<f32>
Cost: 388

Operation name: stablehlo.slice
cache miss
3411559083
%384 = stablehlo.slice %256 [0:1, 192:240] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 479

Operation name: stablehlo.slice
cache miss
3411559083
%385 = stablehlo.slice %257 [0:1, 192:240] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 277

Operation name: stablehlo.concatenate
cache miss
895440948
%386 = stablehlo.concatenate %384, %385, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 494

Operation name: stablehlo.dot_general
cache miss
2902940993
%387 = stablehlo.dot_general %386, %381, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<2xf32>) -> tensor<48xf32>
Cost: 167

Operation name: stablehlo.slice
cache miss
2798019243
%388 = stablehlo.slice %251 [240:288] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 178

Operation name: stablehlo.slice
cache miss
1619767244
%389 = stablehlo.slice %253 [0:1, 240:288] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 369

Operation name: stablehlo.slice
cache miss
1619767244
%390 = stablehlo.slice %254 [0:1, 240:288] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 298

Operation name: stablehlo.concatenate
cache miss
895440948
%391 = stablehlo.concatenate %389, %390, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 375

Operation name: stablehlo.dot_general
cache miss
1097263507
%392 = stablehlo.dot_general %391, %388, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<48xf32>) -> tensor<2xf32>
Cost: 302

Operation name: stablehlo.divide
cache miss
1531341356
%393 = stablehlo.divide %392, %cst_1 : tensor<2xf32>
Cost: 276

Operation name: stablehlo.maximum
cache miss
641216813
%2733 = stablehlo.maximum %arg14, %arg15 : tensor<f32>
Cost: 176

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%394 = stablehlo.reduce(%393 init: %cst_3) applies stablehlo.maximum across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 172

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%395 = stablehlo.broadcast_in_dim %394, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 172

Operation name: stablehlo.compare
Cost: 0

Operation name: stablehlo.convert
cache miss
1629337212
%397 = stablehlo.convert %396 : (tensor<2xi1>) -> tensor<2xf32>
Cost: 478

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 170

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%398 = stablehlo.reduce(%397 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 172

Operation name: stablehlo.subtract
cache miss
3818527485
%399 = stablehlo.subtract %393, %395 : tensor<2xf32>
Cost: 171

Operation name: stablehlo.exponential
cache miss
3699945286
%400 = stablehlo.exponential %399 : tensor<2xf32>
Cost: 171

Operation name: stablehlo.slice
cache miss
2078395662
%401 = stablehlo.slice %400 [0:1] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 175

Operation name: stablehlo.reshape
cache miss
1900484018
%402 = stablehlo.reshape %401 : (tensor<1xf32>) -> tensor<f32>
Cost: 275

Operation name: stablehlo.slice
cache miss
4032994822
%403 = stablehlo.slice %400 [1:2] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 172

Operation name: stablehlo.reshape
cache miss
1900484018
%404 = stablehlo.reshape %403 : (tensor<1xf32>) -> tensor<f32>
Cost: 442

Operation name: stablehlo.add
cache miss
1937139428
%405 = stablehlo.add %402, %404 : tensor<f32>
Cost: 183

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%406 = stablehlo.broadcast_in_dim %405, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 417

Operation name: stablehlo.divide
cache miss
1531341356
%407 = stablehlo.divide %400, %406 : tensor<2xf32>
Cost: 364

Operation name: stablehlo.multiply
cache miss
1284973678
%408 = stablehlo.multiply %405, %405 : tensor<f32>
Cost: 213

Operation name: stablehlo.divide
cache miss
719102371
%409 = stablehlo.divide %cst_7, %408 : tensor<f32>
Cost: 409

Operation name: stablehlo.slice
cache miss
1619767244
%410 = stablehlo.slice %256 [0:1, 240:288] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 283

Operation name: stablehlo.slice
cache miss
1619767244
%411 = stablehlo.slice %257 [0:1, 240:288] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 276

Operation name: stablehlo.concatenate
cache miss
895440948
%412 = stablehlo.concatenate %410, %411, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 325

Operation name: stablehlo.dot_general
cache miss
2902940993
%413 = stablehlo.dot_general %412, %407, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<2xf32>) -> tensor<48xf32>
Cost: 294

Operation name: stablehlo.concatenate
cache miss
292037604
%414 = stablehlo.concatenate %283, %309, %335, %361, %387, %413, dim = 0 : (tensor<48xf32>, tensor<48xf32>, tensor<48xf32>, tensor<48xf32>, tensor<48xf32>, tensor<48xf32>) -> tensor<288xf32>
Cost: 265

Operation name: stablehlo.slice
cache miss
3584813111
%415 = stablehlo.slice %arg8 [1:2, 0:288, 0:288] : (tensor<6x288x288xf32>) -> tensor<1x288x288xf32>
Cost: 65659

Operation name: stablehlo.reshape
cache miss
3219454195
%416 = stablehlo.reshape %415 : (tensor<1x288x288xf32>) -> tensor<288x288xf32>
Cost: 50882

Operation name: stablehlo.dot_general
cache miss
1061109412
%417 = stablehlo.dot_general %416, %414, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288x288xf32>, tensor<288xf32>) -> tensor<288xf32>
Cost: 482

Operation name: stablehlo.add
cache miss
3847999983
%418 = stablehlo.add %224, %417 : tensor<288xf32>
Cost: 265

Operation name: stablehlo.slice
cache miss
2525166936
%419 = stablehlo.slice %arg2 [1:2, 0:288] : (tensor<6x288xf32>) -> tensor<1x288xf32>
Cost: 262

Operation name: stablehlo.reshape
cache miss
1377738304
%420 = stablehlo.reshape %419 : (tensor<1x288xf32>) -> tensor<288xf32>
Cost: 255

Operation name: stablehlo.dot_general
cache miss
2778462547
%421 = stablehlo.dot_general %418, %418, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288xf32>) -> tensor<f32>
Cost: 174

Operation name: stablehlo.divide
cache miss
719102371
%422 = stablehlo.divide %421, %cst_5 : tensor<f32>
Cost: 282

Operation name: stablehlo.add
cache miss
1937139428
%423 = stablehlo.add %422, %cst_4 : tensor<f32>
Cost: 427

Operation name: stablehlo.sqrt
cache miss
1884932271
%424 = stablehlo.sqrt %423 : tensor<f32>
Cost: 330

Operation name: stablehlo.divide
cache miss
719102371
%425 = stablehlo.divide %cst_6, %424 : tensor<f32>
Cost: 193

Operation name: stablehlo.divide
cache miss
719102371
%426 = stablehlo.divide %cst_7, %424 : tensor<f32>
Cost: 180

Operation name: stablehlo.multiply
cache miss
1284973678
%427 = stablehlo.multiply %424, %424 : tensor<f32>
Cost: 364

Operation name: stablehlo.divide
cache miss
719102371
%428 = stablehlo.divide %cst_7, %427 : tensor<f32>
Cost: 272

Operation name: stablehlo.multiply
cache miss
2511610444
%429 = stablehlo.multiply %420, %418 : tensor<288xf32>
Cost: 513

Operation name: stablehlo.broadcast_in_dim
cache miss
2616490109
%430 = stablehlo.broadcast_in_dim %426, dims = [] : (tensor<f32>) -> tensor<288xf32>
Cost: 392

Operation name: stablehlo.multiply
cache miss
2511610444
%431 = stablehlo.multiply %429, %430 : tensor<288xf32>
Cost: 389

Operation name: stablehlo.slice
cache miss
832959024
%432 = stablehlo.slice %arg4 [1:2, 0:768, 0:288] : (tensor<6x768x288xf32>) -> tensor<1x768x288xf32>
Cost: 81070

Operation name: stablehlo.reshape
cache miss
3265769957
%433 = stablehlo.reshape %432 : (tensor<1x768x288xf32>) -> tensor<768x288xf32>
Cost: 99285

Operation name: stablehlo.dot_general
cache miss
2529236298
%434 = stablehlo.dot_general %433, %431, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<768x288xf32>, tensor<288xf32>) -> tensor<768xf32>
Cost: 6091

Operation name: stablehlo.slice
cache miss
832959024
%435 = stablehlo.slice %arg6 [1:2, 0:768, 0:288] : (tensor<6x768x288xf32>) -> tensor<1x768x288xf32>
Cost: 108426

Operation name: stablehlo.reshape
cache miss
3265769957
%436 = stablehlo.reshape %435 : (tensor<1x768x288xf32>) -> tensor<768x288xf32>
Cost: 52301

Operation name: stablehlo.dot_general
cache miss
2529236298
%437 = stablehlo.dot_general %436, %431, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<768x288xf32>, tensor<288xf32>) -> tensor<768xf32>
Cost: 2857

Operation name: stablehlo.negate
cache miss
4256221255
%438 = stablehlo.negate %434 : tensor<768xf32>
Cost: 281

Operation name: stablehlo.exponential
cache miss
6921514
%439 = stablehlo.exponential %438 : tensor<768xf32>
Cost: 446

Operation name: stablehlo.add
cache miss
389450582
%440 = stablehlo.add %cst_0, %439 : tensor<768xf32>
Cost: 574

Operation name: stablehlo.divide
cache miss
2134516102
%441 = stablehlo.divide %cst_0, %440 : tensor<768xf32>
Cost: 452

Operation name: stablehlo.multiply
cache miss
4137946040
%442 = stablehlo.multiply %440, %440 : tensor<768xf32>
Cost: 439

Operation name: stablehlo.divide
cache miss
2134516102
%443 = stablehlo.divide %cst_0, %442 : tensor<768xf32>
Cost: 557

Operation name: stablehlo.multiply
cache miss
4137946040
%444 = stablehlo.multiply %434, %441 : tensor<768xf32>
Cost: 483

Operation name: stablehlo.multiply
cache miss
4137946040
%445 = stablehlo.multiply %444, %437 : tensor<768xf32>
Cost: 445

Operation name: stablehlo.slice
cache miss
1541160384
%446 = stablehlo.slice %arg5 [1:2, 0:288, 0:768] : (tensor<6x288x768xf32>) -> tensor<1x288x768xf32>
Cost: 101763

Operation name: stablehlo.reshape
cache miss
2162077039
%447 = stablehlo.reshape %446 : (tensor<1x288x768xf32>) -> tensor<288x768xf32>
Cost: 89035

Operation name: stablehlo.dot_general
cache miss
1061109412
%448 = stablehlo.dot_general %447, %445, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288x768xf32>, tensor<768xf32>) -> tensor<288xf32>
Cost: 4094

Operation name: stablehlo.add
cache miss
3847999983
%449 = stablehlo.add %418, %448 : tensor<288xf32>
Cost: 529

Operation name: stablehlo.slice
cache miss
3733494965
%450 = stablehlo.slice %arg1 [2:3, 0:288] : (tensor<6x288xf32>) -> tensor<1x288xf32>
Cost: 531

Operation name: stablehlo.reshape
cache miss
1377738304
%451 = stablehlo.reshape %450 : (tensor<1x288xf32>) -> tensor<288xf32>
Cost: 534

Operation name: stablehlo.dot_general
cache miss
2778462547
%452 = stablehlo.dot_general %449, %449, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288xf32>) -> tensor<f32>
Cost: 277

Operation name: stablehlo.divide
cache miss
719102371
%453 = stablehlo.divide %452, %cst_5 : tensor<f32>
Cost: 179

Operation name: stablehlo.add
cache miss
1937139428
%454 = stablehlo.add %453, %cst_4 : tensor<f32>
Cost: 177

Operation name: stablehlo.sqrt
cache miss
1884932271
%455 = stablehlo.sqrt %454 : tensor<f32>
Cost: 414

Operation name: stablehlo.divide
cache miss
719102371
%456 = stablehlo.divide %cst_6, %455 : tensor<f32>
Cost: 281

Operation name: stablehlo.divide
cache miss
719102371
%457 = stablehlo.divide %cst_7, %455 : tensor<f32>
Cost: 281

Operation name: stablehlo.multiply
cache miss
1284973678
%458 = stablehlo.multiply %455, %455 : tensor<f32>
Cost: 350

Operation name: stablehlo.divide
cache miss
719102371
%459 = stablehlo.divide %cst_7, %458 : tensor<f32>
Cost: 285

Operation name: stablehlo.multiply
cache miss
2511610444
%460 = stablehlo.multiply %451, %449 : tensor<288xf32>
Cost: 670

Operation name: stablehlo.broadcast_in_dim
cache miss
2616490109
%461 = stablehlo.broadcast_in_dim %457, dims = [] : (tensor<f32>) -> tensor<288xf32>
Cost: 558

Operation name: stablehlo.multiply
cache miss
2511610444
%462 = stablehlo.multiply %460, %461 : tensor<288xf32>
Cost: 683

Operation name: stablehlo.slice
cache miss
2872633878
%463 = stablehlo.slice %arg9 [2:3, 0:288, 0:288] : (tensor<6x288x288xf32>) -> tensor<1x288x288xf32>
Cost: 47706

Operation name: stablehlo.reshape
cache miss
3219454195
%464 = stablehlo.reshape %463 : (tensor<1x288x288xf32>) -> tensor<288x288xf32>
Cost: 51436

Operation name: stablehlo.dot_general
cache miss
1061109412
%465 = stablehlo.dot_general %464, %462, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288x288xf32>, tensor<288xf32>) -> tensor<288xf32>
Cost: 364

Operation name: stablehlo.slice
cache miss
2872633878
%466 = stablehlo.slice %arg7 [2:3, 0:288, 0:288] : (tensor<6x288x288xf32>) -> tensor<1x288x288xf32>
Cost: 51853

Operation name: stablehlo.reshape
cache miss
3219454195
%467 = stablehlo.reshape %466 : (tensor<1x288x288xf32>) -> tensor<288x288xf32>
Cost: 41848

Operation name: stablehlo.dot_general
cache miss
1061109412
%468 = stablehlo.dot_general %467, %462, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288x288xf32>, tensor<288xf32>) -> tensor<288xf32>
Cost: 455

Operation name: stablehlo.slice
cache miss
2872633878
%469 = stablehlo.slice %arg10 [2:3, 0:288, 0:288] : (tensor<6x288x288xf32>) -> tensor<1x288x288xf32>
Cost: 38734

Operation name: stablehlo.reshape
cache miss
3219454195
%470 = stablehlo.reshape %469 : (tensor<1x288x288xf32>) -> tensor<288x288xf32>
Cost: 43912

Operation name: stablehlo.dot_general
cache miss
1061109412
%471 = stablehlo.dot_general %470, %462, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288x288xf32>, tensor<288xf32>) -> tensor<288xf32>
Cost: 451

Operation name: stablehlo.reshape
cache miss
2392077026
%472 = stablehlo.reshape %465 : (tensor<288xf32>) -> tensor<144x2xf32>
Cost: 548

Operation name: stablehlo.reshape
cache miss
2392077026
%473 = stablehlo.reshape %468 : (tensor<288xf32>) -> tensor<144x2xf32>
Cost: 535

Operation name: stablehlo.dot_general
cache miss
865228849
%474 = stablehlo.dot_general %cst_8, %473, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<144x2x2xf32>, tensor<144x2xf32>) -> tensor<144x2xf32>
Cost: 712

Operation name: stablehlo.dot_general
cache miss
865228849
%475 = stablehlo.dot_general %cst_8, %472, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<144x2x2xf32>, tensor<144x2xf32>) -> tensor<144x2xf32>
Cost: 413

Operation name: stablehlo.reshape
cache miss
1377738304
%476 = stablehlo.reshape %475 : (tensor<144x2xf32>) -> tensor<288xf32>
Cost: 488

Operation name: stablehlo.slice
cache miss
4053320730
%477 = stablehlo.slice %arg11 [2:3, 0:1, 0:288] : (tensor<6x1x288xf32>) -> tensor<1x1x288xf32>
Cost: 553

Operation name: stablehlo.reshape
cache miss
2960881163
%478 = stablehlo.reshape %477 : (tensor<1x1x288xf32>) -> tensor<1x288xf32>
Cost: 532

Operation name: stablehlo.reshape
cache miss
2960881163
%479 = stablehlo.reshape %474 : (tensor<144x2xf32>) -> tensor<1x288xf32>
Cost: 514

Operation name: stablehlo.slice
cache miss
4053320730
%480 = stablehlo.slice %arg12 [2:3, 0:1, 0:288] : (tensor<6x1x288xf32>) -> tensor<1x1x288xf32>
Cost: 434

Operation name: stablehlo.reshape
cache miss
2960881163
%481 = stablehlo.reshape %480 : (tensor<1x1x288xf32>) -> tensor<1x288xf32>
Cost: 681

Operation name: stablehlo.reshape
cache miss
2960881163
%482 = stablehlo.reshape %471 : (tensor<288xf32>) -> tensor<1x288xf32>
Cost: 391

Operation name: stablehlo.slice
cache miss
2839717066
%483 = stablehlo.slice %476 [0:48] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 622

Operation name: stablehlo.slice
cache miss
3729170049
%484 = stablehlo.slice %478 [0:1, 0:48] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 363

Operation name: stablehlo.slice
cache miss
3729170049
%485 = stablehlo.slice %479 [0:1, 0:48] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 373

Operation name: stablehlo.concatenate
cache miss
895440948
%486 = stablehlo.concatenate %484, %485, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 277

Operation name: stablehlo.dot_general
cache miss
1097263507
%487 = stablehlo.dot_general %486, %483, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<48xf32>) -> tensor<2xf32>
Cost: 185

Operation name: stablehlo.divide
cache miss
1531341356
%488 = stablehlo.divide %487, %cst_1 : tensor<2xf32>
Cost: 291

Operation name: stablehlo.maximum
cache miss
641216813
%2733 = stablehlo.maximum %arg14, %arg15 : tensor<f32>
Cost: 295

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%489 = stablehlo.reduce(%488 init: %cst_3) applies stablehlo.maximum across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 413

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%490 = stablehlo.broadcast_in_dim %489, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 293

Operation name: stablehlo.compare
Cost: 0

Operation name: stablehlo.convert
cache miss
1629337212
%492 = stablehlo.convert %491 : (tensor<2xi1>) -> tensor<2xf32>
Cost: 314

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 390

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%493 = stablehlo.reduce(%492 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 292

Operation name: stablehlo.subtract
cache miss
3818527485
%494 = stablehlo.subtract %488, %490 : tensor<2xf32>
Cost: 296

Operation name: stablehlo.exponential
cache miss
3699945286
%495 = stablehlo.exponential %494 : tensor<2xf32>
Cost: 286

Operation name: stablehlo.slice
cache miss
2078395662
%496 = stablehlo.slice %495 [0:1] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 183

Operation name: stablehlo.reshape
cache miss
1900484018
%497 = stablehlo.reshape %496 : (tensor<1xf32>) -> tensor<f32>
Cost: 285

Operation name: stablehlo.slice
cache miss
4032994822
%498 = stablehlo.slice %495 [1:2] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 298

Operation name: stablehlo.reshape
cache miss
1900484018
%499 = stablehlo.reshape %498 : (tensor<1xf32>) -> tensor<f32>
Cost: 400

Operation name: stablehlo.add
cache miss
1937139428
%500 = stablehlo.add %497, %499 : tensor<f32>
Cost: 312

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%501 = stablehlo.broadcast_in_dim %500, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 376

Operation name: stablehlo.divide
cache miss
1531341356
%502 = stablehlo.divide %495, %501 : tensor<2xf32>
Cost: 377

Operation name: stablehlo.multiply
cache miss
1284973678
%503 = stablehlo.multiply %500, %500 : tensor<f32>
Cost: 285

Operation name: stablehlo.divide
cache miss
719102371
%504 = stablehlo.divide %cst_7, %503 : tensor<f32>
Cost: 476

Operation name: stablehlo.slice
cache miss
3729170049
%505 = stablehlo.slice %481 [0:1, 0:48] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 279

Operation name: stablehlo.slice
cache miss
3729170049
%506 = stablehlo.slice %482 [0:1, 0:48] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 274

Operation name: stablehlo.concatenate
cache miss
895440948
%507 = stablehlo.concatenate %505, %506, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 370

Operation name: stablehlo.dot_general
cache miss
2902940993
%508 = stablehlo.dot_general %507, %502, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<2xf32>) -> tensor<48xf32>
Cost: 360

Operation name: stablehlo.slice
cache miss
4032389970
%509 = stablehlo.slice %476 [48:96] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 281

Operation name: stablehlo.slice
cache miss
3581210888
%510 = stablehlo.slice %478 [0:1, 48:96] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 210

Operation name: stablehlo.slice
cache miss
3581210888
%511 = stablehlo.slice %479 [0:1, 48:96] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 172

Operation name: stablehlo.concatenate
cache miss
895440948
%512 = stablehlo.concatenate %510, %511, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 177

Operation name: stablehlo.dot_general
cache miss
1097263507
%513 = stablehlo.dot_general %512, %509, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<48xf32>) -> tensor<2xf32>
Cost: 175

Operation name: stablehlo.divide
cache miss
1531341356
%514 = stablehlo.divide %513, %cst_1 : tensor<2xf32>
Cost: 405

Operation name: stablehlo.maximum
cache miss
641216813
%2733 = stablehlo.maximum %arg14, %arg15 : tensor<f32>
Cost: 286

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%515 = stablehlo.reduce(%514 init: %cst_3) applies stablehlo.maximum across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 374

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%516 = stablehlo.broadcast_in_dim %515, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 282

Operation name: stablehlo.compare
Cost: 0

Operation name: stablehlo.convert
cache miss
1629337212
%518 = stablehlo.convert %517 : (tensor<2xi1>) -> tensor<2xf32>
Cost: 526

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 272

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%519 = stablehlo.reduce(%518 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 173

Operation name: stablehlo.subtract
cache miss
3818527485
%520 = stablehlo.subtract %514, %516 : tensor<2xf32>
Cost: 171

Operation name: stablehlo.exponential
cache miss
3699945286
%521 = stablehlo.exponential %520 : tensor<2xf32>
Cost: 373

Operation name: stablehlo.slice
cache miss
2078395662
%522 = stablehlo.slice %521 [0:1] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 365

Operation name: stablehlo.reshape
cache miss
1900484018
%523 = stablehlo.reshape %522 : (tensor<1xf32>) -> tensor<f32>
Cost: 365

Operation name: stablehlo.slice
cache miss
4032994822
%524 = stablehlo.slice %521 [1:2] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 360

Operation name: stablehlo.reshape
cache miss
1900484018
%525 = stablehlo.reshape %524 : (tensor<1xf32>) -> tensor<f32>
Cost: 480

Operation name: stablehlo.add
cache miss
1937139428
%526 = stablehlo.add %523, %525 : tensor<f32>
Cost: 186

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%527 = stablehlo.broadcast_in_dim %526, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 171

Operation name: stablehlo.divide
cache miss
1531341356
%528 = stablehlo.divide %521, %527 : tensor<2xf32>
Cost: 272

Operation name: stablehlo.multiply
cache miss
1284973678
%529 = stablehlo.multiply %526, %526 : tensor<f32>
Cost: 434

Operation name: stablehlo.divide
cache miss
719102371
%530 = stablehlo.divide %cst_7, %529 : tensor<f32>
Cost: 276

Operation name: stablehlo.slice
cache miss
3581210888
%531 = stablehlo.slice %481 [0:1, 48:96] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 177

Operation name: stablehlo.slice
cache miss
3581210888
%532 = stablehlo.slice %482 [0:1, 48:96] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 184

Operation name: stablehlo.concatenate
cache miss
895440948
%533 = stablehlo.concatenate %531, %532, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 481

Operation name: stablehlo.dot_general
cache miss
2902940993
%534 = stablehlo.dot_general %533, %528, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<2xf32>) -> tensor<48xf32>
Cost: 471

Operation name: stablehlo.slice
cache miss
1375222232
%535 = stablehlo.slice %476 [96:144] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 369

Operation name: stablehlo.slice
cache miss
1957506058
%536 = stablehlo.slice %478 [0:1, 96:144] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 362

Operation name: stablehlo.slice
cache miss
1957506058
%537 = stablehlo.slice %479 [0:1, 96:144] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 279

Operation name: stablehlo.concatenate
cache miss
895440948
%538 = stablehlo.concatenate %536, %537, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 369

Operation name: stablehlo.dot_general
cache miss
1097263507
%539 = stablehlo.dot_general %538, %535, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<48xf32>) -> tensor<2xf32>
Cost: 177

Operation name: stablehlo.divide
cache miss
1531341356
%540 = stablehlo.divide %539, %cst_1 : tensor<2xf32>
Cost: 382

Operation name: stablehlo.maximum
cache miss
641216813
%2733 = stablehlo.maximum %arg14, %arg15 : tensor<f32>
Cost: 173

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%541 = stablehlo.reduce(%540 init: %cst_3) applies stablehlo.maximum across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 365

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%542 = stablehlo.broadcast_in_dim %541, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 271

Operation name: stablehlo.compare
Cost: 0

Operation name: stablehlo.convert
cache miss
1629337212
%544 = stablehlo.convert %543 : (tensor<2xi1>) -> tensor<2xf32>
Cost: 365

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 369

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%545 = stablehlo.reduce(%544 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 386

Operation name: stablehlo.subtract
cache miss
3818527485
%546 = stablehlo.subtract %540, %542 : tensor<2xf32>
Cost: 506

Operation name: stablehlo.exponential
cache miss
3699945286
%547 = stablehlo.exponential %546 : tensor<2xf32>
Cost: 303

Operation name: stablehlo.slice
cache miss
2078395662
%548 = stablehlo.slice %547 [0:1] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 173

Operation name: stablehlo.reshape
cache miss
1900484018
%549 = stablehlo.reshape %548 : (tensor<1xf32>) -> tensor<f32>
Cost: 276

Operation name: stablehlo.slice
cache miss
4032994822
%550 = stablehlo.slice %547 [1:2] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 171

Operation name: stablehlo.reshape
cache miss
1900484018
%551 = stablehlo.reshape %550 : (tensor<1xf32>) -> tensor<f32>
Cost: 361

Operation name: stablehlo.add
cache miss
1937139428
%552 = stablehlo.add %549, %551 : tensor<f32>
Cost: 274

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%553 = stablehlo.broadcast_in_dim %552, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 173

Operation name: stablehlo.divide
cache miss
1531341356
%554 = stablehlo.divide %547, %553 : tensor<2xf32>
Cost: 285

Operation name: stablehlo.multiply
cache miss
1284973678
%555 = stablehlo.multiply %552, %552 : tensor<f32>
Cost: 365

Operation name: stablehlo.divide
cache miss
719102371
%556 = stablehlo.divide %cst_7, %555 : tensor<f32>
Cost: 272

Operation name: stablehlo.slice
cache miss
1957506058
%557 = stablehlo.slice %481 [0:1, 96:144] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 274

Operation name: stablehlo.slice
cache miss
1957506058
%558 = stablehlo.slice %482 [0:1, 96:144] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 174

Operation name: stablehlo.concatenate
cache miss
895440948
%559 = stablehlo.concatenate %557, %558, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 421

Operation name: stablehlo.dot_general
cache miss
2902940993
%560 = stablehlo.dot_general %559, %554, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<2xf32>) -> tensor<48xf32>
Cost: 264

Operation name: stablehlo.slice
cache miss
3469783879
%561 = stablehlo.slice %476 [144:192] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 278

Operation name: stablehlo.slice
cache miss
3339692416
%562 = stablehlo.slice %478 [0:1, 144:192] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 279

Operation name: stablehlo.slice
cache miss
3339692416
%563 = stablehlo.slice %479 [0:1, 144:192] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 486

Operation name: stablehlo.concatenate
cache miss
895440948
%564 = stablehlo.concatenate %562, %563, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 187

Operation name: stablehlo.dot_general
cache miss
1097263507
%565 = stablehlo.dot_general %564, %561, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<48xf32>) -> tensor<2xf32>
Cost: 174

Operation name: stablehlo.divide
cache miss
1531341356
%566 = stablehlo.divide %565, %cst_1 : tensor<2xf32>
Cost: 312

Operation name: stablehlo.maximum
cache miss
641216813
%2733 = stablehlo.maximum %arg14, %arg15 : tensor<f32>
Cost: 177

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%567 = stablehlo.reduce(%566 init: %cst_3) applies stablehlo.maximum across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 435

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%568 = stablehlo.broadcast_in_dim %567, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 363

Operation name: stablehlo.compare
Cost: 0

Operation name: stablehlo.convert
cache miss
1629337212
%570 = stablehlo.convert %569 : (tensor<2xi1>) -> tensor<2xf32>
Cost: 273

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 362

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%571 = stablehlo.reduce(%570 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 363

Operation name: stablehlo.subtract
cache miss
3818527485
%572 = stablehlo.subtract %566, %568 : tensor<2xf32>
Cost: 359

Operation name: stablehlo.exponential
cache miss
3699945286
%573 = stablehlo.exponential %572 : tensor<2xf32>
Cost: 275

Operation name: stablehlo.slice
cache miss
2078395662
%574 = stablehlo.slice %573 [0:1] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 385

Operation name: stablehlo.reshape
cache miss
1900484018
%575 = stablehlo.reshape %574 : (tensor<1xf32>) -> tensor<f32>
Cost: 375

Operation name: stablehlo.slice
cache miss
4032994822
%576 = stablehlo.slice %573 [1:2] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 444

Operation name: stablehlo.reshape
cache miss
1900484018
%577 = stablehlo.reshape %576 : (tensor<1xf32>) -> tensor<f32>
Cost: 277

Operation name: stablehlo.add
cache miss
1937139428
%578 = stablehlo.add %575, %577 : tensor<f32>
Cost: 276

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%579 = stablehlo.broadcast_in_dim %578, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 366

Operation name: stablehlo.divide
cache miss
1531341356
%580 = stablehlo.divide %573, %579 : tensor<2xf32>
Cost: 301

Operation name: stablehlo.multiply
cache miss
1284973678
%581 = stablehlo.multiply %578, %578 : tensor<f32>
Cost: 172

Operation name: stablehlo.divide
cache miss
719102371
%582 = stablehlo.divide %cst_7, %581 : tensor<f32>
Cost: 279

Operation name: stablehlo.slice
cache miss
3339692416
%583 = stablehlo.slice %481 [0:1, 144:192] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 185

Operation name: stablehlo.slice
cache miss
3339692416
%584 = stablehlo.slice %482 [0:1, 144:192] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 175

Operation name: stablehlo.concatenate
cache miss
895440948
%585 = stablehlo.concatenate %583, %584, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 398

Operation name: stablehlo.dot_general
cache miss
2902940993
%586 = stablehlo.dot_general %585, %580, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<2xf32>) -> tensor<48xf32>
Cost: 166

Operation name: stablehlo.slice
cache miss
1801550340
%587 = stablehlo.slice %476 [192:240] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 173

Operation name: stablehlo.slice
cache miss
3411559083
%588 = stablehlo.slice %478 [0:1, 192:240] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 369

Operation name: stablehlo.slice
cache miss
3411559083
%589 = stablehlo.slice %479 [0:1, 192:240] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 279

Operation name: stablehlo.concatenate
cache miss
895440948
%590 = stablehlo.concatenate %588, %589, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 178

Operation name: stablehlo.dot_general
cache miss
1097263507
%591 = stablehlo.dot_general %590, %587, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<48xf32>) -> tensor<2xf32>
Cost: 273

Operation name: stablehlo.divide
cache miss
1531341356
%592 = stablehlo.divide %591, %cst_1 : tensor<2xf32>
Cost: 372

Operation name: stablehlo.maximum
cache miss
641216813
%2733 = stablehlo.maximum %arg14, %arg15 : tensor<f32>
Cost: 384

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%593 = stablehlo.reduce(%592 init: %cst_3) applies stablehlo.maximum across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 369

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%594 = stablehlo.broadcast_in_dim %593, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 359

Operation name: stablehlo.compare
Cost: 0

Operation name: stablehlo.convert
cache miss
1629337212
%596 = stablehlo.convert %595 : (tensor<2xi1>) -> tensor<2xf32>
Cost: 366

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 300

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%597 = stablehlo.reduce(%596 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 371

Operation name: stablehlo.subtract
cache miss
3818527485
%598 = stablehlo.subtract %592, %594 : tensor<2xf32>
Cost: 323

Operation name: stablehlo.exponential
cache miss
3699945286
%599 = stablehlo.exponential %598 : tensor<2xf32>
Cost: 359

Operation name: stablehlo.slice
cache miss
2078395662
%600 = stablehlo.slice %599 [0:1] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 365

Operation name: stablehlo.reshape
cache miss
1900484018
%601 = stablehlo.reshape %600 : (tensor<1xf32>) -> tensor<f32>
Cost: 276

Operation name: stablehlo.slice
cache miss
4032994822
%602 = stablehlo.slice %599 [1:2] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 350

Operation name: stablehlo.reshape
cache miss
1900484018
%603 = stablehlo.reshape %602 : (tensor<1xf32>) -> tensor<f32>
Cost: 274

Operation name: stablehlo.add
cache miss
1937139428
%604 = stablehlo.add %601, %603 : tensor<f32>
Cost: 387

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%605 = stablehlo.broadcast_in_dim %604, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 357

Operation name: stablehlo.divide
cache miss
1531341356
%606 = stablehlo.divide %599, %605 : tensor<2xf32>
Cost: 274

Operation name: stablehlo.multiply
cache miss
1284973678
%607 = stablehlo.multiply %604, %604 : tensor<f32>
Cost: 174

Operation name: stablehlo.divide
cache miss
719102371
%608 = stablehlo.divide %cst_7, %607 : tensor<f32>
Cost: 272

Operation name: stablehlo.slice
cache miss
3411559083
%609 = stablehlo.slice %481 [0:1, 192:240] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 378

Operation name: stablehlo.slice
cache miss
3411559083
%610 = stablehlo.slice %482 [0:1, 192:240] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 469

Operation name: stablehlo.concatenate
cache miss
895440948
%611 = stablehlo.concatenate %609, %610, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 367

Operation name: stablehlo.dot_general
cache miss
2902940993
%612 = stablehlo.dot_general %611, %606, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<2xf32>) -> tensor<48xf32>
Cost: 359

Operation name: stablehlo.slice
cache miss
2798019243
%613 = stablehlo.slice %476 [240:288] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 276

Operation name: stablehlo.slice
cache miss
1619767244
%614 = stablehlo.slice %478 [0:1, 240:288] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 275

Operation name: stablehlo.slice
cache miss
1619767244
%615 = stablehlo.slice %479 [0:1, 240:288] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 392

Operation name: stablehlo.concatenate
cache miss
895440948
%616 = stablehlo.concatenate %614, %615, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 371

Operation name: stablehlo.dot_general
cache miss
1097263507
%617 = stablehlo.dot_general %616, %613, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<48xf32>) -> tensor<2xf32>
Cost: 248

Operation name: stablehlo.divide
cache miss
1531341356
%618 = stablehlo.divide %617, %cst_1 : tensor<2xf32>
Cost: 285

Operation name: stablehlo.maximum
cache miss
641216813
%2733 = stablehlo.maximum %arg14, %arg15 : tensor<f32>
Cost: 275

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%619 = stablehlo.reduce(%618 init: %cst_3) applies stablehlo.maximum across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 369

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%620 = stablehlo.broadcast_in_dim %619, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 277

Operation name: stablehlo.compare
Cost: 0

Operation name: stablehlo.convert
cache miss
1629337212
%622 = stablehlo.convert %621 : (tensor<2xi1>) -> tensor<2xf32>
Cost: 272

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 170

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%623 = stablehlo.reduce(%622 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 368

Operation name: stablehlo.subtract
cache miss
3818527485
%624 = stablehlo.subtract %618, %620 : tensor<2xf32>
Cost: 365

Operation name: stablehlo.exponential
cache miss
3699945286
%625 = stablehlo.exponential %624 : tensor<2xf32>
Cost: 329

Operation name: stablehlo.slice
cache miss
2078395662
%626 = stablehlo.slice %625 [0:1] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 170

Operation name: stablehlo.reshape
cache miss
1900484018
%627 = stablehlo.reshape %626 : (tensor<1xf32>) -> tensor<f32>
Cost: 274

Operation name: stablehlo.slice
cache miss
4032994822
%628 = stablehlo.slice %625 [1:2] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 384

Operation name: stablehlo.reshape
cache miss
1900484018
%629 = stablehlo.reshape %628 : (tensor<1xf32>) -> tensor<f32>
Cost: 277

Operation name: stablehlo.add
cache miss
1937139428
%630 = stablehlo.add %627, %629 : tensor<f32>
Cost: 273

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%631 = stablehlo.broadcast_in_dim %630, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 271

Operation name: stablehlo.divide
cache miss
1531341356
%632 = stablehlo.divide %625, %631 : tensor<2xf32>
Cost: 387

Operation name: stablehlo.multiply
cache miss
1284973678
%633 = stablehlo.multiply %630, %630 : tensor<f32>
Cost: 571

Operation name: stablehlo.divide
cache miss
719102371
%634 = stablehlo.divide %cst_7, %633 : tensor<f32>
Cost: 365

Operation name: stablehlo.slice
cache miss
1619767244
%635 = stablehlo.slice %481 [0:1, 240:288] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 292

Operation name: stablehlo.slice
cache miss
1619767244
%636 = stablehlo.slice %482 [0:1, 240:288] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 278

Operation name: stablehlo.concatenate
cache miss
895440948
%637 = stablehlo.concatenate %635, %636, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 281

Operation name: stablehlo.dot_general
cache miss
2902940993
%638 = stablehlo.dot_general %637, %632, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<2xf32>) -> tensor<48xf32>
Cost: 355

Operation name: stablehlo.concatenate
cache miss
292037604
%639 = stablehlo.concatenate %508, %534, %560, %586, %612, %638, dim = 0 : (tensor<48xf32>, tensor<48xf32>, tensor<48xf32>, tensor<48xf32>, tensor<48xf32>, tensor<48xf32>) -> tensor<288xf32>
Cost: 514

Operation name: stablehlo.slice
cache miss
2872633878
%640 = stablehlo.slice %arg8 [2:3, 0:288, 0:288] : (tensor<6x288x288xf32>) -> tensor<1x288x288xf32>
Cost: 45969

Operation name: stablehlo.reshape
cache miss
3219454195
%641 = stablehlo.reshape %640 : (tensor<1x288x288xf32>) -> tensor<288x288xf32>
Cost: 41439

Operation name: stablehlo.dot_general
cache miss
1061109412
%642 = stablehlo.dot_general %641, %639, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288x288xf32>, tensor<288xf32>) -> tensor<288xf32>
Cost: 459

Operation name: stablehlo.add
cache miss
3847999983
%643 = stablehlo.add %449, %642 : tensor<288xf32>
Cost: 402

Operation name: stablehlo.slice
cache miss
3733494965
%644 = stablehlo.slice %arg2 [2:3, 0:288] : (tensor<6x288xf32>) -> tensor<1x288xf32>
Cost: 504

Operation name: stablehlo.reshape
cache miss
1377738304
%645 = stablehlo.reshape %644 : (tensor<1x288xf32>) -> tensor<288xf32>
Cost: 606

Operation name: stablehlo.dot_general
cache miss
2778462547
%646 = stablehlo.dot_general %643, %643, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288xf32>) -> tensor<f32>
Cost: 275

Operation name: stablehlo.divide
cache miss
719102371
%647 = stablehlo.divide %646, %cst_5 : tensor<f32>
Cost: 403

Operation name: stablehlo.add
cache miss
1937139428
%648 = stablehlo.add %647, %cst_4 : tensor<f32>
Cost: 288

Operation name: stablehlo.sqrt
cache miss
1884932271
%649 = stablehlo.sqrt %648 : tensor<f32>
Cost: 284

Operation name: stablehlo.divide
cache miss
719102371
%650 = stablehlo.divide %cst_6, %649 : tensor<f32>
Cost: 382

Operation name: stablehlo.divide
cache miss
719102371
%651 = stablehlo.divide %cst_7, %649 : tensor<f32>
Cost: 385

Operation name: stablehlo.multiply
cache miss
1284973678
%652 = stablehlo.multiply %649, %649 : tensor<f32>
Cost: 276

Operation name: stablehlo.divide
cache miss
719102371
%653 = stablehlo.divide %cst_7, %652 : tensor<f32>
Cost: 363

Operation name: stablehlo.multiply
cache miss
2511610444
%654 = stablehlo.multiply %645, %643 : tensor<288xf32>
Cost: 399

Operation name: stablehlo.broadcast_in_dim
cache miss
2616490109
%655 = stablehlo.broadcast_in_dim %651, dims = [] : (tensor<f32>) -> tensor<288xf32>
Cost: 502

Operation name: stablehlo.multiply
cache miss
2511610444
%656 = stablehlo.multiply %654, %655 : tensor<288xf32>
Cost: 518

Operation name: stablehlo.slice
cache miss
1581303485
%657 = stablehlo.slice %arg4 [2:3, 0:768, 0:288] : (tensor<6x768x288xf32>) -> tensor<1x768x288xf32>
Cost: 137148

Operation name: stablehlo.reshape
cache miss
3265769957
%658 = stablehlo.reshape %657 : (tensor<1x768x288xf32>) -> tensor<768x288xf32>
Cost: 113890

Operation name: stablehlo.dot_general
cache miss
2529236298
%659 = stablehlo.dot_general %658, %656, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<768x288xf32>, tensor<288xf32>) -> tensor<768xf32>
Cost: 5597

Operation name: stablehlo.slice
cache miss
1581303485
%660 = stablehlo.slice %arg6 [2:3, 0:768, 0:288] : (tensor<6x768x288xf32>) -> tensor<1x768x288xf32>
Cost: 99591

Operation name: stablehlo.reshape
cache miss
3265769957
%661 = stablehlo.reshape %660 : (tensor<1x768x288xf32>) -> tensor<768x288xf32>
Cost: 127909

Operation name: stablehlo.dot_general
cache miss
2529236298
%662 = stablehlo.dot_general %661, %656, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<768x288xf32>, tensor<288xf32>) -> tensor<768xf32>
Cost: 5553

Operation name: stablehlo.negate
cache miss
4256221255
%663 = stablehlo.negate %659 : tensor<768xf32>
Cost: 424

Operation name: stablehlo.exponential
cache miss
6921514
%664 = stablehlo.exponential %663 : tensor<768xf32>
Cost: 281

Operation name: stablehlo.add
cache miss
389450582
%665 = stablehlo.add %cst_0, %664 : tensor<768xf32>
Cost: 316

Operation name: stablehlo.divide
cache miss
2134516102
%666 = stablehlo.divide %cst_0, %665 : tensor<768xf32>
Cost: 548

Operation name: stablehlo.multiply
cache miss
4137946040
%667 = stablehlo.multiply %665, %665 : tensor<768xf32>
Cost: 484

Operation name: stablehlo.divide
cache miss
2134516102
%668 = stablehlo.divide %cst_0, %667 : tensor<768xf32>
Cost: 528

Operation name: stablehlo.multiply
cache miss
4137946040
%669 = stablehlo.multiply %659, %666 : tensor<768xf32>
Cost: 358

Operation name: stablehlo.multiply
cache miss
4137946040
%670 = stablehlo.multiply %669, %662 : tensor<768xf32>
Cost: 665

Operation name: stablehlo.slice
cache miss
1363388598
%671 = stablehlo.slice %arg5 [2:3, 0:288, 0:768] : (tensor<6x288x768xf32>) -> tensor<1x288x768xf32>
Cost: 105221

Operation name: stablehlo.reshape
cache miss
2162077039
%672 = stablehlo.reshape %671 : (tensor<1x288x768xf32>) -> tensor<288x768xf32>
Cost: 99055

Operation name: stablehlo.dot_general
cache miss
1061109412
%673 = stablehlo.dot_general %672, %670, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288x768xf32>, tensor<768xf32>) -> tensor<288xf32>
Cost: 4647

Operation name: stablehlo.add
cache miss
3847999983
%674 = stablehlo.add %643, %673 : tensor<288xf32>
Cost: 518

Operation name: stablehlo.slice
cache miss
1331838498
%675 = stablehlo.slice %arg1 [3:4, 0:288] : (tensor<6x288xf32>) -> tensor<1x288xf32>
Cost: 534

Operation name: stablehlo.reshape
cache miss
1377738304
%676 = stablehlo.reshape %675 : (tensor<1x288xf32>) -> tensor<288xf32>
Cost: 554

Operation name: stablehlo.dot_general
cache miss
2778462547
%677 = stablehlo.dot_general %674, %674, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288xf32>) -> tensor<f32>
Cost: 370

Operation name: stablehlo.divide
cache miss
719102371
%678 = stablehlo.divide %677, %cst_5 : tensor<f32>
Cost: 403

Operation name: stablehlo.add
cache miss
1937139428
%679 = stablehlo.add %678, %cst_4 : tensor<f32>
Cost: 286

Operation name: stablehlo.sqrt
cache miss
1884932271
%680 = stablehlo.sqrt %679 : tensor<f32>
Cost: 181

Operation name: stablehlo.divide
cache miss
719102371
%681 = stablehlo.divide %cst_6, %680 : tensor<f32>
Cost: 566

Operation name: stablehlo.divide
cache miss
719102371
%682 = stablehlo.divide %cst_7, %680 : tensor<f32>
Cost: 284

Operation name: stablehlo.multiply
cache miss
1284973678
%683 = stablehlo.multiply %680, %680 : tensor<f32>
Cost: 422

Operation name: stablehlo.divide
cache miss
719102371
%684 = stablehlo.divide %cst_7, %683 : tensor<f32>
Cost: 178

Operation name: stablehlo.multiply
cache miss
2511610444
%685 = stablehlo.multiply %676, %674 : tensor<288xf32>
Cost: 400

Operation name: stablehlo.broadcast_in_dim
cache miss
2616490109
%686 = stablehlo.broadcast_in_dim %682, dims = [] : (tensor<f32>) -> tensor<288xf32>
Cost: 397

Operation name: stablehlo.multiply
cache miss
2511610444
%687 = stablehlo.multiply %685, %686 : tensor<288xf32>
Cost: 412

Operation name: stablehlo.slice
cache miss
1589645409
%688 = stablehlo.slice %arg9 [3:4, 0:288, 0:288] : (tensor<6x288x288xf32>) -> tensor<1x288x288xf32>
Cost: 50921

Operation name: stablehlo.reshape
cache miss
3219454195
%689 = stablehlo.reshape %688 : (tensor<1x288x288xf32>) -> tensor<288x288xf32>
Cost: 46091

Operation name: stablehlo.dot_general
cache miss
1061109412
%690 = stablehlo.dot_general %689, %687, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288x288xf32>, tensor<288xf32>) -> tensor<288xf32>
Cost: 323

Operation name: stablehlo.slice
cache miss
1589645409
%691 = stablehlo.slice %arg7 [3:4, 0:288, 0:288] : (tensor<6x288x288xf32>) -> tensor<1x288x288xf32>
Cost: 49925

Operation name: stablehlo.reshape
cache miss
3219454195
%692 = stablehlo.reshape %691 : (tensor<1x288x288xf32>) -> tensor<288x288xf32>
Cost: 48221

Operation name: stablehlo.dot_general
cache miss
1061109412
%693 = stablehlo.dot_general %692, %687, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288x288xf32>, tensor<288xf32>) -> tensor<288xf32>
Cost: 507

Operation name: stablehlo.slice
cache miss
1589645409
%694 = stablehlo.slice %arg10 [3:4, 0:288, 0:288] : (tensor<6x288x288xf32>) -> tensor<1x288x288xf32>
Cost: 69283

Operation name: stablehlo.reshape
cache miss
3219454195
%695 = stablehlo.reshape %694 : (tensor<1x288x288xf32>) -> tensor<288x288xf32>
Cost: 44850

Operation name: stablehlo.dot_general
cache miss
1061109412
%696 = stablehlo.dot_general %695, %687, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288x288xf32>, tensor<288xf32>) -> tensor<288xf32>
Cost: 227

Operation name: stablehlo.reshape
cache miss
2392077026
%697 = stablehlo.reshape %690 : (tensor<288xf32>) -> tensor<144x2xf32>
Cost: 545

Operation name: stablehlo.reshape
cache miss
2392077026
%698 = stablehlo.reshape %693 : (tensor<288xf32>) -> tensor<144x2xf32>
Cost: 426

Operation name: stablehlo.dot_general
cache miss
865228849
%699 = stablehlo.dot_general %cst_8, %698, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<144x2x2xf32>, tensor<144x2xf32>) -> tensor<144x2xf32>
Cost: 640

Operation name: stablehlo.dot_general
cache miss
865228849
%700 = stablehlo.dot_general %cst_8, %697, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<144x2x2xf32>, tensor<144x2xf32>) -> tensor<144x2xf32>
Cost: 632

Operation name: stablehlo.reshape
cache miss
1377738304
%701 = stablehlo.reshape %700 : (tensor<144x2xf32>) -> tensor<288xf32>
Cost: 562

Operation name: stablehlo.slice
cache miss
744886025
%702 = stablehlo.slice %arg11 [3:4, 0:1, 0:288] : (tensor<6x1x288xf32>) -> tensor<1x1x288xf32>
Cost: 623

Operation name: stablehlo.reshape
cache miss
2960881163
%703 = stablehlo.reshape %702 : (tensor<1x1x288xf32>) -> tensor<1x288xf32>
Cost: 615

Operation name: stablehlo.reshape
cache miss
2960881163
%704 = stablehlo.reshape %699 : (tensor<144x2xf32>) -> tensor<1x288xf32>
Cost: 404

Operation name: stablehlo.slice
cache miss
744886025
%705 = stablehlo.slice %arg12 [3:4, 0:1, 0:288] : (tensor<6x1x288xf32>) -> tensor<1x1x288xf32>
Cost: 499

Operation name: stablehlo.reshape
cache miss
2960881163
%706 = stablehlo.reshape %705 : (tensor<1x1x288xf32>) -> tensor<1x288xf32>
Cost: 401

Operation name: stablehlo.reshape
cache miss
2960881163
%707 = stablehlo.reshape %696 : (tensor<288xf32>) -> tensor<1x288xf32>
Cost: 501

Operation name: stablehlo.slice
cache miss
2839717066
%708 = stablehlo.slice %701 [0:48] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 362

Operation name: stablehlo.slice
cache miss
3729170049
%709 = stablehlo.slice %703 [0:1, 0:48] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 274

Operation name: stablehlo.slice
cache miss
3729170049
%710 = stablehlo.slice %704 [0:1, 0:48] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 297

Operation name: stablehlo.concatenate
cache miss
895440948
%711 = stablehlo.concatenate %709, %710, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 374

Operation name: stablehlo.dot_general
cache miss
1097263507
%712 = stablehlo.dot_general %711, %708, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<48xf32>) -> tensor<2xf32>
Cost: 290

Operation name: stablehlo.divide
cache miss
1531341356
%713 = stablehlo.divide %712, %cst_1 : tensor<2xf32>
Cost: 380

Operation name: stablehlo.maximum
cache miss
641216813
%2733 = stablehlo.maximum %arg14, %arg15 : tensor<f32>
Cost: 393

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%714 = stablehlo.reduce(%713 init: %cst_3) applies stablehlo.maximum across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 288

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%715 = stablehlo.broadcast_in_dim %714, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 383

Operation name: stablehlo.compare
Cost: 0

Operation name: stablehlo.convert
cache miss
1629337212
%717 = stablehlo.convert %716 : (tensor<2xi1>) -> tensor<2xf32>
Cost: 388

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 381

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%718 = stablehlo.reduce(%717 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 381

Operation name: stablehlo.subtract
cache miss
3818527485
%719 = stablehlo.subtract %713, %715 : tensor<2xf32>
Cost: 383

Operation name: stablehlo.exponential
cache miss
3699945286
%720 = stablehlo.exponential %719 : tensor<2xf32>
Cost: 285

Operation name: stablehlo.slice
cache miss
2078395662
%721 = stablehlo.slice %720 [0:1] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 294

Operation name: stablehlo.reshape
cache miss
1900484018
%722 = stablehlo.reshape %721 : (tensor<1xf32>) -> tensor<f32>
Cost: 401

Operation name: stablehlo.slice
cache miss
4032994822
%723 = stablehlo.slice %720 [1:2] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 396

Operation name: stablehlo.reshape
cache miss
1900484018
%724 = stablehlo.reshape %723 : (tensor<1xf32>) -> tensor<f32>
Cost: 379

Operation name: stablehlo.add
cache miss
1937139428
%725 = stablehlo.add %722, %724 : tensor<f32>
Cost: 284

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%726 = stablehlo.broadcast_in_dim %725, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 310

Operation name: stablehlo.divide
cache miss
1531341356
%727 = stablehlo.divide %720, %726 : tensor<2xf32>
Cost: 281

Operation name: stablehlo.multiply
cache miss
1284973678
%728 = stablehlo.multiply %725, %725 : tensor<f32>
Cost: 281

Operation name: stablehlo.divide
cache miss
719102371
%729 = stablehlo.divide %cst_7, %728 : tensor<f32>
Cost: 371

Operation name: stablehlo.slice
cache miss
3729170049
%730 = stablehlo.slice %706 [0:1, 0:48] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 278

Operation name: stablehlo.slice
cache miss
3729170049
%731 = stablehlo.slice %707 [0:1, 0:48] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 174

Operation name: stablehlo.concatenate
cache miss
895440948
%732 = stablehlo.concatenate %730, %731, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 372

Operation name: stablehlo.dot_general
cache miss
2902940993
%733 = stablehlo.dot_general %732, %727, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<2xf32>) -> tensor<48xf32>
Cost: 274

Operation name: stablehlo.slice
cache miss
4032389970
%734 = stablehlo.slice %701 [48:96] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 394

Operation name: stablehlo.slice
cache miss
3581210888
%735 = stablehlo.slice %703 [0:1, 48:96] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 391

Operation name: stablehlo.slice
cache miss
3581210888
%736 = stablehlo.slice %704 [0:1, 48:96] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 394

Operation name: stablehlo.concatenate
cache miss
895440948
%737 = stablehlo.concatenate %735, %736, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 374

Operation name: stablehlo.dot_general
cache miss
1097263507
%738 = stablehlo.dot_general %737, %734, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<48xf32>) -> tensor<2xf32>
Cost: 370

Operation name: stablehlo.divide
cache miss
1531341356
%739 = stablehlo.divide %738, %cst_1 : tensor<2xf32>
Cost: 500

Operation name: stablehlo.maximum
cache miss
641216813
%2733 = stablehlo.maximum %arg14, %arg15 : tensor<f32>
Cost: 284

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%740 = stablehlo.reduce(%739 init: %cst_3) applies stablehlo.maximum across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 379

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%741 = stablehlo.broadcast_in_dim %740, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 422

Operation name: stablehlo.compare
Cost: 0

Operation name: stablehlo.convert
cache miss
1629337212
%743 = stablehlo.convert %742 : (tensor<2xi1>) -> tensor<2xf32>
Cost: 410

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 367

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%744 = stablehlo.reduce(%743 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 274

Operation name: stablehlo.subtract
cache miss
3818527485
%745 = stablehlo.subtract %739, %741 : tensor<2xf32>
Cost: 172

Operation name: stablehlo.exponential
cache miss
3699945286
%746 = stablehlo.exponential %745 : tensor<2xf32>
Cost: 275

Operation name: stablehlo.slice
cache miss
2078395662
%747 = stablehlo.slice %746 [0:1] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 170

Operation name: stablehlo.reshape
cache miss
1900484018
%748 = stablehlo.reshape %747 : (tensor<1xf32>) -> tensor<f32>
Cost: 273

Operation name: stablehlo.slice
cache miss
4032994822
%749 = stablehlo.slice %746 [1:2] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 362

Operation name: stablehlo.reshape
cache miss
1900484018
%750 = stablehlo.reshape %749 : (tensor<1xf32>) -> tensor<f32>
Cost: 274

Operation name: stablehlo.add
cache miss
1937139428
%751 = stablehlo.add %748, %750 : tensor<f32>
Cost: 276

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%752 = stablehlo.broadcast_in_dim %751, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 364

Operation name: stablehlo.divide
cache miss
1531341356
%753 = stablehlo.divide %746, %752 : tensor<2xf32>
Cost: 275

Operation name: stablehlo.multiply
cache miss
1284973678
%754 = stablehlo.multiply %751, %751 : tensor<f32>
Cost: 274

Operation name: stablehlo.divide
cache miss
719102371
%755 = stablehlo.divide %cst_7, %754 : tensor<f32>
Cost: 279

Operation name: stablehlo.slice
cache miss
3581210888
%756 = stablehlo.slice %706 [0:1, 48:96] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 307

Operation name: stablehlo.slice
cache miss
3581210888
%757 = stablehlo.slice %707 [0:1, 48:96] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 304

Operation name: stablehlo.concatenate
cache miss
895440948
%758 = stablehlo.concatenate %756, %757, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 281

Operation name: stablehlo.dot_general
cache miss
2902940993
%759 = stablehlo.dot_general %758, %753, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<2xf32>) -> tensor<48xf32>
Cost: 364

Operation name: stablehlo.slice
cache miss
1375222232
%760 = stablehlo.slice %701 [96:144] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 278

Operation name: stablehlo.slice
cache miss
1957506058
%761 = stablehlo.slice %703 [0:1, 96:144] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 369

Operation name: stablehlo.slice
cache miss
1957506058
%762 = stablehlo.slice %704 [0:1, 96:144] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 277

Operation name: stablehlo.concatenate
cache miss
895440948
%763 = stablehlo.concatenate %761, %762, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 370

Operation name: stablehlo.dot_general
cache miss
1097263507
%764 = stablehlo.dot_general %763, %760, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<48xf32>) -> tensor<2xf32>
Cost: 174

Operation name: stablehlo.divide
cache miss
1531341356
%765 = stablehlo.divide %764, %cst_1 : tensor<2xf32>
Cost: 179

Operation name: stablehlo.maximum
cache miss
641216813
%2733 = stablehlo.maximum %arg14, %arg15 : tensor<f32>
Cost: 278

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%766 = stablehlo.reduce(%765 init: %cst_3) applies stablehlo.maximum across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 367

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%767 = stablehlo.broadcast_in_dim %766, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 361

Operation name: stablehlo.compare
Cost: 0

Operation name: stablehlo.convert
cache miss
1629337212
%769 = stablehlo.convert %768 : (tensor<2xi1>) -> tensor<2xf32>
Cost: 171

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 214

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%770 = stablehlo.reduce(%769 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 364

Operation name: stablehlo.subtract
cache miss
3818527485
%771 = stablehlo.subtract %765, %767 : tensor<2xf32>
Cost: 363

Operation name: stablehlo.exponential
cache miss
3699945286
%772 = stablehlo.exponential %771 : tensor<2xf32>
Cost: 274

Operation name: stablehlo.slice
cache miss
2078395662
%773 = stablehlo.slice %772 [0:1] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 272

Operation name: stablehlo.reshape
cache miss
1900484018
%774 = stablehlo.reshape %773 : (tensor<1xf32>) -> tensor<f32>
Cost: 282

Operation name: stablehlo.slice
cache miss
4032994822
%775 = stablehlo.slice %772 [1:2] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 276

Operation name: stablehlo.reshape
cache miss
1900484018
%776 = stablehlo.reshape %775 : (tensor<1xf32>) -> tensor<f32>
Cost: 270

Operation name: stablehlo.add
cache miss
1937139428
%777 = stablehlo.add %774, %776 : tensor<f32>
Cost: 525

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%778 = stablehlo.broadcast_in_dim %777, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 399

Operation name: stablehlo.divide
cache miss
1531341356
%779 = stablehlo.divide %772, %778 : tensor<2xf32>
Cost: 276

Operation name: stablehlo.multiply
cache miss
1284973678
%780 = stablehlo.multiply %777, %777 : tensor<f32>
Cost: 270

Operation name: stablehlo.divide
cache miss
719102371
%781 = stablehlo.divide %cst_7, %780 : tensor<f32>
Cost: 360

Operation name: stablehlo.slice
cache miss
1957506058
%782 = stablehlo.slice %706 [0:1, 96:144] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 276

Operation name: stablehlo.slice
cache miss
1957506058
%783 = stablehlo.slice %707 [0:1, 96:144] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 409

Operation name: stablehlo.concatenate
cache miss
895440948
%784 = stablehlo.concatenate %782, %783, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 287

Operation name: stablehlo.dot_general
cache miss
2902940993
%785 = stablehlo.dot_general %784, %779, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<2xf32>) -> tensor<48xf32>
Cost: 167

Operation name: stablehlo.slice
cache miss
3469783879
%786 = stablehlo.slice %701 [144:192] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 371

Operation name: stablehlo.slice
cache miss
3339692416
%787 = stablehlo.slice %703 [0:1, 144:192] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 300

Operation name: stablehlo.slice
cache miss
3339692416
%788 = stablehlo.slice %704 [0:1, 144:192] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 364

Operation name: stablehlo.concatenate
cache miss
895440948
%789 = stablehlo.concatenate %787, %788, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 374

Operation name: stablehlo.dot_general
cache miss
1097263507
%790 = stablehlo.dot_general %789, %786, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<48xf32>) -> tensor<2xf32>
Cost: 278

Operation name: stablehlo.divide
cache miss
1531341356
%791 = stablehlo.divide %790, %cst_1 : tensor<2xf32>
Cost: 362

Operation name: stablehlo.maximum
cache miss
641216813
%2733 = stablehlo.maximum %arg14, %arg15 : tensor<f32>
Cost: 181

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%792 = stablehlo.reduce(%791 init: %cst_3) applies stablehlo.maximum across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 279

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%793 = stablehlo.broadcast_in_dim %792, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 174

Operation name: stablehlo.compare
Cost: 0

Operation name: stablehlo.convert
cache miss
1629337212
%795 = stablehlo.convert %794 : (tensor<2xi1>) -> tensor<2xf32>
Cost: 170

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 365

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%796 = stablehlo.reduce(%795 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 385

Operation name: stablehlo.subtract
cache miss
3818527485
%797 = stablehlo.subtract %791, %793 : tensor<2xf32>
Cost: 366

Operation name: stablehlo.exponential
cache miss
3699945286
%798 = stablehlo.exponential %797 : tensor<2xf32>
Cost: 275

Operation name: stablehlo.slice
cache miss
2078395662
%799 = stablehlo.slice %798 [0:1] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 304

Operation name: stablehlo.reshape
cache miss
1900484018
%800 = stablehlo.reshape %799 : (tensor<1xf32>) -> tensor<f32>
Cost: 384

Operation name: stablehlo.slice
cache miss
4032994822
%801 = stablehlo.slice %798 [1:2] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 274

Operation name: stablehlo.reshape
cache miss
1900484018
%802 = stablehlo.reshape %801 : (tensor<1xf32>) -> tensor<f32>
Cost: 417

Operation name: stablehlo.add
cache miss
1937139428
%803 = stablehlo.add %800, %802 : tensor<f32>
Cost: 488

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%804 = stablehlo.broadcast_in_dim %803, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 386

Operation name: stablehlo.divide
cache miss
1531341356
%805 = stablehlo.divide %798, %804 : tensor<2xf32>
Cost: 170

Operation name: stablehlo.multiply
cache miss
1284973678
%806 = stablehlo.multiply %803, %803 : tensor<f32>
Cost: 366

Operation name: stablehlo.divide
cache miss
719102371
%807 = stablehlo.divide %cst_7, %806 : tensor<f32>
Cost: 177

Operation name: stablehlo.slice
cache miss
3339692416
%808 = stablehlo.slice %706 [0:1, 144:192] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 182

Operation name: stablehlo.slice
cache miss
3339692416
%809 = stablehlo.slice %707 [0:1, 144:192] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 174

Operation name: stablehlo.concatenate
cache miss
895440948
%810 = stablehlo.concatenate %808, %809, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 306

Operation name: stablehlo.dot_general
cache miss
2902940993
%811 = stablehlo.dot_general %810, %805, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<2xf32>) -> tensor<48xf32>
Cost: 169

Operation name: stablehlo.slice
cache miss
1801550340
%812 = stablehlo.slice %701 [192:240] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 276

Operation name: stablehlo.slice
cache miss
3411559083
%813 = stablehlo.slice %703 [0:1, 192:240] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 176

Operation name: stablehlo.slice
cache miss
3411559083
%814 = stablehlo.slice %704 [0:1, 192:240] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 298

Operation name: stablehlo.concatenate
cache miss
895440948
%815 = stablehlo.concatenate %813, %814, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 177

Operation name: stablehlo.dot_general
cache miss
1097263507
%816 = stablehlo.dot_general %815, %812, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<48xf32>) -> tensor<2xf32>
Cost: 177

Operation name: stablehlo.divide
cache miss
1531341356
%817 = stablehlo.divide %816, %cst_1 : tensor<2xf32>
Cost: 302

Operation name: stablehlo.maximum
cache miss
641216813
%2733 = stablehlo.maximum %arg14, %arg15 : tensor<f32>
Cost: 282

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%818 = stablehlo.reduce(%817 init: %cst_3) applies stablehlo.maximum across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 273

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%819 = stablehlo.broadcast_in_dim %818, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 274

Operation name: stablehlo.compare
Cost: 0

Operation name: stablehlo.convert
cache miss
1629337212
%821 = stablehlo.convert %820 : (tensor<2xi1>) -> tensor<2xf32>
Cost: 272

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 281

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%822 = stablehlo.reduce(%821 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 472

Operation name: stablehlo.subtract
cache miss
3818527485
%823 = stablehlo.subtract %817, %819 : tensor<2xf32>
Cost: 365

Operation name: stablehlo.exponential
cache miss
3699945286
%824 = stablehlo.exponential %823 : tensor<2xf32>
Cost: 271

Operation name: stablehlo.slice
cache miss
2078395662
%825 = stablehlo.slice %824 [0:1] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 274

Operation name: stablehlo.reshape
cache miss
1900484018
%826 = stablehlo.reshape %825 : (tensor<1xf32>) -> tensor<f32>
Cost: 371

Operation name: stablehlo.slice
cache miss
4032994822
%827 = stablehlo.slice %824 [1:2] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 274

Operation name: stablehlo.reshape
cache miss
1900484018
%828 = stablehlo.reshape %827 : (tensor<1xf32>) -> tensor<f32>
Cost: 367

Operation name: stablehlo.add
cache miss
1937139428
%829 = stablehlo.add %826, %828 : tensor<f32>
Cost: 367

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%830 = stablehlo.broadcast_in_dim %829, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 174

Operation name: stablehlo.divide
cache miss
1531341356
%831 = stablehlo.divide %824, %830 : tensor<2xf32>
Cost: 270

Operation name: stablehlo.multiply
cache miss
1284973678
%832 = stablehlo.multiply %829, %829 : tensor<f32>
Cost: 272

Operation name: stablehlo.divide
cache miss
719102371
%833 = stablehlo.divide %cst_7, %832 : tensor<f32>
Cost: 306

Operation name: stablehlo.slice
cache miss
3411559083
%834 = stablehlo.slice %706 [0:1, 192:240] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 305

Operation name: stablehlo.slice
cache miss
3411559083
%835 = stablehlo.slice %707 [0:1, 192:240] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 373

Operation name: stablehlo.concatenate
cache miss
895440948
%836 = stablehlo.concatenate %834, %835, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 307

Operation name: stablehlo.dot_general
cache miss
2902940993
%837 = stablehlo.dot_general %836, %831, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<2xf32>) -> tensor<48xf32>
Cost: 506

Operation name: stablehlo.slice
cache miss
2798019243
%838 = stablehlo.slice %701 [240:288] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 371

Operation name: stablehlo.slice
cache miss
1619767244
%839 = stablehlo.slice %703 [0:1, 240:288] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 276

Operation name: stablehlo.slice
cache miss
1619767244
%840 = stablehlo.slice %704 [0:1, 240:288] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 277

Operation name: stablehlo.concatenate
cache miss
895440948
%841 = stablehlo.concatenate %839, %840, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 324

Operation name: stablehlo.dot_general
cache miss
1097263507
%842 = stablehlo.dot_general %841, %838, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<48xf32>) -> tensor<2xf32>
Cost: 175

Operation name: stablehlo.divide
cache miss
1531341356
%843 = stablehlo.divide %842, %cst_1 : tensor<2xf32>
Cost: 178

Operation name: stablehlo.maximum
cache miss
641216813
%2733 = stablehlo.maximum %arg14, %arg15 : tensor<f32>
Cost: 181

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%844 = stablehlo.reduce(%843 init: %cst_3) applies stablehlo.maximum across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 369

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%845 = stablehlo.broadcast_in_dim %844, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 366

Operation name: stablehlo.compare
Cost: 0

Operation name: stablehlo.convert
cache miss
1629337212
%847 = stablehlo.convert %846 : (tensor<2xi1>) -> tensor<2xf32>
Cost: 271

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 277

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%848 = stablehlo.reduce(%847 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 173

Operation name: stablehlo.subtract
cache miss
3818527485
%849 = stablehlo.subtract %843, %845 : tensor<2xf32>
Cost: 242

Operation name: stablehlo.exponential
cache miss
3699945286
%850 = stablehlo.exponential %849 : tensor<2xf32>
Cost: 363

Operation name: stablehlo.slice
cache miss
2078395662
%851 = stablehlo.slice %850 [0:1] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 273

Operation name: stablehlo.reshape
cache miss
1900484018
%852 = stablehlo.reshape %851 : (tensor<1xf32>) -> tensor<f32>
Cost: 277

Operation name: stablehlo.slice
cache miss
4032994822
%853 = stablehlo.slice %850 [1:2] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 297

Operation name: stablehlo.reshape
cache miss
1900484018
%854 = stablehlo.reshape %853 : (tensor<1xf32>) -> tensor<f32>
Cost: 174

Operation name: stablehlo.add
cache miss
1937139428
%855 = stablehlo.add %852, %854 : tensor<f32>
Cost: 306

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%856 = stablehlo.broadcast_in_dim %855, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 390

Operation name: stablehlo.divide
cache miss
1531341356
%857 = stablehlo.divide %850, %856 : tensor<2xf32>
Cost: 365

Operation name: stablehlo.multiply
cache miss
1284973678
%858 = stablehlo.multiply %855, %855 : tensor<f32>
Cost: 278

Operation name: stablehlo.divide
cache miss
719102371
%859 = stablehlo.divide %cst_7, %858 : tensor<f32>
Cost: 274

Operation name: stablehlo.slice
cache miss
1619767244
%860 = stablehlo.slice %706 [0:1, 240:288] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 376

Operation name: stablehlo.slice
cache miss
1619767244
%861 = stablehlo.slice %707 [0:1, 240:288] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 279

Operation name: stablehlo.concatenate
cache miss
895440948
%862 = stablehlo.concatenate %860, %861, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 370

Operation name: stablehlo.dot_general
cache miss
2902940993
%863 = stablehlo.dot_general %862, %857, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<2xf32>) -> tensor<48xf32>
Cost: 358

Operation name: stablehlo.concatenate
cache miss
292037604
%864 = stablehlo.concatenate %733, %759, %785, %811, %837, %863, dim = 0 : (tensor<48xf32>, tensor<48xf32>, tensor<48xf32>, tensor<48xf32>, tensor<48xf32>, tensor<48xf32>) -> tensor<288xf32>
Cost: 327

Operation name: stablehlo.slice
cache miss
1589645409
%865 = stablehlo.slice %arg8 [3:4, 0:288, 0:288] : (tensor<6x288x288xf32>) -> tensor<1x288x288xf32>
Cost: 43260

Operation name: stablehlo.reshape
cache miss
3219454195
%866 = stablehlo.reshape %865 : (tensor<1x288x288xf32>) -> tensor<288x288xf32>
Cost: 45778

Operation name: stablehlo.dot_general
cache miss
1061109412
%867 = stablehlo.dot_general %866, %864, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288x288xf32>, tensor<288xf32>) -> tensor<288xf32>
Cost: 364

Operation name: stablehlo.add
cache miss
3847999983
%868 = stablehlo.add %674, %867 : tensor<288xf32>
Cost: 653

Operation name: stablehlo.slice
cache miss
1331838498
%869 = stablehlo.slice %arg2 [3:4, 0:288] : (tensor<6x288xf32>) -> tensor<1x288xf32>
Cost: 427

Operation name: stablehlo.reshape
cache miss
1377738304
%870 = stablehlo.reshape %869 : (tensor<1x288xf32>) -> tensor<288xf32>
Cost: 577

Operation name: stablehlo.dot_general
cache miss
2778462547
%871 = stablehlo.dot_general %868, %868, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288xf32>) -> tensor<f32>
Cost: 299

Operation name: stablehlo.divide
cache miss
719102371
%872 = stablehlo.divide %871, %cst_5 : tensor<f32>
Cost: 304

Operation name: stablehlo.add
cache miss
1937139428
%873 = stablehlo.add %872, %cst_4 : tensor<f32>
Cost: 283

Operation name: stablehlo.sqrt
cache miss
1884932271
%874 = stablehlo.sqrt %873 : tensor<f32>
Cost: 300

Operation name: stablehlo.divide
cache miss
719102371
%875 = stablehlo.divide %cst_6, %874 : tensor<f32>
Cost: 180

Operation name: stablehlo.divide
cache miss
719102371
%876 = stablehlo.divide %cst_7, %874 : tensor<f32>
Cost: 313

Operation name: stablehlo.multiply
cache miss
1284973678
%877 = stablehlo.multiply %874, %874 : tensor<f32>
Cost: 273

Operation name: stablehlo.divide
cache miss
719102371
%878 = stablehlo.divide %cst_7, %877 : tensor<f32>
Cost: 528

Operation name: stablehlo.multiply
cache miss
2511610444
%879 = stablehlo.multiply %870, %868 : tensor<288xf32>
Cost: 615

Operation name: stablehlo.broadcast_in_dim
cache miss
2616490109
%880 = stablehlo.broadcast_in_dim %876, dims = [] : (tensor<f32>) -> tensor<288xf32>
Cost: 438

Operation name: stablehlo.multiply
cache miss
2511610444
%881 = stablehlo.multiply %879, %880 : tensor<288xf32>
Cost: 444

Operation name: stablehlo.slice
cache miss
3251358893
%882 = stablehlo.slice %arg4 [3:4, 0:768, 0:288] : (tensor<6x768x288xf32>) -> tensor<1x768x288xf32>
Cost: 94447

Operation name: stablehlo.reshape
cache miss
3265769957
%883 = stablehlo.reshape %882 : (tensor<1x768x288xf32>) -> tensor<768x288xf32>
Cost: 96813

Operation name: stablehlo.dot_general
cache miss
2529236298
%884 = stablehlo.dot_general %883, %881, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<768x288xf32>, tensor<288xf32>) -> tensor<768xf32>
Cost: 3694

Operation name: stablehlo.slice
cache miss
3251358893
%885 = stablehlo.slice %arg6 [3:4, 0:768, 0:288] : (tensor<6x768x288xf32>) -> tensor<1x768x288xf32>
Cost: 106048

Operation name: stablehlo.reshape
cache miss
3265769957
%886 = stablehlo.reshape %885 : (tensor<1x768x288xf32>) -> tensor<768x288xf32>
Cost: 92397

Operation name: stablehlo.dot_general
cache miss
2529236298
%887 = stablehlo.dot_general %886, %881, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<768x288xf32>, tensor<288xf32>) -> tensor<768xf32>
Cost: 5170

Operation name: stablehlo.negate
cache miss
4256221255
%888 = stablehlo.negate %884 : tensor<768xf32>
Cost: 288

Operation name: stablehlo.exponential
cache miss
6921514
%889 = stablehlo.exponential %888 : tensor<768xf32>
Cost: 288

Operation name: stablehlo.add
cache miss
389450582
%890 = stablehlo.add %cst_0, %889 : tensor<768xf32>
Cost: 315

Operation name: stablehlo.divide
cache miss
2134516102
%891 = stablehlo.divide %cst_0, %890 : tensor<768xf32>
Cost: 287

Operation name: stablehlo.multiply
cache miss
4137946040
%892 = stablehlo.multiply %890, %890 : tensor<768xf32>
Cost: 361

Operation name: stablehlo.divide
cache miss
2134516102
%893 = stablehlo.divide %cst_0, %892 : tensor<768xf32>
Cost: 440

Operation name: stablehlo.multiply
cache miss
4137946040
%894 = stablehlo.multiply %884, %891 : tensor<768xf32>
Cost: 318

Operation name: stablehlo.multiply
cache miss
4137946040
%895 = stablehlo.multiply %894, %887 : tensor<768xf32>
Cost: 305

Operation name: stablehlo.slice
cache miss
1132801721
%896 = stablehlo.slice %arg5 [3:4, 0:288, 0:768] : (tensor<6x288x768xf32>) -> tensor<1x288x768xf32>
Cost: 114047

Operation name: stablehlo.reshape
cache miss
2162077039
%897 = stablehlo.reshape %896 : (tensor<1x288x768xf32>) -> tensor<288x768xf32>
Cost: 99030

Operation name: stablehlo.dot_general
cache miss
1061109412
%898 = stablehlo.dot_general %897, %895, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288x768xf32>, tensor<768xf32>) -> tensor<288xf32>
Cost: 5967

Operation name: stablehlo.add
cache miss
3847999983
%899 = stablehlo.add %868, %898 : tensor<288xf32>
Cost: 669

Operation name: stablehlo.slice
cache miss
1979475117
%900 = stablehlo.slice %arg1 [4:5, 0:288] : (tensor<6x288xf32>) -> tensor<1x288xf32>
Cost: 402

Operation name: stablehlo.reshape
cache miss
1377738304
%901 = stablehlo.reshape %900 : (tensor<1x288xf32>) -> tensor<288xf32>
Cost: 399

Operation name: stablehlo.dot_general
cache miss
2778462547
%902 = stablehlo.dot_general %899, %899, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288xf32>) -> tensor<f32>
Cost: 176

Operation name: stablehlo.divide
cache miss
719102371
%903 = stablehlo.divide %902, %cst_5 : tensor<f32>
Cost: 383

Operation name: stablehlo.add
cache miss
1937139428
%904 = stablehlo.add %903, %cst_4 : tensor<f32>
Cost: 400

Operation name: stablehlo.sqrt
cache miss
1884932271
%905 = stablehlo.sqrt %904 : tensor<f32>
Cost: 378

Operation name: stablehlo.divide
cache miss
719102371
%906 = stablehlo.divide %cst_6, %905 : tensor<f32>
Cost: 370

Operation name: stablehlo.divide
cache miss
719102371
%907 = stablehlo.divide %cst_7, %905 : tensor<f32>
Cost: 380

Operation name: stablehlo.multiply
cache miss
1284973678
%908 = stablehlo.multiply %905, %905 : tensor<f32>
Cost: 346

Operation name: stablehlo.divide
cache miss
719102371
%909 = stablehlo.divide %cst_7, %908 : tensor<f32>
Cost: 384

Operation name: stablehlo.multiply
cache miss
2511610444
%910 = stablehlo.multiply %901, %899 : tensor<288xf32>
Cost: 398

Operation name: stablehlo.broadcast_in_dim
cache miss
2616490109
%911 = stablehlo.broadcast_in_dim %907, dims = [] : (tensor<f32>) -> tensor<288xf32>
Cost: 268

Operation name: stablehlo.multiply
cache miss
2511610444
%912 = stablehlo.multiply %910, %911 : tensor<288xf32>
Cost: 401

Operation name: stablehlo.slice
cache miss
1763605157
%913 = stablehlo.slice %arg9 [4:5, 0:288, 0:288] : (tensor<6x288x288xf32>) -> tensor<1x288x288xf32>
Cost: 69696

Operation name: stablehlo.reshape
cache miss
3219454195
%914 = stablehlo.reshape %913 : (tensor<1x288x288xf32>) -> tensor<288x288xf32>
Cost: 38884

Operation name: stablehlo.dot_general
cache miss
1061109412
%915 = stablehlo.dot_general %914, %912, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288x288xf32>, tensor<288xf32>) -> tensor<288xf32>
Cost: 475

Operation name: stablehlo.slice
cache miss
1763605157
%916 = stablehlo.slice %arg7 [4:5, 0:288, 0:288] : (tensor<6x288x288xf32>) -> tensor<1x288x288xf32>
Cost: 41814

Operation name: stablehlo.reshape
cache miss
3219454195
%917 = stablehlo.reshape %916 : (tensor<1x288x288xf32>) -> tensor<288x288xf32>
Cost: 36971

Operation name: stablehlo.dot_general
cache miss
1061109412
%918 = stablehlo.dot_general %917, %912, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288x288xf32>, tensor<288xf32>) -> tensor<288xf32>
Cost: 462

Operation name: stablehlo.slice
cache miss
1763605157
%919 = stablehlo.slice %arg10 [4:5, 0:288, 0:288] : (tensor<6x288x288xf32>) -> tensor<1x288x288xf32>
Cost: 47976

Operation name: stablehlo.reshape
cache miss
3219454195
%920 = stablehlo.reshape %919 : (tensor<1x288x288xf32>) -> tensor<288x288xf32>
Cost: 40607

Operation name: stablehlo.dot_general
cache miss
1061109412
%921 = stablehlo.dot_general %920, %912, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288x288xf32>, tensor<288xf32>) -> tensor<288xf32>
Cost: 345

Operation name: stablehlo.reshape
cache miss
2392077026
%922 = stablehlo.reshape %915 : (tensor<288xf32>) -> tensor<144x2xf32>
Cost: 425

Operation name: stablehlo.reshape
cache miss
2392077026
%923 = stablehlo.reshape %918 : (tensor<288xf32>) -> tensor<144x2xf32>
Cost: 706

Operation name: stablehlo.dot_general
cache miss
865228849
%924 = stablehlo.dot_general %cst_8, %923, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<144x2x2xf32>, tensor<144x2xf32>) -> tensor<144x2xf32>
Cost: 1024

Operation name: stablehlo.dot_general
cache miss
865228849
%925 = stablehlo.dot_general %cst_8, %922, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<144x2x2xf32>, tensor<144x2xf32>) -> tensor<144x2xf32>
Cost: 646

Operation name: stablehlo.reshape
cache miss
1377738304
%926 = stablehlo.reshape %925 : (tensor<144x2xf32>) -> tensor<288xf32>
Cost: 587

Operation name: stablehlo.slice
cache miss
3074145628
%927 = stablehlo.slice %arg11 [4:5, 0:1, 0:288] : (tensor<6x1x288xf32>) -> tensor<1x1x288xf32>
Cost: 512

Operation name: stablehlo.reshape
cache miss
2960881163
%928 = stablehlo.reshape %927 : (tensor<1x1x288xf32>) -> tensor<1x288xf32>
Cost: 272

Operation name: stablehlo.reshape
cache miss
2960881163
%929 = stablehlo.reshape %924 : (tensor<144x2xf32>) -> tensor<1x288xf32>
Cost: 603

Operation name: stablehlo.slice
cache miss
3074145628
%930 = stablehlo.slice %arg12 [4:5, 0:1, 0:288] : (tensor<6x1x288xf32>) -> tensor<1x1x288xf32>
Cost: 408

Operation name: stablehlo.reshape
cache miss
2960881163
%931 = stablehlo.reshape %930 : (tensor<1x1x288xf32>) -> tensor<1x288xf32>
Cost: 817

Operation name: stablehlo.reshape
cache miss
2960881163
%932 = stablehlo.reshape %921 : (tensor<288xf32>) -> tensor<1x288xf32>
Cost: 273

Operation name: stablehlo.slice
cache miss
2839717066
%933 = stablehlo.slice %926 [0:48] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 278

Operation name: stablehlo.slice
cache miss
3729170049
%934 = stablehlo.slice %928 [0:1, 0:48] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 277

Operation name: stablehlo.slice
cache miss
3729170049
%935 = stablehlo.slice %929 [0:1, 0:48] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 176

Operation name: stablehlo.concatenate
cache miss
895440948
%936 = stablehlo.concatenate %934, %935, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 282

Operation name: stablehlo.dot_general
cache miss
1097263507
%937 = stablehlo.dot_general %936, %933, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<48xf32>) -> tensor<2xf32>
Cost: 390

Operation name: stablehlo.divide
cache miss
1531341356
%938 = stablehlo.divide %937, %cst_1 : tensor<2xf32>
Cost: 297

Operation name: stablehlo.maximum
cache miss
641216813
%2733 = stablehlo.maximum %arg14, %arg15 : tensor<f32>
Cost: 188

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%939 = stablehlo.reduce(%938 init: %cst_3) applies stablehlo.maximum across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 317

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%940 = stablehlo.broadcast_in_dim %939, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 394

Operation name: stablehlo.compare
Cost: 0

Operation name: stablehlo.convert
cache miss
1629337212
%942 = stablehlo.convert %941 : (tensor<2xi1>) -> tensor<2xf32>
Cost: 313

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 397

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%943 = stablehlo.reduce(%942 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 390

Operation name: stablehlo.subtract
cache miss
3818527485
%944 = stablehlo.subtract %938, %940 : tensor<2xf32>
Cost: 282

Operation name: stablehlo.exponential
cache miss
3699945286
%945 = stablehlo.exponential %944 : tensor<2xf32>
Cost: 193

Operation name: stablehlo.slice
cache miss
2078395662
%946 = stablehlo.slice %945 [0:1] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 379

Operation name: stablehlo.reshape
cache miss
1900484018
%947 = stablehlo.reshape %946 : (tensor<1xf32>) -> tensor<f32>
Cost: 290

Operation name: stablehlo.slice
cache miss
4032994822
%948 = stablehlo.slice %945 [1:2] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 279

Operation name: stablehlo.reshape
cache miss
1900484018
%949 = stablehlo.reshape %948 : (tensor<1xf32>) -> tensor<f32>
Cost: 279

Operation name: stablehlo.add
cache miss
1937139428
%950 = stablehlo.add %947, %949 : tensor<f32>
Cost: 302

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%951 = stablehlo.broadcast_in_dim %950, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 341

Operation name: stablehlo.divide
cache miss
1531341356
%952 = stablehlo.divide %945, %951 : tensor<2xf32>
Cost: 282

Operation name: stablehlo.multiply
cache miss
1284973678
%953 = stablehlo.multiply %950, %950 : tensor<f32>
Cost: 399

Operation name: stablehlo.divide
cache miss
719102371
%954 = stablehlo.divide %cst_7, %953 : tensor<f32>
Cost: 287

Operation name: stablehlo.slice
cache miss
3729170049
%955 = stablehlo.slice %931 [0:1, 0:48] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 277

Operation name: stablehlo.slice
cache miss
3729170049
%956 = stablehlo.slice %932 [0:1, 0:48] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 296

Operation name: stablehlo.concatenate
cache miss
895440948
%957 = stablehlo.concatenate %955, %956, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 423

Operation name: stablehlo.dot_general
cache miss
2902940993
%958 = stablehlo.dot_general %957, %952, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<2xf32>) -> tensor<48xf32>
Cost: 379

Operation name: stablehlo.slice
cache miss
4032389970
%959 = stablehlo.slice %926 [48:96] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 371

Operation name: stablehlo.slice
cache miss
3581210888
%960 = stablehlo.slice %928 [0:1, 48:96] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 412

Operation name: stablehlo.slice
cache miss
3581210888
%961 = stablehlo.slice %929 [0:1, 48:96] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 196

Operation name: stablehlo.concatenate
cache miss
895440948
%962 = stablehlo.concatenate %960, %961, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 177

Operation name: stablehlo.dot_general
cache miss
1097263507
%963 = stablehlo.dot_general %962, %959, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<48xf32>) -> tensor<2xf32>
Cost: 176

Operation name: stablehlo.divide
cache miss
1531341356
%964 = stablehlo.divide %963, %cst_1 : tensor<2xf32>
Cost: 373

Operation name: stablehlo.maximum
cache miss
641216813
%2733 = stablehlo.maximum %arg14, %arg15 : tensor<f32>
Cost: 286

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%965 = stablehlo.reduce(%964 init: %cst_3) applies stablehlo.maximum across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 183

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%966 = stablehlo.broadcast_in_dim %965, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 284

Operation name: stablehlo.compare
Cost: 0

Operation name: stablehlo.convert
cache miss
1629337212
%968 = stablehlo.convert %967 : (tensor<2xi1>) -> tensor<2xf32>
Cost: 366

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 271

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%969 = stablehlo.reduce(%968 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 290

Operation name: stablehlo.subtract
cache miss
3818527485
%970 = stablehlo.subtract %964, %966 : tensor<2xf32>
Cost: 175

Operation name: stablehlo.exponential
cache miss
3699945286
%971 = stablehlo.exponential %970 : tensor<2xf32>
Cost: 359

Operation name: stablehlo.slice
cache miss
2078395662
%972 = stablehlo.slice %971 [0:1] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 474

Operation name: stablehlo.reshape
cache miss
1900484018
%973 = stablehlo.reshape %972 : (tensor<1xf32>) -> tensor<f32>
Cost: 370

Operation name: stablehlo.slice
cache miss
4032994822
%974 = stablehlo.slice %971 [1:2] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 376

Operation name: stablehlo.reshape
cache miss
1900484018
%975 = stablehlo.reshape %974 : (tensor<1xf32>) -> tensor<f32>
Cost: 367

Operation name: stablehlo.add
cache miss
1937139428
%976 = stablehlo.add %973, %975 : tensor<f32>
Cost: 298

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%977 = stablehlo.broadcast_in_dim %976, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 276

Operation name: stablehlo.divide
cache miss
1531341356
%978 = stablehlo.divide %971, %977 : tensor<2xf32>
Cost: 349

Operation name: stablehlo.multiply
cache miss
1284973678
%979 = stablehlo.multiply %976, %976 : tensor<f32>
Cost: 172

Operation name: stablehlo.divide
cache miss
719102371
%980 = stablehlo.divide %cst_7, %979 : tensor<f32>
Cost: 175

Operation name: stablehlo.slice
cache miss
3581210888
%981 = stablehlo.slice %931 [0:1, 48:96] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 365

Operation name: stablehlo.slice
cache miss
3581210888
%982 = stablehlo.slice %932 [0:1, 48:96] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 464

Operation name: stablehlo.concatenate
cache miss
895440948
%983 = stablehlo.concatenate %981, %982, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 177

Operation name: stablehlo.dot_general
cache miss
2902940993
%984 = stablehlo.dot_general %983, %978, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<2xf32>) -> tensor<48xf32>
Cost: 167

Operation name: stablehlo.slice
cache miss
1375222232
%985 = stablehlo.slice %926 [96:144] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 174

Operation name: stablehlo.slice
cache miss
1957506058
%986 = stablehlo.slice %928 [0:1, 96:144] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 369

Operation name: stablehlo.slice
cache miss
1957506058
%987 = stablehlo.slice %929 [0:1, 96:144] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 392

Operation name: stablehlo.concatenate
cache miss
895440948
%988 = stablehlo.concatenate %986, %987, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 543

Operation name: stablehlo.dot_general
cache miss
1097263507
%989 = stablehlo.dot_general %988, %985, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<48xf32>) -> tensor<2xf32>
Cost: 875

Operation name: stablehlo.divide
cache miss
1531341356
%990 = stablehlo.divide %989, %cst_1 : tensor<2xf32>
Cost: 288

Operation name: stablehlo.maximum
cache miss
641216813
%2733 = stablehlo.maximum %arg14, %arg15 : tensor<f32>
Cost: 179

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%991 = stablehlo.reduce(%990 init: %cst_3) applies stablehlo.maximum across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 448

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%992 = stablehlo.broadcast_in_dim %991, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 411

Operation name: stablehlo.compare
Cost: 0

Operation name: stablehlo.convert
cache miss
1629337212
%994 = stablehlo.convert %993 : (tensor<2xi1>) -> tensor<2xf32>
Cost: 488

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 272

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%995 = stablehlo.reduce(%994 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 365

Operation name: stablehlo.subtract
cache miss
3818527485
%996 = stablehlo.subtract %990, %992 : tensor<2xf32>
Cost: 383

Operation name: stablehlo.exponential
cache miss
3699945286
%997 = stablehlo.exponential %996 : tensor<2xf32>
Cost: 386

Operation name: stablehlo.slice
cache miss
2078395662
%998 = stablehlo.slice %997 [0:1] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 275

Operation name: stablehlo.reshape
cache miss
1900484018
%999 = stablehlo.reshape %998 : (tensor<1xf32>) -> tensor<f32>
Cost: 401

Operation name: stablehlo.slice
cache miss
4032994822
%1000 = stablehlo.slice %997 [1:2] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 174

Operation name: stablehlo.reshape
cache miss
1900484018
%1001 = stablehlo.reshape %1000 : (tensor<1xf32>) -> tensor<f32>
Cost: 173

Operation name: stablehlo.add
cache miss
1937139428
%1002 = stablehlo.add %999, %1001 : tensor<f32>
Cost: 389

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1003 = stablehlo.broadcast_in_dim %1002, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 170

Operation name: stablehlo.divide
cache miss
1531341356
%1004 = stablehlo.divide %997, %1003 : tensor<2xf32>
Cost: 173

Operation name: stablehlo.multiply
cache miss
1284973678
%1005 = stablehlo.multiply %1002, %1002 : tensor<f32>
Cost: 388

Operation name: stablehlo.divide
cache miss
719102371
%1006 = stablehlo.divide %cst_7, %1005 : tensor<f32>
Cost: 276

Operation name: stablehlo.slice
cache miss
1957506058
%1007 = stablehlo.slice %931 [0:1, 96:144] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 372

Operation name: stablehlo.slice
cache miss
1957506058
%1008 = stablehlo.slice %932 [0:1, 96:144] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 388

Operation name: stablehlo.concatenate
cache miss
895440948
%1009 = stablehlo.concatenate %1007, %1008, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 313

Operation name: stablehlo.dot_general
cache miss
2902940993
%1010 = stablehlo.dot_general %1009, %1004, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<2xf32>) -> tensor<48xf32>
Cost: 402

Operation name: stablehlo.slice
cache miss
3469783879
%1011 = stablehlo.slice %926 [144:192] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 394

Operation name: stablehlo.slice
cache miss
3339692416
%1012 = stablehlo.slice %928 [0:1, 144:192] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 371

Operation name: stablehlo.slice
cache miss
3339692416
%1013 = stablehlo.slice %929 [0:1, 144:192] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 394

Operation name: stablehlo.concatenate
cache miss
895440948
%1014 = stablehlo.concatenate %1012, %1013, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 372

Operation name: stablehlo.dot_general
cache miss
1097263507
%1015 = stablehlo.dot_general %1014, %1011, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<48xf32>) -> tensor<2xf32>
Cost: 408

Operation name: stablehlo.divide
cache miss
1531341356
%1016 = stablehlo.divide %1015, %cst_1 : tensor<2xf32>
Cost: 389

Operation name: stablehlo.maximum
cache miss
641216813
%2733 = stablehlo.maximum %arg14, %arg15 : tensor<f32>
Cost: 231

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%1017 = stablehlo.reduce(%1016 init: %cst_3) applies stablehlo.maximum across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 367

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1018 = stablehlo.broadcast_in_dim %1017, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 415

Operation name: stablehlo.compare
Cost: 0

Operation name: stablehlo.convert
cache miss
1629337212
%1020 = stablehlo.convert %1019 : (tensor<2xi1>) -> tensor<2xf32>
Cost: 172

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 225

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%1021 = stablehlo.reduce(%1020 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 172

Operation name: stablehlo.subtract
cache miss
3818527485
%1022 = stablehlo.subtract %1016, %1018 : tensor<2xf32>
Cost: 523

Operation name: stablehlo.exponential
cache miss
3699945286
%1023 = stablehlo.exponential %1022 : tensor<2xf32>
Cost: 365

Operation name: stablehlo.slice
cache miss
2078395662
%1024 = stablehlo.slice %1023 [0:1] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 172

Operation name: stablehlo.reshape
cache miss
1900484018
%1025 = stablehlo.reshape %1024 : (tensor<1xf32>) -> tensor<f32>
Cost: 300

Operation name: stablehlo.slice
cache miss
4032994822
%1026 = stablehlo.slice %1023 [1:2] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 384

Operation name: stablehlo.reshape
cache miss
1900484018
%1027 = stablehlo.reshape %1026 : (tensor<1xf32>) -> tensor<f32>
Cost: 173

Operation name: stablehlo.add
cache miss
1937139428
%1028 = stablehlo.add %1025, %1027 : tensor<f32>
Cost: 364

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1029 = stablehlo.broadcast_in_dim %1028, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 261

Operation name: stablehlo.divide
cache miss
1531341356
%1030 = stablehlo.divide %1023, %1029 : tensor<2xf32>
Cost: 367

Operation name: stablehlo.multiply
cache miss
1284973678
%1031 = stablehlo.multiply %1028, %1028 : tensor<f32>
Cost: 450

Operation name: stablehlo.divide
cache miss
719102371
%1032 = stablehlo.divide %cst_7, %1031 : tensor<f32>
Cost: 333

Operation name: stablehlo.slice
cache miss
3339692416
%1033 = stablehlo.slice %931 [0:1, 144:192] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 386

Operation name: stablehlo.slice
cache miss
3339692416
%1034 = stablehlo.slice %932 [0:1, 144:192] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 367

Operation name: stablehlo.concatenate
cache miss
895440948
%1035 = stablehlo.concatenate %1033, %1034, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 372

Operation name: stablehlo.dot_general
cache miss
2902940993
%1036 = stablehlo.dot_general %1035, %1030, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<2xf32>) -> tensor<48xf32>
Cost: 377

Operation name: stablehlo.slice
cache miss
1801550340
%1037 = stablehlo.slice %926 [192:240] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 387

Operation name: stablehlo.slice
cache miss
3411559083
%1038 = stablehlo.slice %928 [0:1, 192:240] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 276

Operation name: stablehlo.slice
cache miss
3411559083
%1039 = stablehlo.slice %929 [0:1, 192:240] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 211

Operation name: stablehlo.concatenate
cache miss
895440948
%1040 = stablehlo.concatenate %1038, %1039, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 174

Operation name: stablehlo.dot_general
cache miss
1097263507
%1041 = stablehlo.dot_general %1040, %1037, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<48xf32>) -> tensor<2xf32>
Cost: 273

Operation name: stablehlo.divide
cache miss
1531341356
%1042 = stablehlo.divide %1041, %cst_1 : tensor<2xf32>
Cost: 412

Operation name: stablehlo.maximum
cache miss
641216813
%2733 = stablehlo.maximum %arg14, %arg15 : tensor<f32>
Cost: 369

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%1043 = stablehlo.reduce(%1042 init: %cst_3) applies stablehlo.maximum across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 402

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1044 = stablehlo.broadcast_in_dim %1043, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 282

Operation name: stablehlo.compare
Cost: 0

Operation name: stablehlo.convert
cache miss
1629337212
%1046 = stablehlo.convert %1045 : (tensor<2xi1>) -> tensor<2xf32>
Cost: 18380

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 362

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%1047 = stablehlo.reduce(%1046 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 361

Operation name: stablehlo.subtract
cache miss
3818527485
%1048 = stablehlo.subtract %1042, %1044 : tensor<2xf32>
Cost: 270

Operation name: stablehlo.exponential
cache miss
3699945286
%1049 = stablehlo.exponential %1048 : tensor<2xf32>
Cost: 358

Operation name: stablehlo.slice
cache miss
2078395662
%1050 = stablehlo.slice %1049 [0:1] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 271

Operation name: stablehlo.reshape
cache miss
1900484018
%1051 = stablehlo.reshape %1050 : (tensor<1xf32>) -> tensor<f32>
Cost: 172

Operation name: stablehlo.slice
cache miss
4032994822
%1052 = stablehlo.slice %1049 [1:2] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 291

Operation name: stablehlo.reshape
cache miss
1900484018
%1053 = stablehlo.reshape %1052 : (tensor<1xf32>) -> tensor<f32>
Cost: 370

Operation name: stablehlo.add
cache miss
1937139428
%1054 = stablehlo.add %1051, %1053 : tensor<f32>
Cost: 327

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1055 = stablehlo.broadcast_in_dim %1054, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 372

Operation name: stablehlo.divide
cache miss
1531341356
%1056 = stablehlo.divide %1049, %1055 : tensor<2xf32>
Cost: 364

Operation name: stablehlo.multiply
cache miss
1284973678
%1057 = stablehlo.multiply %1054, %1054 : tensor<f32>
Cost: 273

Operation name: stablehlo.divide
cache miss
719102371
%1058 = stablehlo.divide %cst_7, %1057 : tensor<f32>
Cost: 369

Operation name: stablehlo.slice
cache miss
3411559083
%1059 = stablehlo.slice %931 [0:1, 192:240] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 370

Operation name: stablehlo.slice
cache miss
3411559083
%1060 = stablehlo.slice %932 [0:1, 192:240] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 172

Operation name: stablehlo.concatenate
cache miss
895440948
%1061 = stablehlo.concatenate %1059, %1060, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 368

Operation name: stablehlo.dot_general
cache miss
2902940993
%1062 = stablehlo.dot_general %1061, %1056, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<2xf32>) -> tensor<48xf32>
Cost: 265

Operation name: stablehlo.slice
cache miss
2798019243
%1063 = stablehlo.slice %926 [240:288] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 553

Operation name: stablehlo.slice
cache miss
1619767244
%1064 = stablehlo.slice %928 [0:1, 240:288] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 173

Operation name: stablehlo.slice
cache miss
1619767244
%1065 = stablehlo.slice %929 [0:1, 240:288] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 369

Operation name: stablehlo.concatenate
cache miss
895440948
%1066 = stablehlo.concatenate %1064, %1065, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 502

Operation name: stablehlo.dot_general
cache miss
1097263507
%1067 = stablehlo.dot_general %1066, %1063, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<48xf32>) -> tensor<2xf32>
Cost: 388

Operation name: stablehlo.divide
cache miss
1531341356
%1068 = stablehlo.divide %1067, %cst_1 : tensor<2xf32>
Cost: 378

Operation name: stablehlo.maximum
cache miss
641216813
%2733 = stablehlo.maximum %arg14, %arg15 : tensor<f32>
Cost: 285

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%1069 = stablehlo.reduce(%1068 init: %cst_3) applies stablehlo.maximum across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 272

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1070 = stablehlo.broadcast_in_dim %1069, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 414

Operation name: stablehlo.compare
Cost: 0

Operation name: stablehlo.convert
cache miss
1629337212
%1072 = stablehlo.convert %1071 : (tensor<2xi1>) -> tensor<2xf32>
Cost: 276

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 270

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%1073 = stablehlo.reduce(%1072 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 271

Operation name: stablehlo.subtract
cache miss
3818527485
%1074 = stablehlo.subtract %1068, %1070 : tensor<2xf32>
Cost: 368

Operation name: stablehlo.exponential
cache miss
3699945286
%1075 = stablehlo.exponential %1074 : tensor<2xf32>
Cost: 405

Operation name: stablehlo.slice
cache miss
2078395662
%1076 = stablehlo.slice %1075 [0:1] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 274

Operation name: stablehlo.reshape
cache miss
1900484018
%1077 = stablehlo.reshape %1076 : (tensor<1xf32>) -> tensor<f32>
Cost: 309

Operation name: stablehlo.slice
cache miss
4032994822
%1078 = stablehlo.slice %1075 [1:2] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 362

Operation name: stablehlo.reshape
cache miss
1900484018
%1079 = stablehlo.reshape %1078 : (tensor<1xf32>) -> tensor<f32>
Cost: 369

Operation name: stablehlo.add
cache miss
1937139428
%1080 = stablehlo.add %1077, %1079 : tensor<f32>
Cost: 277

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1081 = stablehlo.broadcast_in_dim %1080, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 359

Operation name: stablehlo.divide
cache miss
1531341356
%1082 = stablehlo.divide %1075, %1081 : tensor<2xf32>
Cost: 271

Operation name: stablehlo.multiply
cache miss
1284973678
%1083 = stablehlo.multiply %1080, %1080 : tensor<f32>
Cost: 278

Operation name: stablehlo.divide
cache miss
719102371
%1084 = stablehlo.divide %cst_7, %1083 : tensor<f32>
Cost: 300

Operation name: stablehlo.slice
cache miss
1619767244
%1085 = stablehlo.slice %931 [0:1, 240:288] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 191

Operation name: stablehlo.slice
cache miss
1619767244
%1086 = stablehlo.slice %932 [0:1, 240:288] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 175

Operation name: stablehlo.concatenate
cache miss
895440948
%1087 = stablehlo.concatenate %1085, %1086, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 369

Operation name: stablehlo.dot_general
cache miss
2902940993
%1088 = stablehlo.dot_general %1087, %1082, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<2xf32>) -> tensor<48xf32>
Cost: 270

Operation name: stablehlo.concatenate
cache miss
292037604
%1089 = stablehlo.concatenate %958, %984, %1010, %1036, %1062, %1088, dim = 0 : (tensor<48xf32>, tensor<48xf32>, tensor<48xf32>, tensor<48xf32>, tensor<48xf32>, tensor<48xf32>) -> tensor<288xf32>
Cost: 389

Operation name: stablehlo.slice
cache miss
1763605157
%1090 = stablehlo.slice %arg8 [4:5, 0:288, 0:288] : (tensor<6x288x288xf32>) -> tensor<1x288x288xf32>
Cost: 51059

Operation name: stablehlo.reshape
cache miss
3219454195
%1091 = stablehlo.reshape %1090 : (tensor<1x288x288xf32>) -> tensor<288x288xf32>
Cost: 63769

Operation name: stablehlo.dot_general
cache miss
1061109412
%1092 = stablehlo.dot_general %1091, %1089, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288x288xf32>, tensor<288xf32>) -> tensor<288xf32>
Cost: 225

Operation name: stablehlo.add
cache miss
3847999983
%1093 = stablehlo.add %899, %1092 : tensor<288xf32>
Cost: 539

Operation name: stablehlo.slice
cache miss
1979475117
%1094 = stablehlo.slice %arg2 [4:5, 0:288] : (tensor<6x288xf32>) -> tensor<1x288xf32>
Cost: 546

Operation name: stablehlo.reshape
cache miss
1377738304
%1095 = stablehlo.reshape %1094 : (tensor<1x288xf32>) -> tensor<288xf32>
Cost: 532

Operation name: stablehlo.dot_general
cache miss
2778462547
%1096 = stablehlo.dot_general %1093, %1093, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288xf32>) -> tensor<f32>
Cost: 364

Operation name: stablehlo.divide
cache miss
719102371
%1097 = stablehlo.divide %1096, %cst_5 : tensor<f32>
Cost: 375

Operation name: stablehlo.add
cache miss
1937139428
%1098 = stablehlo.add %1097, %cst_4 : tensor<f32>
Cost: 374

Operation name: stablehlo.sqrt
cache miss
1884932271
%1099 = stablehlo.sqrt %1098 : tensor<f32>
Cost: 516

Operation name: stablehlo.divide
cache miss
719102371
%1100 = stablehlo.divide %cst_6, %1099 : tensor<f32>
Cost: 377

Operation name: stablehlo.divide
cache miss
719102371
%1101 = stablehlo.divide %cst_7, %1099 : tensor<f32>
Cost: 309

Operation name: stablehlo.multiply
cache miss
1284973678
%1102 = stablehlo.multiply %1099, %1099 : tensor<f32>
Cost: 332

Operation name: stablehlo.divide
cache miss
719102371
%1103 = stablehlo.divide %cst_7, %1102 : tensor<f32>
Cost: 295

Operation name: stablehlo.multiply
cache miss
2511610444
%1104 = stablehlo.multiply %1095, %1093 : tensor<288xf32>
Cost: 307

Operation name: stablehlo.broadcast_in_dim
cache miss
2616490109
%1105 = stablehlo.broadcast_in_dim %1101, dims = [] : (tensor<f32>) -> tensor<288xf32>
Cost: 512

Operation name: stablehlo.multiply
cache miss
2511610444
%1106 = stablehlo.multiply %1104, %1105 : tensor<288xf32>
Cost: 403

Operation name: stablehlo.slice
cache miss
550082050
%1107 = stablehlo.slice %arg4 [4:5, 0:768, 0:288] : (tensor<6x768x288xf32>) -> tensor<1x768x288xf32>
Cost: 94072

Operation name: stablehlo.reshape
cache miss
3265769957
%1108 = stablehlo.reshape %1107 : (tensor<1x768x288xf32>) -> tensor<768x288xf32>
Cost: 76617

Operation name: stablehlo.dot_general
cache miss
2529236298
%1109 = stablehlo.dot_general %1108, %1106, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<768x288xf32>, tensor<288xf32>) -> tensor<768xf32>
Cost: 3582

Operation name: stablehlo.slice
cache miss
550082050
%1110 = stablehlo.slice %arg6 [4:5, 0:768, 0:288] : (tensor<6x768x288xf32>) -> tensor<1x768x288xf32>
Cost: 101257

Operation name: stablehlo.reshape
cache miss
3265769957
%1111 = stablehlo.reshape %1110 : (tensor<1x768x288xf32>) -> tensor<768x288xf32>
Cost: 111619

Operation name: stablehlo.dot_general
cache miss
2529236298
%1112 = stablehlo.dot_general %1111, %1106, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<768x288xf32>, tensor<288xf32>) -> tensor<768xf32>
Cost: 6275

Operation name: stablehlo.negate
cache miss
4256221255
%1113 = stablehlo.negate %1109 : tensor<768xf32>
Cost: 583

Operation name: stablehlo.exponential
cache miss
6921514
%1114 = stablehlo.exponential %1113 : tensor<768xf32>
Cost: 554

Operation name: stablehlo.add
cache miss
389450582
%1115 = stablehlo.add %cst_0, %1114 : tensor<768xf32>
Cost: 607

Operation name: stablehlo.divide
cache miss
2134516102
%1116 = stablehlo.divide %cst_0, %1115 : tensor<768xf32>
Cost: 457

Operation name: stablehlo.multiply
cache miss
4137946040
%1117 = stablehlo.multiply %1115, %1115 : tensor<768xf32>
Cost: 445

Operation name: stablehlo.divide
cache miss
2134516102
%1118 = stablehlo.divide %cst_0, %1117 : tensor<768xf32>
Cost: 419

Operation name: stablehlo.multiply
cache miss
4137946040
%1119 = stablehlo.multiply %1109, %1116 : tensor<768xf32>
Cost: 335

Operation name: stablehlo.multiply
cache miss
4137946040
%1120 = stablehlo.multiply %1119, %1112 : tensor<768xf32>
Cost: 633

Operation name: stablehlo.slice
cache miss
1224611605
%1121 = stablehlo.slice %arg5 [4:5, 0:288, 0:768] : (tensor<6x288x768xf32>) -> tensor<1x288x768xf32>
Cost: 108633

Operation name: stablehlo.reshape
cache miss
2162077039
%1122 = stablehlo.reshape %1121 : (tensor<1x288x768xf32>) -> tensor<288x768xf32>
Cost: 105220

Operation name: stablehlo.dot_general
cache miss
1061109412
%1123 = stablehlo.dot_general %1122, %1120, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288x768xf32>, tensor<768xf32>) -> tensor<288xf32>
Cost: 3854

Operation name: stablehlo.add
cache miss
3847999983
%1124 = stablehlo.add %1093, %1123 : tensor<288xf32>
Cost: 403

Operation name: stablehlo.slice
cache miss
2420989316
%1125 = stablehlo.slice %arg1 [5:6, 0:288] : (tensor<6x288xf32>) -> tensor<1x288xf32>
Cost: 262

Operation name: stablehlo.reshape
cache miss
1377738304
%1126 = stablehlo.reshape %1125 : (tensor<1x288xf32>) -> tensor<288xf32>
Cost: 441

Operation name: stablehlo.dot_general
cache miss
2778462547
%1127 = stablehlo.dot_general %1124, %1124, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288xf32>) -> tensor<f32>
Cost: 278

Operation name: stablehlo.divide
cache miss
719102371
%1128 = stablehlo.divide %1127, %cst_5 : tensor<f32>
Cost: 371

Operation name: stablehlo.add
cache miss
1937139428
%1129 = stablehlo.add %1128, %cst_4 : tensor<f32>
Cost: 287

Operation name: stablehlo.sqrt
cache miss
1884932271
%1130 = stablehlo.sqrt %1129 : tensor<f32>
Cost: 281

Operation name: stablehlo.divide
cache miss
719102371
%1131 = stablehlo.divide %cst_6, %1130 : tensor<f32>
Cost: 284

Operation name: stablehlo.divide
cache miss
719102371
%1132 = stablehlo.divide %cst_7, %1130 : tensor<f32>
Cost: 283

Operation name: stablehlo.multiply
cache miss
1284973678
%1133 = stablehlo.multiply %1130, %1130 : tensor<f32>
Cost: 284

Operation name: stablehlo.divide
cache miss
719102371
%1134 = stablehlo.divide %cst_7, %1133 : tensor<f32>
Cost: 178

Operation name: stablehlo.multiply
cache miss
2511610444
%1135 = stablehlo.multiply %1126, %1124 : tensor<288xf32>
Cost: 524

Operation name: stablehlo.broadcast_in_dim
cache miss
2616490109
%1136 = stablehlo.broadcast_in_dim %1132, dims = [] : (tensor<f32>) -> tensor<288xf32>
Cost: 402

Operation name: stablehlo.multiply
cache miss
2511610444
%1137 = stablehlo.multiply %1135, %1136 : tensor<288xf32>
Cost: 293

Operation name: stablehlo.slice
cache miss
4088799856
%1138 = stablehlo.slice %arg9 [5:6, 0:288, 0:288] : (tensor<6x288x288xf32>) -> tensor<1x288x288xf32>
Cost: 44588

Operation name: stablehlo.reshape
cache miss
3219454195
%1139 = stablehlo.reshape %1138 : (tensor<1x288x288xf32>) -> tensor<288x288xf32>
Cost: 44038

Operation name: stablehlo.dot_general
cache miss
1061109412
%1140 = stablehlo.dot_general %1139, %1137, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288x288xf32>, tensor<288xf32>) -> tensor<288xf32>
Cost: 494

Operation name: stablehlo.slice
cache miss
4088799856
%1141 = stablehlo.slice %arg7 [5:6, 0:288, 0:288] : (tensor<6x288x288xf32>) -> tensor<1x288x288xf32>
Cost: 50948

Operation name: stablehlo.reshape
cache miss
3219454195
%1142 = stablehlo.reshape %1141 : (tensor<1x288x288xf32>) -> tensor<288x288xf32>
Cost: 24558

Operation name: stablehlo.dot_general
cache miss
1061109412
%1143 = stablehlo.dot_general %1142, %1137, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288x288xf32>, tensor<288xf32>) -> tensor<288xf32>
Cost: 462

Operation name: stablehlo.slice
cache miss
4088799856
%1144 = stablehlo.slice %arg10 [5:6, 0:288, 0:288] : (tensor<6x288x288xf32>) -> tensor<1x288x288xf32>
Cost: 15086

Operation name: stablehlo.reshape
cache miss
3219454195
%1145 = stablehlo.reshape %1144 : (tensor<1x288x288xf32>) -> tensor<288x288xf32>
Cost: 25165

Operation name: stablehlo.dot_general
cache miss
1061109412
%1146 = stablehlo.dot_general %1145, %1137, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288x288xf32>, tensor<288xf32>) -> tensor<288xf32>
Cost: 226

Operation name: stablehlo.reshape
cache miss
2392077026
%1147 = stablehlo.reshape %1140 : (tensor<288xf32>) -> tensor<144x2xf32>
Cost: 416

Operation name: stablehlo.reshape
cache miss
2392077026
%1148 = stablehlo.reshape %1143 : (tensor<288xf32>) -> tensor<144x2xf32>
Cost: 544

Operation name: stablehlo.dot_general
cache miss
865228849
%1149 = stablehlo.dot_general %cst_8, %1148, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<144x2x2xf32>, tensor<144x2xf32>) -> tensor<144x2xf32>
Cost: 629

Operation name: stablehlo.dot_general
cache miss
865228849
%1150 = stablehlo.dot_general %cst_8, %1147, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<144x2x2xf32>, tensor<144x2xf32>) -> tensor<144x2xf32>
Cost: 438

Operation name: stablehlo.reshape
cache miss
1377738304
%1151 = stablehlo.reshape %1150 : (tensor<144x2xf32>) -> tensor<288xf32>
Cost: 510

Operation name: stablehlo.slice
cache miss
4017416395
%1152 = stablehlo.slice %arg11 [5:6, 0:1, 0:288] : (tensor<6x1x288xf32>) -> tensor<1x1x288xf32>
Cost: 572

Operation name: stablehlo.reshape
cache miss
2960881163
%1153 = stablehlo.reshape %1152 : (tensor<1x1x288xf32>) -> tensor<1x288xf32>
Cost: 457

Operation name: stablehlo.reshape
cache miss
2960881163
%1154 = stablehlo.reshape %1149 : (tensor<144x2xf32>) -> tensor<1x288xf32>
Cost: 426

Operation name: stablehlo.slice
cache miss
4017416395
%1155 = stablehlo.slice %arg12 [5:6, 0:1, 0:288] : (tensor<6x1x288xf32>) -> tensor<1x1x288xf32>
Cost: 523

Operation name: stablehlo.reshape
cache miss
2960881163
%1156 = stablehlo.reshape %1155 : (tensor<1x1x288xf32>) -> tensor<1x288xf32>
Cost: 546

Operation name: stablehlo.reshape
cache miss
2960881163
%1157 = stablehlo.reshape %1146 : (tensor<288xf32>) -> tensor<1x288xf32>
Cost: 405

Operation name: stablehlo.slice
cache miss
2839717066
%1158 = stablehlo.slice %1151 [0:48] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 277

Operation name: stablehlo.slice
cache miss
3729170049
%1159 = stablehlo.slice %1153 [0:1, 0:48] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 389

Operation name: stablehlo.slice
cache miss
3729170049
%1160 = stablehlo.slice %1154 [0:1, 0:48] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 176

Operation name: stablehlo.concatenate
cache miss
895440948
%1161 = stablehlo.concatenate %1159, %1160, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 282

Operation name: stablehlo.dot_general
cache miss
1097263507
%1162 = stablehlo.dot_general %1161, %1158, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<48xf32>) -> tensor<2xf32>
Cost: 184

Operation name: stablehlo.divide
cache miss
1531341356
%1163 = stablehlo.divide %1162, %cst_1 : tensor<2xf32>
Cost: 286

Operation name: stablehlo.maximum
cache miss
641216813
%2733 = stablehlo.maximum %arg14, %arg15 : tensor<f32>
Cost: 307

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%1164 = stablehlo.reduce(%1163 init: %cst_3) applies stablehlo.maximum across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 478

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1165 = stablehlo.broadcast_in_dim %1164, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 366

Operation name: stablehlo.compare
Cost: 0

Operation name: stablehlo.convert
cache miss
1629337212
%1167 = stablehlo.convert %1166 : (tensor<2xi1>) -> tensor<2xf32>
Cost: 295

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 303

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%1168 = stablehlo.reduce(%1167 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 407

Operation name: stablehlo.subtract
cache miss
3818527485
%1169 = stablehlo.subtract %1163, %1165 : tensor<2xf32>
Cost: 323

Operation name: stablehlo.exponential
cache miss
3699945286
%1170 = stablehlo.exponential %1169 : tensor<2xf32>
Cost: 285

Operation name: stablehlo.slice
cache miss
2078395662
%1171 = stablehlo.slice %1170 [0:1] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 502

Operation name: stablehlo.reshape
cache miss
1900484018
%1172 = stablehlo.reshape %1171 : (tensor<1xf32>) -> tensor<f32>
Cost: 388

Operation name: stablehlo.slice
cache miss
4032994822
%1173 = stablehlo.slice %1170 [1:2] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 308

Operation name: stablehlo.reshape
cache miss
1900484018
%1174 = stablehlo.reshape %1173 : (tensor<1xf32>) -> tensor<f32>
Cost: 178

Operation name: stablehlo.add
cache miss
1937139428
%1175 = stablehlo.add %1172, %1174 : tensor<f32>
Cost: 279

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1176 = stablehlo.broadcast_in_dim %1175, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 378

Operation name: stablehlo.divide
cache miss
1531341356
%1177 = stablehlo.divide %1170, %1176 : tensor<2xf32>
Cost: 383

Operation name: stablehlo.multiply
cache miss
1284973678
%1178 = stablehlo.multiply %1175, %1175 : tensor<f32>
Cost: 181

Operation name: stablehlo.divide
cache miss
719102371
%1179 = stablehlo.divide %cst_7, %1178 : tensor<f32>
Cost: 609

Operation name: stablehlo.slice
cache miss
3729170049
%1180 = stablehlo.slice %1156 [0:1, 0:48] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 374

Operation name: stablehlo.slice
cache miss
3729170049
%1181 = stablehlo.slice %1157 [0:1, 0:48] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 280

Operation name: stablehlo.concatenate
cache miss
895440948
%1182 = stablehlo.concatenate %1180, %1181, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 370

Operation name: stablehlo.dot_general
cache miss
2902940993
%1183 = stablehlo.dot_general %1182, %1177, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<2xf32>) -> tensor<48xf32>
Cost: 393

Operation name: stablehlo.slice
cache miss
4032389970
%1184 = stablehlo.slice %1151 [48:96] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 286

Operation name: stablehlo.slice
cache miss
3581210888
%1185 = stablehlo.slice %1153 [0:1, 48:96] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 372

Operation name: stablehlo.slice
cache miss
3581210888
%1186 = stablehlo.slice %1154 [0:1, 48:96] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 188

Operation name: stablehlo.concatenate
cache miss
895440948
%1187 = stablehlo.concatenate %1185, %1186, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 288

Operation name: stablehlo.dot_general
cache miss
1097263507
%1188 = stablehlo.dot_general %1187, %1184, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<48xf32>) -> tensor<2xf32>
Cost: 308

Operation name: stablehlo.divide
cache miss
1531341356
%1189 = stablehlo.divide %1188, %cst_1 : tensor<2xf32>
Cost: 514

Operation name: stablehlo.maximum
cache miss
641216813
%2733 = stablehlo.maximum %arg14, %arg15 : tensor<f32>
Cost: 202

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%1190 = stablehlo.reduce(%1189 init: %cst_3) applies stablehlo.maximum across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 401

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1191 = stablehlo.broadcast_in_dim %1190, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 380

Operation name: stablehlo.compare
Cost: 0

Operation name: stablehlo.convert
cache miss
1629337212
%1193 = stablehlo.convert %1192 : (tensor<2xi1>) -> tensor<2xf32>
Cost: 288

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 274

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%1194 = stablehlo.reduce(%1193 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 316

Operation name: stablehlo.subtract
cache miss
3818527485
%1195 = stablehlo.subtract %1189, %1191 : tensor<2xf32>
Cost: 365

Operation name: stablehlo.exponential
cache miss
3699945286
%1196 = stablehlo.exponential %1195 : tensor<2xf32>
Cost: 369

Operation name: stablehlo.slice
cache miss
2078395662
%1197 = stablehlo.slice %1196 [0:1] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 363

Operation name: stablehlo.reshape
cache miss
1900484018
%1198 = stablehlo.reshape %1197 : (tensor<1xf32>) -> tensor<f32>
Cost: 397

Operation name: stablehlo.slice
cache miss
4032994822
%1199 = stablehlo.slice %1196 [1:2] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 271

Operation name: stablehlo.reshape
cache miss
1900484018
%1200 = stablehlo.reshape %1199 : (tensor<1xf32>) -> tensor<f32>
Cost: 170

Operation name: stablehlo.add
cache miss
1937139428
%1201 = stablehlo.add %1198, %1200 : tensor<f32>
Cost: 365

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1202 = stablehlo.broadcast_in_dim %1201, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 406

Operation name: stablehlo.divide
cache miss
1531341356
%1203 = stablehlo.divide %1196, %1202 : tensor<2xf32>
Cost: 276

Operation name: stablehlo.multiply
cache miss
1284973678
%1204 = stablehlo.multiply %1201, %1201 : tensor<f32>
Cost: 369

Operation name: stablehlo.divide
cache miss
719102371
%1205 = stablehlo.divide %cst_7, %1204 : tensor<f32>
Cost: 273

Operation name: stablehlo.slice
cache miss
3581210888
%1206 = stablehlo.slice %1156 [0:1, 48:96] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 390

Operation name: stablehlo.slice
cache miss
3581210888
%1207 = stablehlo.slice %1157 [0:1, 48:96] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 191

Operation name: stablehlo.concatenate
cache miss
895440948
%1208 = stablehlo.concatenate %1206, %1207, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 279

Operation name: stablehlo.dot_general
cache miss
2902940993
%1209 = stablehlo.dot_general %1208, %1203, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<2xf32>) -> tensor<48xf32>
Cost: 170

Operation name: stablehlo.slice
cache miss
1375222232
%1210 = stablehlo.slice %1151 [96:144] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 303

Operation name: stablehlo.slice
cache miss
1957506058
%1211 = stablehlo.slice %1153 [0:1, 96:144] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 276

Operation name: stablehlo.slice
cache miss
1957506058
%1212 = stablehlo.slice %1154 [0:1, 96:144] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 368

Operation name: stablehlo.concatenate
cache miss
895440948
%1213 = stablehlo.concatenate %1211, %1212, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 283

Operation name: stablehlo.dot_general
cache miss
1097263507
%1214 = stablehlo.dot_general %1213, %1210, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<48xf32>) -> tensor<2xf32>
Cost: 300

Operation name: stablehlo.divide
cache miss
1531341356
%1215 = stablehlo.divide %1214, %cst_1 : tensor<2xf32>
Cost: 395

Operation name: stablehlo.maximum
cache miss
641216813
%2733 = stablehlo.maximum %arg14, %arg15 : tensor<f32>
Cost: 476

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%1216 = stablehlo.reduce(%1215 init: %cst_3) applies stablehlo.maximum across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 273

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1217 = stablehlo.broadcast_in_dim %1216, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 414

Operation name: stablehlo.compare
Cost: 0

Operation name: stablehlo.convert
cache miss
1629337212
%1219 = stablehlo.convert %1218 : (tensor<2xi1>) -> tensor<2xf32>
Cost: 174

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 300

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%1220 = stablehlo.reduce(%1219 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 185

Operation name: stablehlo.subtract
cache miss
3818527485
%1221 = stablehlo.subtract %1215, %1217 : tensor<2xf32>
Cost: 363

Operation name: stablehlo.exponential
cache miss
3699945286
%1222 = stablehlo.exponential %1221 : tensor<2xf32>
Cost: 360

Operation name: stablehlo.slice
cache miss
2078395662
%1223 = stablehlo.slice %1222 [0:1] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 387

Operation name: stablehlo.reshape
cache miss
1900484018
%1224 = stablehlo.reshape %1223 : (tensor<1xf32>) -> tensor<f32>
Cost: 272

Operation name: stablehlo.slice
cache miss
4032994822
%1225 = stablehlo.slice %1222 [1:2] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 303

Operation name: stablehlo.reshape
cache miss
1900484018
%1226 = stablehlo.reshape %1225 : (tensor<1xf32>) -> tensor<f32>
Cost: 366

Operation name: stablehlo.add
cache miss
1937139428
%1227 = stablehlo.add %1224, %1226 : tensor<f32>
Cost: 363

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1228 = stablehlo.broadcast_in_dim %1227, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 276

Operation name: stablehlo.divide
cache miss
1531341356
%1229 = stablehlo.divide %1222, %1228 : tensor<2xf32>
Cost: 365

Operation name: stablehlo.multiply
cache miss
1284973678
%1230 = stablehlo.multiply %1227, %1227 : tensor<f32>
Cost: 323

Operation name: stablehlo.divide
cache miss
719102371
%1231 = stablehlo.divide %cst_7, %1230 : tensor<f32>
Cost: 391

Operation name: stablehlo.slice
cache miss
1957506058
%1232 = stablehlo.slice %1156 [0:1, 96:144] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 375

Operation name: stablehlo.slice
cache miss
1957506058
%1233 = stablehlo.slice %1157 [0:1, 96:144] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 280

Operation name: stablehlo.concatenate
cache miss
895440948
%1234 = stablehlo.concatenate %1232, %1233, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 378

Operation name: stablehlo.dot_general
cache miss
2902940993
%1235 = stablehlo.dot_general %1234, %1229, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<2xf32>) -> tensor<48xf32>
Cost: 358

Operation name: stablehlo.slice
cache miss
3469783879
%1236 = stablehlo.slice %1151 [144:192] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 499

Operation name: stablehlo.slice
cache miss
3339692416
%1237 = stablehlo.slice %1153 [0:1, 144:192] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 302

Operation name: stablehlo.slice
cache miss
3339692416
%1238 = stablehlo.slice %1154 [0:1, 144:192] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 370

Operation name: stablehlo.concatenate
cache miss
895440948
%1239 = stablehlo.concatenate %1237, %1238, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 371

Operation name: stablehlo.dot_general
cache miss
1097263507
%1240 = stablehlo.dot_general %1239, %1236, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<48xf32>) -> tensor<2xf32>
Cost: 393

Operation name: stablehlo.divide
cache miss
1531341356
%1241 = stablehlo.divide %1240, %cst_1 : tensor<2xf32>
Cost: 282

Operation name: stablehlo.maximum
cache miss
641216813
%2733 = stablehlo.maximum %arg14, %arg15 : tensor<f32>
Cost: 481

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%1242 = stablehlo.reduce(%1241 init: %cst_3) applies stablehlo.maximum across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 173

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1243 = stablehlo.broadcast_in_dim %1242, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 254

Operation name: stablehlo.compare
Cost: 0

Operation name: stablehlo.convert
cache miss
1629337212
%1245 = stablehlo.convert %1244 : (tensor<2xi1>) -> tensor<2xf32>
Cost: 305

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 370

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%1246 = stablehlo.reduce(%1245 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 367

Operation name: stablehlo.subtract
cache miss
3818527485
%1247 = stablehlo.subtract %1241, %1243 : tensor<2xf32>
Cost: 511

Operation name: stablehlo.exponential
cache miss
3699945286
%1248 = stablehlo.exponential %1247 : tensor<2xf32>
Cost: 364

Operation name: stablehlo.slice
cache miss
2078395662
%1249 = stablehlo.slice %1248 [0:1] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 175

Operation name: stablehlo.reshape
cache miss
1900484018
%1250 = stablehlo.reshape %1249 : (tensor<1xf32>) -> tensor<f32>
Cost: 369

Operation name: stablehlo.slice
cache miss
4032994822
%1251 = stablehlo.slice %1248 [1:2] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 362

Operation name: stablehlo.reshape
cache miss
1900484018
%1252 = stablehlo.reshape %1251 : (tensor<1xf32>) -> tensor<f32>
Cost: 389

Operation name: stablehlo.add
cache miss
1937139428
%1253 = stablehlo.add %1250, %1252 : tensor<f32>
Cost: 364

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1254 = stablehlo.broadcast_in_dim %1253, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 385

Operation name: stablehlo.divide
cache miss
1531341356
%1255 = stablehlo.divide %1248, %1254 : tensor<2xf32>
Cost: 366

Operation name: stablehlo.multiply
cache miss
1284973678
%1256 = stablehlo.multiply %1253, %1253 : tensor<f32>
Cost: 304

Operation name: stablehlo.divide
cache miss
719102371
%1257 = stablehlo.divide %cst_7, %1256 : tensor<f32>
Cost: 562

Operation name: stablehlo.slice
cache miss
3339692416
%1258 = stablehlo.slice %1156 [0:1, 144:192] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 390

Operation name: stablehlo.slice
cache miss
3339692416
%1259 = stablehlo.slice %1157 [0:1, 144:192] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 393

Operation name: stablehlo.concatenate
cache miss
895440948
%1260 = stablehlo.concatenate %1258, %1259, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 397

Operation name: stablehlo.dot_general
cache miss
2902940993
%1261 = stablehlo.dot_general %1260, %1255, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<2xf32>) -> tensor<48xf32>
Cost: 354

Operation name: stablehlo.slice
cache miss
1801550340
%1262 = stablehlo.slice %1151 [192:240] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 365

Operation name: stablehlo.slice
cache miss
3411559083
%1263 = stablehlo.slice %1153 [0:1, 192:240] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 301

Operation name: stablehlo.slice
cache miss
3411559083
%1264 = stablehlo.slice %1154 [0:1, 192:240] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 277

Operation name: stablehlo.concatenate
cache miss
895440948
%1265 = stablehlo.concatenate %1263, %1264, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 328

Operation name: stablehlo.dot_general
cache miss
1097263507
%1266 = stablehlo.dot_general %1265, %1262, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<48xf32>) -> tensor<2xf32>
Cost: 384

Operation name: stablehlo.divide
cache miss
1531341356
%1267 = stablehlo.divide %1266, %cst_1 : tensor<2xf32>
Cost: 312

Operation name: stablehlo.maximum
cache miss
641216813
%2733 = stablehlo.maximum %arg14, %arg15 : tensor<f32>
Cost: 471

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%1268 = stablehlo.reduce(%1267 init: %cst_3) applies stablehlo.maximum across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 366

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1269 = stablehlo.broadcast_in_dim %1268, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 275

Operation name: stablehlo.compare
Cost: 0

Operation name: stablehlo.convert
cache miss
1629337212
%1271 = stablehlo.convert %1270 : (tensor<2xi1>) -> tensor<2xf32>
Cost: 340

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 175

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%1272 = stablehlo.reduce(%1271 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 384

Operation name: stablehlo.subtract
cache miss
3818527485
%1273 = stablehlo.subtract %1267, %1269 : tensor<2xf32>
Cost: 383

Operation name: stablehlo.exponential
cache miss
3699945286
%1274 = stablehlo.exponential %1273 : tensor<2xf32>
Cost: 362

Operation name: stablehlo.slice
cache miss
2078395662
%1275 = stablehlo.slice %1274 [0:1] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 269

Operation name: stablehlo.reshape
cache miss
1900484018
%1276 = stablehlo.reshape %1275 : (tensor<1xf32>) -> tensor<f32>
Cost: 301

Operation name: stablehlo.slice
cache miss
4032994822
%1277 = stablehlo.slice %1274 [1:2] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 299

Operation name: stablehlo.reshape
cache miss
1900484018
%1278 = stablehlo.reshape %1277 : (tensor<1xf32>) -> tensor<f32>
Cost: 319

Operation name: stablehlo.add
cache miss
1937139428
%1279 = stablehlo.add %1276, %1278 : tensor<f32>
Cost: 300

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1280 = stablehlo.broadcast_in_dim %1279, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 174

Operation name: stablehlo.divide
cache miss
1531341356
%1281 = stablehlo.divide %1274, %1280 : tensor<2xf32>
Cost: 170

Operation name: stablehlo.multiply
cache miss
1284973678
%1282 = stablehlo.multiply %1279, %1279 : tensor<f32>
Cost: 389

Operation name: stablehlo.divide
cache miss
719102371
%1283 = stablehlo.divide %cst_7, %1282 : tensor<f32>
Cost: 270

Operation name: stablehlo.slice
cache miss
3411559083
%1284 = stablehlo.slice %1156 [0:1, 192:240] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 176

Operation name: stablehlo.slice
cache miss
3411559083
%1285 = stablehlo.slice %1157 [0:1, 192:240] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 468

Operation name: stablehlo.concatenate
cache miss
895440948
%1286 = stablehlo.concatenate %1284, %1285, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 179

Operation name: stablehlo.dot_general
cache miss
2902940993
%1287 = stablehlo.dot_general %1286, %1281, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<2xf32>) -> tensor<48xf32>
Cost: 293

Operation name: stablehlo.slice
cache miss
2798019243
%1288 = stablehlo.slice %1151 [240:288] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 305

Operation name: stablehlo.slice
cache miss
1619767244
%1289 = stablehlo.slice %1153 [0:1, 240:288] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 173

Operation name: stablehlo.slice
cache miss
1619767244
%1290 = stablehlo.slice %1154 [0:1, 240:288] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 176

Operation name: stablehlo.concatenate
cache miss
895440948
%1291 = stablehlo.concatenate %1289, %1290, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 372

Operation name: stablehlo.dot_general
cache miss
1097263507
%1292 = stablehlo.dot_general %1291, %1288, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<48xf32>) -> tensor<2xf32>
Cost: 300

Operation name: stablehlo.divide
cache miss
1531341356
%1293 = stablehlo.divide %1292, %cst_1 : tensor<2xf32>
Cost: 308

Operation name: stablehlo.maximum
cache miss
641216813
%2733 = stablehlo.maximum %arg14, %arg15 : tensor<f32>
Cost: 364

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%1294 = stablehlo.reduce(%1293 init: %cst_3) applies stablehlo.maximum across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 221

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1295 = stablehlo.broadcast_in_dim %1294, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 364

Operation name: stablehlo.compare
Cost: 0

Operation name: stablehlo.convert
cache miss
1629337212
%1297 = stablehlo.convert %1296 : (tensor<2xi1>) -> tensor<2xf32>
Cost: 308

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 390

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%1298 = stablehlo.reduce(%1297 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 273

Operation name: stablehlo.subtract
cache miss
3818527485
%1299 = stablehlo.subtract %1293, %1295 : tensor<2xf32>
Cost: 271

Operation name: stablehlo.exponential
cache miss
3699945286
%1300 = stablehlo.exponential %1299 : tensor<2xf32>
Cost: 174

Operation name: stablehlo.slice
cache miss
2078395662
%1301 = stablehlo.slice %1300 [0:1] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 299

Operation name: stablehlo.reshape
cache miss
1900484018
%1302 = stablehlo.reshape %1301 : (tensor<1xf32>) -> tensor<f32>
Cost: 318

Operation name: stablehlo.slice
cache miss
4032994822
%1303 = stablehlo.slice %1300 [1:2] : (tensor<2xf32>) -> tensor<1xf32>
Cost: 299

Operation name: stablehlo.reshape
cache miss
1900484018
%1304 = stablehlo.reshape %1303 : (tensor<1xf32>) -> tensor<f32>
Cost: 272

Operation name: stablehlo.add
cache miss
1937139428
%1305 = stablehlo.add %1302, %1304 : tensor<f32>
Cost: 333

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1306 = stablehlo.broadcast_in_dim %1305, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 394

Operation name: stablehlo.divide
cache miss
1531341356
%1307 = stablehlo.divide %1300, %1306 : tensor<2xf32>
Cost: 274

Operation name: stablehlo.multiply
cache miss
1284973678
%1308 = stablehlo.multiply %1305, %1305 : tensor<f32>
Cost: 279

Operation name: stablehlo.divide
cache miss
719102371
%1309 = stablehlo.divide %cst_7, %1308 : tensor<f32>
Cost: 363

Operation name: stablehlo.slice
cache miss
1619767244
%1310 = stablehlo.slice %1156 [0:1, 240:288] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 175

Operation name: stablehlo.slice
cache miss
1619767244
%1311 = stablehlo.slice %1157 [0:1, 240:288] : (tensor<1x288xf32>) -> tensor<1x48xf32>
Cost: 275

Operation name: stablehlo.concatenate
cache miss
895440948
%1312 = stablehlo.concatenate %1310, %1311, dim = 0 : (tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<2x48xf32>
Cost: 280

Operation name: stablehlo.dot_general
cache miss
2902940993
%1313 = stablehlo.dot_general %1312, %1307, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x48xf32>, tensor<2xf32>) -> tensor<48xf32>
Cost: 167

Operation name: stablehlo.concatenate
cache miss
292037604
%1314 = stablehlo.concatenate %1183, %1209, %1235, %1261, %1287, %1313, dim = 0 : (tensor<48xf32>, tensor<48xf32>, tensor<48xf32>, tensor<48xf32>, tensor<48xf32>, tensor<48xf32>) -> tensor<288xf32>
Cost: 575

Operation name: stablehlo.slice
cache miss
4088799856
%1315 = stablehlo.slice %arg8 [5:6, 0:288, 0:288] : (tensor<6x288x288xf32>) -> tensor<1x288x288xf32>
Cost: 24380

Operation name: stablehlo.reshape
cache miss
3219454195
%1316 = stablehlo.reshape %1315 : (tensor<1x288x288xf32>) -> tensor<288x288xf32>
Cost: 15839

Operation name: stablehlo.dot_general
cache miss
1061109412
%1317 = stablehlo.dot_general %1316, %1314, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288x288xf32>, tensor<288xf32>) -> tensor<288xf32>
Cost: 429

Operation name: stablehlo.add
cache miss
3847999983
%1318 = stablehlo.add %1124, %1317 : tensor<288xf32>
Cost: 284

Operation name: stablehlo.slice
cache miss
2420989316
%1319 = stablehlo.slice %arg2 [5:6, 0:288] : (tensor<6x288xf32>) -> tensor<1x288xf32>
Cost: 1440

Operation name: stablehlo.reshape
cache miss
1377738304
%1320 = stablehlo.reshape %1319 : (tensor<1x288xf32>) -> tensor<288xf32>
Cost: 600

Operation name: stablehlo.dot_general
cache miss
2778462547
%1321 = stablehlo.dot_general %1318, %1318, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288xf32>) -> tensor<f32>
Cost: 291

Operation name: stablehlo.divide
cache miss
719102371
%1322 = stablehlo.divide %1321, %cst_5 : tensor<f32>
Cost: 280

Operation name: stablehlo.add
cache miss
1937139428
%1323 = stablehlo.add %1322, %cst_4 : tensor<f32>
Cost: 281

Operation name: stablehlo.sqrt
cache miss
1884932271
%1324 = stablehlo.sqrt %1323 : tensor<f32>
Cost: 372

Operation name: stablehlo.divide
cache miss
719102371
%1325 = stablehlo.divide %cst_6, %1324 : tensor<f32>
Cost: 376

Operation name: stablehlo.divide
cache miss
719102371
%1326 = stablehlo.divide %cst_7, %1324 : tensor<f32>
Cost: 180

Operation name: stablehlo.multiply
cache miss
1284973678
%1327 = stablehlo.multiply %1324, %1324 : tensor<f32>
Cost: 487

Operation name: stablehlo.divide
cache miss
719102371
%1328 = stablehlo.divide %cst_7, %1327 : tensor<f32>
Cost: 174

Operation name: stablehlo.multiply
cache miss
2511610444
%1329 = stablehlo.multiply %1320, %1318 : tensor<288xf32>
Cost: 480

Operation name: stablehlo.broadcast_in_dim
cache miss
2616490109
%1330 = stablehlo.broadcast_in_dim %1326, dims = [] : (tensor<f32>) -> tensor<288xf32>
Cost: 280

Operation name: stablehlo.multiply
cache miss
2511610444
%1331 = stablehlo.multiply %1329, %1330 : tensor<288xf32>
Cost: 576

Operation name: stablehlo.slice
cache miss
545426124
%1332 = stablehlo.slice %arg4 [5:6, 0:768, 0:288] : (tensor<6x768x288xf32>) -> tensor<1x768x288xf32>
Cost: 46945

Operation name: stablehlo.reshape
cache miss
3265769957
%1333 = stablehlo.reshape %1332 : (tensor<1x768x288xf32>) -> tensor<768x288xf32>
Cost: 43294

Operation name: stablehlo.dot_general
cache miss
2529236298
%1334 = stablehlo.dot_general %1333, %1331, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<768x288xf32>, tensor<288xf32>) -> tensor<768xf32>
Cost: 4604

Operation name: stablehlo.slice
cache miss
545426124
%1335 = stablehlo.slice %arg6 [5:6, 0:768, 0:288] : (tensor<6x768x288xf32>) -> tensor<1x768x288xf32>
Cost: 48379

Operation name: stablehlo.reshape
cache miss
3265769957
%1336 = stablehlo.reshape %1335 : (tensor<1x768x288xf32>) -> tensor<768x288xf32>
Cost: 44121

Operation name: stablehlo.dot_general
cache miss
2529236298
%1337 = stablehlo.dot_general %1336, %1331, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<768x288xf32>, tensor<288xf32>) -> tensor<768xf32>
Cost: 6337

Operation name: stablehlo.negate
cache miss
4256221255
%1338 = stablehlo.negate %1334 : tensor<768xf32>
Cost: 443

Operation name: stablehlo.exponential
cache miss
6921514
%1339 = stablehlo.exponential %1338 : tensor<768xf32>
Cost: 278

Operation name: stablehlo.add
cache miss
389450582
%1340 = stablehlo.add %cst_0, %1339 : tensor<768xf32>
Cost: 590

Operation name: stablehlo.divide
cache miss
2134516102
%1341 = stablehlo.divide %cst_0, %1340 : tensor<768xf32>
Cost: 602

Operation name: stablehlo.multiply
cache miss
4137946040
%1342 = stablehlo.multiply %1340, %1340 : tensor<768xf32>
Cost: 555

Operation name: stablehlo.divide
cache miss
2134516102
%1343 = stablehlo.divide %cst_0, %1342 : tensor<768xf32>
Cost: 633

Operation name: stablehlo.multiply
cache miss
4137946040
%1344 = stablehlo.multiply %1334, %1341 : tensor<768xf32>
Cost: 460

Operation name: stablehlo.multiply
cache miss
4137946040
%1345 = stablehlo.multiply %1344, %1337 : tensor<768xf32>
Cost: 595

Operation name: stablehlo.slice
cache miss
983132653
%1346 = stablehlo.slice %arg5 [5:6, 0:288, 0:768] : (tensor<6x288x768xf32>) -> tensor<1x288x768xf32>
Cost: 64026

Operation name: stablehlo.reshape
cache miss
2162077039
%1347 = stablehlo.reshape %1346 : (tensor<1x288x768xf32>) -> tensor<288x768xf32>
Cost: 71371

Operation name: stablehlo.dot_general
cache miss
1061109412
%1348 = stablehlo.dot_general %1347, %1345, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288x768xf32>, tensor<768xf32>) -> tensor<288xf32>
Cost: 5470

Operation name: stablehlo.add
cache miss
3847999983
%1349 = stablehlo.add %1318, %1348 : tensor<288xf32>
Cost: 258

Operation name: stablehlo.dot_general
cache miss
2778462547
%1350 = stablehlo.dot_general %1349, %1349, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288xf32>) -> tensor<f32>
Cost: 180

Operation name: stablehlo.divide
cache miss
719102371
%1351 = stablehlo.divide %1350, %cst_5 : tensor<f32>
Cost: 179

Operation name: stablehlo.add
cache miss
1937139428
%1352 = stablehlo.add %1351, %cst_4 : tensor<f32>
Cost: 180

Operation name: stablehlo.sqrt
cache miss
1884932271
%1353 = stablehlo.sqrt %1352 : tensor<f32>
Cost: 457

Operation name: stablehlo.divide
cache miss
719102371
%1354 = stablehlo.divide %cst_6, %1353 : tensor<f32>
Cost: 397

Operation name: stablehlo.divide
cache miss
719102371
%1355 = stablehlo.divide %cst_7, %1353 : tensor<f32>
Cost: 375

Operation name: stablehlo.multiply
cache miss
1284973678
%1356 = stablehlo.multiply %1353, %1353 : tensor<f32>
Cost: 341

Operation name: stablehlo.divide
cache miss
719102371
%1357 = stablehlo.divide %cst_7, %1356 : tensor<f32>
Cost: 175

Operation name: stablehlo.multiply
cache miss
2511610444
%1358 = stablehlo.multiply %arg3, %1349 : tensor<288xf32>
Cost: 548

Operation name: stablehlo.multiply
cache miss
2511610444
%1359 = stablehlo.multiply %1358, %arg13 : tensor<288xf32>
Cost: 664

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 274

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%1360 = stablehlo.reduce(%1359 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<288xf32>, tensor<f32>) -> tensor<f32>
Cost: 450

Operation name: stablehlo.multiply
cache miss
1284973678
%1361 = stablehlo.multiply %1360, %1357 : tensor<f32>
Cost: 396

Operation name: stablehlo.negate
cache miss
391293031
%1362 = stablehlo.negate %1361 : tensor<f32>
Cost: 401

Operation name: stablehlo.multiply
cache miss
1284973678
%1363 = stablehlo.multiply %1362, %1354 : tensor<f32>
Cost: 379

Operation name: stablehlo.divide
cache miss
719102371
%1364 = stablehlo.divide %1363, %cst_5 : tensor<f32>
Cost: 382

Operation name: stablehlo.dot_general
cache miss
1413888672
%1365 = stablehlo.dot_general %1364, %1349, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<f32>, tensor<288xf32>) -> tensor<288xf32>
Cost: 519

Operation name: stablehlo.add
cache miss
3847999983
%1366 = stablehlo.add %1365, %1365 : tensor<288xf32>
Cost: 404

Operation name: stablehlo.broadcast_in_dim
cache miss
2616490109
%1367 = stablehlo.broadcast_in_dim %1355, dims = [] : (tensor<f32>) -> tensor<288xf32>
Cost: 527

Operation name: stablehlo.multiply
cache miss
2511610444
%1368 = stablehlo.multiply %arg13, %1367 : tensor<288xf32>
Cost: 519

Operation name: stablehlo.multiply
cache miss
2511610444
%1369 = stablehlo.multiply %arg3, %1368 : tensor<288xf32>
Cost: 590

Operation name: stablehlo.add
cache miss
3847999983
%1370 = stablehlo.add %1366, %1369 : tensor<288xf32>
Cost: 539

Operation name: stablehlo.dot_general
cache miss
2840382466
%1371 = stablehlo.dot_general %1370, %1347, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288x768xf32>) -> tensor<768xf32>
Cost: 10727

Operation name: stablehlo.multiply
cache miss
4137946040
%1372 = stablehlo.multiply %1344, %1371 : tensor<768xf32>
Cost: 583

Operation name: stablehlo.dot_general
cache miss
1491685315
%1373 = stablehlo.dot_general %1372, %1336, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<768xf32>, tensor<768x288xf32>) -> tensor<288xf32>
Cost: 6483

Operation name: stablehlo.dot_general
cache miss
1743356395
%1374 = stablehlo.dot_general %1372, %1331, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<768xf32>, tensor<288xf32>) -> tensor<768x288xf32>
Cost: 91889

Operation name: stablehlo.reshape
cache miss
1628429499
%1375 = stablehlo.reshape %1374 : (tensor<768x288xf32>) -> tensor<1x768x288xf32>
Cost: 114906

Operation name: stablehlo.multiply
cache miss
4137946040
%1376 = stablehlo.multiply %1371, %1337 : tensor<768xf32>
Cost: 431

Operation name: stablehlo.multiply
cache miss
4137946040
%1377 = stablehlo.multiply %1334, %1376 : tensor<768xf32>
Cost: 418

Operation name: stablehlo.multiply
cache miss
4137946040
%1378 = stablehlo.multiply %1377, %1343 : tensor<768xf32>
Cost: 635

Operation name: stablehlo.negate
cache miss
4256221255
%1379 = stablehlo.negate %1378 : tensor<768xf32>
Cost: 417

Operation name: stablehlo.multiply
cache miss
4137946040
%1380 = stablehlo.multiply %1379, %1339 : tensor<768xf32>
Cost: 375

Operation name: stablehlo.negate
cache miss
4256221255
%1381 = stablehlo.negate %1380 : tensor<768xf32>
Cost: 688

Operation name: stablehlo.multiply
cache miss
4137946040
%1382 = stablehlo.multiply %1376, %1341 : tensor<768xf32>
Cost: 469

Operation name: stablehlo.add
cache miss
389450582
%1383 = stablehlo.add %1381, %1382 : tensor<768xf32>
Cost: 501

Operation name: stablehlo.dot_general
cache miss
1491685315
%1384 = stablehlo.dot_general %1383, %1333, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<768xf32>, tensor<768x288xf32>) -> tensor<288xf32>
Cost: 5498

Operation name: stablehlo.add
cache miss
3847999983
%1385 = stablehlo.add %1373, %1384 : tensor<288xf32>
Cost: 305

Operation name: stablehlo.multiply
cache miss
2511610444
%1386 = stablehlo.multiply %1329, %1385 : tensor<288xf32>
Cost: 401

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 326

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%1387 = stablehlo.reduce(%1386 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<288xf32>, tensor<f32>) -> tensor<f32>
Cost: 327

Operation name: stablehlo.multiply
cache miss
1284973678
%1388 = stablehlo.multiply %1387, %1328 : tensor<f32>
Cost: 316

Operation name: stablehlo.negate
cache miss
391293031
%1389 = stablehlo.negate %1388 : tensor<f32>
Cost: 307

Operation name: stablehlo.multiply
cache miss
1284973678
%1390 = stablehlo.multiply %1389, %1325 : tensor<f32>
Cost: 334

Operation name: stablehlo.divide
cache miss
719102371
%1391 = stablehlo.divide %1390, %cst_5 : tensor<f32>
Cost: 335

Operation name: stablehlo.dot_general
cache miss
1413888672
%1392 = stablehlo.dot_general %1391, %1318, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<f32>, tensor<288xf32>) -> tensor<288xf32>
Cost: 409

Operation name: stablehlo.add
cache miss
3847999983
%1393 = stablehlo.add %1370, %1392 : tensor<288xf32>
Cost: 404

Operation name: stablehlo.add
cache miss
3847999983
%1394 = stablehlo.add %1393, %1392 : tensor<288xf32>
Cost: 303

Operation name: stablehlo.multiply
cache miss
2511610444
%1395 = stablehlo.multiply %1385, %1330 : tensor<288xf32>
Cost: 272

Operation name: stablehlo.multiply
cache miss
2511610444
%1396 = stablehlo.multiply %1320, %1395 : tensor<288xf32>
Cost: 417

Operation name: stablehlo.add
cache miss
3847999983
%1397 = stablehlo.add %1394, %1396 : tensor<288xf32>
Cost: 548

Operation name: stablehlo.dot_general
cache miss
1491685315
%1398 = stablehlo.dot_general %1397, %1316, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288x288xf32>) -> tensor<288xf32>
Cost: 339

Operation name: stablehlo.slice
cache miss
2839717066
%1399 = stablehlo.slice %1398 [0:48] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 273

Operation name: stablehlo.slice
cache miss
4032389970
%1400 = stablehlo.slice %1398 [48:96] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 176

Operation name: stablehlo.slice
cache miss
1375222232
%1401 = stablehlo.slice %1398 [96:144] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 181

Operation name: stablehlo.slice
cache miss
3469783879
%1402 = stablehlo.slice %1398 [144:192] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 368

Operation name: stablehlo.slice
cache miss
1801550340
%1403 = stablehlo.slice %1398 [192:240] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 276

Operation name: stablehlo.slice
cache miss
2798019243
%1404 = stablehlo.slice %1398 [240:288] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 276

Operation name: stablehlo.dot_general
cache miss
597698865
%1405 = stablehlo.dot_general %1404, %1312, contracting_dims = [0] x [1], precision = [DEFAULT, DEFAULT] : (tensor<48xf32>, tensor<2x48xf32>) -> tensor<2xf32>
Cost: 295

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1406 = stablehlo.broadcast_in_dim %1309, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 290

Operation name: stablehlo.multiply
cache miss
806204014
%1407 = stablehlo.multiply %1405, %1406 : tensor<2xf32>
Cost: 296

Operation name: stablehlo.multiply
cache miss
806204014
%1408 = stablehlo.multiply %1407, %1300 : tensor<2xf32>
Cost: 405

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 303

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%1409 = stablehlo.reduce(%1408 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 177

Operation name: stablehlo.negate
cache miss
391293031
%1410 = stablehlo.negate %1409 : tensor<f32>
Cost: 280

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1411 = stablehlo.broadcast_in_dim %1410, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 397

Operation name: stablehlo.divide
cache miss
1531341356
%1412 = stablehlo.divide %1405, %1306 : tensor<2xf32>
Cost: 284

Operation name: stablehlo.add
cache miss
1018830424
%1413 = stablehlo.add %1411, %1412 : tensor<2xf32>
Cost: 307

Operation name: stablehlo.multiply
cache miss
806204014
%1414 = stablehlo.multiply %1413, %1300 : tensor<2xf32>
Cost: 369

Operation name: stablehlo.negate
cache miss
37341164
%1415 = stablehlo.negate %1414 : tensor<2xf32>
Cost: 282

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 384

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%1416 = stablehlo.reduce(%1415 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 310

Operation name: stablehlo.divide
cache miss
719102371
%1417 = stablehlo.divide %1416, %1298 : tensor<f32>
Cost: 377

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1418 = stablehlo.broadcast_in_dim %1417, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 379

Operation name: stablehlo.multiply
cache miss
806204014
%1419 = stablehlo.multiply %1418, %1297 : tensor<2xf32>
Cost: 345

Operation name: stablehlo.add
cache miss
1018830424
%1420 = stablehlo.add %1414, %1419 : tensor<2xf32>
Cost: 370

Operation name: stablehlo.divide
cache miss
1531341356
%1421 = stablehlo.divide %1420, %cst_1 : tensor<2xf32>
Cost: 400

Operation name: stablehlo.dot_general
cache miss
2902940993
%1422 = stablehlo.dot_general %1421, %1291, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<2x48xf32>) -> tensor<48xf32>
Cost: 356

Operation name: stablehlo.dot_general
cache miss
2229247683
%1423 = stablehlo.dot_general %1421, %1288, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 390

Operation name: stablehlo.dot_general
cache miss
2229247683
%1424 = stablehlo.dot_general %1307, %1404, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 448

Operation name: stablehlo.dot_general
cache miss
597698865
%1425 = stablehlo.dot_general %1403, %1286, contracting_dims = [0] x [1], precision = [DEFAULT, DEFAULT] : (tensor<48xf32>, tensor<2x48xf32>) -> tensor<2xf32>
Cost: 368

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1426 = stablehlo.broadcast_in_dim %1283, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 274

Operation name: stablehlo.multiply
cache miss
806204014
%1427 = stablehlo.multiply %1425, %1426 : tensor<2xf32>
Cost: 366

Operation name: stablehlo.multiply
cache miss
806204014
%1428 = stablehlo.multiply %1427, %1274 : tensor<2xf32>
Cost: 305

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 174

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%1429 = stablehlo.reduce(%1428 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 508

Operation name: stablehlo.negate
cache miss
391293031
%1430 = stablehlo.negate %1429 : tensor<f32>
Cost: 365

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1431 = stablehlo.broadcast_in_dim %1430, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 446

Operation name: stablehlo.divide
cache miss
1531341356
%1432 = stablehlo.divide %1425, %1280 : tensor<2xf32>
Cost: 513

Operation name: stablehlo.add
cache miss
1018830424
%1433 = stablehlo.add %1431, %1432 : tensor<2xf32>
Cost: 301

Operation name: stablehlo.multiply
cache miss
806204014
%1434 = stablehlo.multiply %1433, %1274 : tensor<2xf32>
Cost: 175

Operation name: stablehlo.negate
cache miss
37341164
%1435 = stablehlo.negate %1434 : tensor<2xf32>
Cost: 362

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 365

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%1436 = stablehlo.reduce(%1435 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 271

Operation name: stablehlo.divide
cache miss
719102371
%1437 = stablehlo.divide %1436, %1272 : tensor<f32>
Cost: 272

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1438 = stablehlo.broadcast_in_dim %1437, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 388

Operation name: stablehlo.multiply
cache miss
806204014
%1439 = stablehlo.multiply %1438, %1271 : tensor<2xf32>
Cost: 273

Operation name: stablehlo.add
cache miss
1018830424
%1440 = stablehlo.add %1434, %1439 : tensor<2xf32>
Cost: 363

Operation name: stablehlo.divide
cache miss
1531341356
%1441 = stablehlo.divide %1440, %cst_1 : tensor<2xf32>
Cost: 271

Operation name: stablehlo.dot_general
cache miss
2902940993
%1442 = stablehlo.dot_general %1441, %1265, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<2x48xf32>) -> tensor<48xf32>
Cost: 265

Operation name: stablehlo.dot_general
cache miss
2229247683
%1443 = stablehlo.dot_general %1441, %1262, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 395

Operation name: stablehlo.dot_general
cache miss
2229247683
%1444 = stablehlo.dot_general %1281, %1403, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 397

Operation name: stablehlo.dot_general
cache miss
597698865
%1445 = stablehlo.dot_general %1402, %1260, contracting_dims = [0] x [1], precision = [DEFAULT, DEFAULT] : (tensor<48xf32>, tensor<2x48xf32>) -> tensor<2xf32>
Cost: 388

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1446 = stablehlo.broadcast_in_dim %1257, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 359

Operation name: stablehlo.multiply
cache miss
806204014
%1447 = stablehlo.multiply %1445, %1446 : tensor<2xf32>
Cost: 382

Operation name: stablehlo.multiply
cache miss
806204014
%1448 = stablehlo.multiply %1447, %1248 : tensor<2xf32>
Cost: 383

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 272

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%1449 = stablehlo.reduce(%1448 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 271

Operation name: stablehlo.negate
cache miss
391293031
%1450 = stablehlo.negate %1449 : tensor<f32>
Cost: 384

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1451 = stablehlo.broadcast_in_dim %1450, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 387

Operation name: stablehlo.divide
cache miss
1531341356
%1452 = stablehlo.divide %1445, %1254 : tensor<2xf32>
Cost: 170

Operation name: stablehlo.add
cache miss
1018830424
%1453 = stablehlo.add %1451, %1452 : tensor<2xf32>
Cost: 362

Operation name: stablehlo.multiply
cache miss
806204014
%1454 = stablehlo.multiply %1453, %1248 : tensor<2xf32>
Cost: 411

Operation name: stablehlo.negate
cache miss
37341164
%1455 = stablehlo.negate %1454 : tensor<2xf32>
Cost: 359

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 381

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%1456 = stablehlo.reduce(%1455 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 303

Operation name: stablehlo.divide
cache miss
719102371
%1457 = stablehlo.divide %1456, %1246 : tensor<f32>
Cost: 410

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1458 = stablehlo.broadcast_in_dim %1457, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 297

Operation name: stablehlo.multiply
cache miss
806204014
%1459 = stablehlo.multiply %1458, %1245 : tensor<2xf32>
Cost: 303

Operation name: stablehlo.add
cache miss
1018830424
%1460 = stablehlo.add %1454, %1459 : tensor<2xf32>
Cost: 361

Operation name: stablehlo.divide
cache miss
1531341356
%1461 = stablehlo.divide %1460, %cst_1 : tensor<2xf32>
Cost: 300

Operation name: stablehlo.dot_general
cache miss
2902940993
%1462 = stablehlo.dot_general %1461, %1239, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<2x48xf32>) -> tensor<48xf32>
Cost: 293

Operation name: stablehlo.dot_general
cache miss
2229247683
%1463 = stablehlo.dot_general %1461, %1236, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 278

Operation name: stablehlo.dot_general
cache miss
2229247683
%1464 = stablehlo.dot_general %1255, %1402, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 314

Operation name: stablehlo.dot_general
cache miss
597698865
%1465 = stablehlo.dot_general %1401, %1234, contracting_dims = [0] x [1], precision = [DEFAULT, DEFAULT] : (tensor<48xf32>, tensor<2x48xf32>) -> tensor<2xf32>
Cost: 355

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1466 = stablehlo.broadcast_in_dim %1231, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 271

Operation name: stablehlo.multiply
cache miss
806204014
%1467 = stablehlo.multiply %1465, %1466 : tensor<2xf32>
Cost: 251

Operation name: stablehlo.multiply
cache miss
806204014
%1468 = stablehlo.multiply %1467, %1222 : tensor<2xf32>
Cost: 300

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 303

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%1469 = stablehlo.reduce(%1468 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 327

Operation name: stablehlo.negate
cache miss
391293031
%1470 = stablehlo.negate %1469 : tensor<f32>
Cost: 372

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1471 = stablehlo.broadcast_in_dim %1470, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 295

Operation name: stablehlo.divide
cache miss
1531341356
%1472 = stablehlo.divide %1465, %1228 : tensor<2xf32>
Cost: 171

Operation name: stablehlo.add
cache miss
1018830424
%1473 = stablehlo.add %1471, %1472 : tensor<2xf32>
Cost: 360

Operation name: stablehlo.multiply
cache miss
806204014
%1474 = stablehlo.multiply %1473, %1222 : tensor<2xf32>
Cost: 173

Operation name: stablehlo.negate
cache miss
37341164
%1475 = stablehlo.negate %1474 : tensor<2xf32>
Cost: 302

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 363

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%1476 = stablehlo.reduce(%1475 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 382

Operation name: stablehlo.divide
cache miss
719102371
%1477 = stablehlo.divide %1476, %1220 : tensor<f32>
Cost: 305

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1478 = stablehlo.broadcast_in_dim %1477, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 381

Operation name: stablehlo.multiply
cache miss
806204014
%1479 = stablehlo.multiply %1478, %1219 : tensor<2xf32>
Cost: 170

Operation name: stablehlo.add
cache miss
1018830424
%1480 = stablehlo.add %1474, %1479 : tensor<2xf32>
Cost: 281

Operation name: stablehlo.divide
cache miss
1531341356
%1481 = stablehlo.divide %1480, %cst_1 : tensor<2xf32>
Cost: 354

Operation name: stablehlo.dot_general
cache miss
2902940993
%1482 = stablehlo.dot_general %1481, %1213, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<2x48xf32>) -> tensor<48xf32>
Cost: 359

Operation name: stablehlo.dot_general
cache miss
2229247683
%1483 = stablehlo.dot_general %1481, %1210, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 371

Operation name: stablehlo.dot_general
cache miss
2229247683
%1484 = stablehlo.dot_general %1229, %1401, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 281

Operation name: stablehlo.dot_general
cache miss
597698865
%1485 = stablehlo.dot_general %1400, %1208, contracting_dims = [0] x [1], precision = [DEFAULT, DEFAULT] : (tensor<48xf32>, tensor<2x48xf32>) -> tensor<2xf32>
Cost: 377

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1486 = stablehlo.broadcast_in_dim %1205, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 172

Operation name: stablehlo.multiply
cache miss
806204014
%1487 = stablehlo.multiply %1485, %1486 : tensor<2xf32>
Cost: 173

Operation name: stablehlo.multiply
cache miss
806204014
%1488 = stablehlo.multiply %1487, %1196 : tensor<2xf32>
Cost: 246

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 292

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%1489 = stablehlo.reduce(%1488 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 272

Operation name: stablehlo.negate
cache miss
391293031
%1490 = stablehlo.negate %1489 : tensor<f32>
Cost: 392

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1491 = stablehlo.broadcast_in_dim %1490, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 249

Operation name: stablehlo.divide
cache miss
1531341356
%1492 = stablehlo.divide %1485, %1202 : tensor<2xf32>
Cost: 170

Operation name: stablehlo.add
cache miss
1018830424
%1493 = stablehlo.add %1491, %1492 : tensor<2xf32>
Cost: 173

Operation name: stablehlo.multiply
cache miss
806204014
%1494 = stablehlo.multiply %1493, %1196 : tensor<2xf32>
Cost: 370

Operation name: stablehlo.negate
cache miss
37341164
%1495 = stablehlo.negate %1494 : tensor<2xf32>
Cost: 355

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 301

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%1496 = stablehlo.reduce(%1495 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 400

Operation name: stablehlo.divide
cache miss
719102371
%1497 = stablehlo.divide %1496, %1194 : tensor<f32>
Cost: 172

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1498 = stablehlo.broadcast_in_dim %1497, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 365

Operation name: stablehlo.multiply
cache miss
806204014
%1499 = stablehlo.multiply %1498, %1193 : tensor<2xf32>
Cost: 381

Operation name: stablehlo.add
cache miss
1018830424
%1500 = stablehlo.add %1494, %1499 : tensor<2xf32>
Cost: 300

Operation name: stablehlo.divide
cache miss
1531341356
%1501 = stablehlo.divide %1500, %cst_1 : tensor<2xf32>
Cost: 294

Operation name: stablehlo.dot_general
cache miss
2902940993
%1502 = stablehlo.dot_general %1501, %1187, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<2x48xf32>) -> tensor<48xf32>
Cost: 181

Operation name: stablehlo.dot_general
cache miss
2229247683
%1503 = stablehlo.dot_general %1501, %1184, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 176

Operation name: stablehlo.dot_general
cache miss
2229247683
%1504 = stablehlo.dot_general %1203, %1400, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 393

Operation name: stablehlo.dot_general
cache miss
597698865
%1505 = stablehlo.dot_general %1399, %1182, contracting_dims = [0] x [1], precision = [DEFAULT, DEFAULT] : (tensor<48xf32>, tensor<2x48xf32>) -> tensor<2xf32>
Cost: 408

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1506 = stablehlo.broadcast_in_dim %1179, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 382

Operation name: stablehlo.multiply
cache miss
806204014
%1507 = stablehlo.multiply %1505, %1506 : tensor<2xf32>
Cost: 382

Operation name: stablehlo.multiply
cache miss
806204014
%1508 = stablehlo.multiply %1507, %1170 : tensor<2xf32>
Cost: 381

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 505

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%1509 = stablehlo.reduce(%1508 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 172

Operation name: stablehlo.negate
cache miss
391293031
%1510 = stablehlo.negate %1509 : tensor<f32>
Cost: 171

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1511 = stablehlo.broadcast_in_dim %1510, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 382

Operation name: stablehlo.divide
cache miss
1531341356
%1512 = stablehlo.divide %1505, %1176 : tensor<2xf32>
Cost: 170

Operation name: stablehlo.add
cache miss
1018830424
%1513 = stablehlo.add %1511, %1512 : tensor<2xf32>
Cost: 334

Operation name: stablehlo.multiply
cache miss
806204014
%1514 = stablehlo.multiply %1513, %1170 : tensor<2xf32>
Cost: 385

Operation name: stablehlo.negate
cache miss
37341164
%1515 = stablehlo.negate %1514 : tensor<2xf32>
Cost: 275

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 384

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%1516 = stablehlo.reduce(%1515 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 385

Operation name: stablehlo.divide
cache miss
719102371
%1517 = stablehlo.divide %1516, %1168 : tensor<f32>
Cost: 385

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1518 = stablehlo.broadcast_in_dim %1517, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 556

Operation name: stablehlo.multiply
cache miss
806204014
%1519 = stablehlo.multiply %1518, %1167 : tensor<2xf32>
Cost: 183

Operation name: stablehlo.add
cache miss
1018830424
%1520 = stablehlo.add %1514, %1519 : tensor<2xf32>
Cost: 386

Operation name: stablehlo.divide
cache miss
1531341356
%1521 = stablehlo.divide %1520, %cst_1 : tensor<2xf32>
Cost: 172

Operation name: stablehlo.dot_general
cache miss
2902940993
%1522 = stablehlo.dot_general %1521, %1161, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<2x48xf32>) -> tensor<48xf32>
Cost: 294

Operation name: stablehlo.concatenate
cache miss
292037604
%1523 = stablehlo.concatenate %1522, %1502, %1482, %1462, %1442, %1422, dim = 0 : (tensor<48xf32>, tensor<48xf32>, tensor<48xf32>, tensor<48xf32>, tensor<48xf32>, tensor<48xf32>) -> tensor<288xf32>
Cost: 307

Operation name: stablehlo.reshape
cache miss
2392077026
%1524 = stablehlo.reshape %1523 : (tensor<288xf32>) -> tensor<144x2xf32>
Cost: 543

Operation name: stablehlo.dot_general
cache miss
1714835160
%1525 = stablehlo.dot_general %1524, %cst_8, batching_dims = [0] x [0], contracting_dims = [1] x [1], precision = [DEFAULT, DEFAULT] : (tensor<144x2xf32>, tensor<144x2x2xf32>) -> tensor<144x2xf32>
Cost: 618

Operation name: stablehlo.reshape
cache miss
1377738304
%1526 = stablehlo.reshape %1525 : (tensor<144x2xf32>) -> tensor<288xf32>
Cost: 550

Operation name: stablehlo.dot_general
cache miss
1491685315
%1527 = stablehlo.dot_general %1526, %1139, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288x288xf32>) -> tensor<288xf32>
Cost: 479

Operation name: stablehlo.dot_general
cache miss
3876455518
%1528 = stablehlo.dot_general %1526, %1137, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288xf32>) -> tensor<288x288xf32>
Cost: 94720

Operation name: stablehlo.reshape
cache miss
552616300
%1529 = stablehlo.reshape %1528 : (tensor<288x288xf32>) -> tensor<1x288x288xf32>
Cost: 75755

Operation name: stablehlo.dot_general
cache miss
2229247683
%1530 = stablehlo.dot_general %1521, %1158, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 326

Operation name: stablehlo.slice
cache miss
3729170049
%1531 = stablehlo.slice %1530 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 276

Operation name: stablehlo.slice
cache miss
3729170049
%1532 = stablehlo.slice %1503 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 174

Operation name: stablehlo.slice
cache miss
3729170049
%1533 = stablehlo.slice %1483 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 302

Operation name: stablehlo.slice
cache miss
3729170049
%1534 = stablehlo.slice %1463 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 311

Operation name: stablehlo.slice
cache miss
3729170049
%1535 = stablehlo.slice %1443 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 311

Operation name: stablehlo.slice
cache miss
3729170049
%1536 = stablehlo.slice %1423 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 278

Operation name: stablehlo.concatenate
cache miss
3995287944
%1537 = stablehlo.concatenate %1531, %1532, %1533, %1534, %1535, %1536, dim = 1 : (tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<1x288xf32>
Cost: 423

Operation name: stablehlo.slice
cache miss
342624074
%1538 = stablehlo.slice %1530 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 305

Operation name: stablehlo.slice
cache miss
342624074
%1539 = stablehlo.slice %1503 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 174

Operation name: stablehlo.slice
cache miss
342624074
%1540 = stablehlo.slice %1483 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 239

Operation name: stablehlo.slice
cache miss
342624074
%1541 = stablehlo.slice %1463 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 486

Operation name: stablehlo.slice
cache miss
342624074
%1542 = stablehlo.slice %1443 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 173

Operation name: stablehlo.slice
cache miss
342624074
%1543 = stablehlo.slice %1423 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 175

Operation name: stablehlo.concatenate
cache miss
3995287944
%1544 = stablehlo.concatenate %1538, %1539, %1540, %1541, %1542, %1543, dim = 1 : (tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<1x288xf32>
Cost: 980

Operation name: stablehlo.reshape
cache miss
2392077026
%1545 = stablehlo.reshape %1544 : (tensor<1x288xf32>) -> tensor<144x2xf32>
Cost: 591

Operation name: stablehlo.dot_general
cache miss
1714835160
%1546 = stablehlo.dot_general %1545, %cst_8, batching_dims = [0] x [0], contracting_dims = [1] x [1], precision = [DEFAULT, DEFAULT] : (tensor<144x2xf32>, tensor<144x2x2xf32>) -> tensor<144x2xf32>
Cost: 413

Operation name: stablehlo.reshape
cache miss
1377738304
%1547 = stablehlo.reshape %1546 : (tensor<144x2xf32>) -> tensor<288xf32>
Cost: 272

Operation name: stablehlo.dot_general
cache miss
1491685315
%1548 = stablehlo.dot_general %1547, %1142, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288x288xf32>) -> tensor<288xf32>
Cost: 234

Operation name: stablehlo.add
cache miss
3847999983
%1549 = stablehlo.add %1527, %1548 : tensor<288xf32>
Cost: 521

Operation name: stablehlo.dot_general
cache miss
3876455518
%1550 = stablehlo.dot_general %1547, %1137, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288xf32>) -> tensor<288x288xf32>
Cost: 18601

Operation name: stablehlo.reshape
cache miss
552616300
%1551 = stablehlo.reshape %1550 : (tensor<288x288xf32>) -> tensor<1x288x288xf32>
Cost: 21238

Operation name: stablehlo.reshape
cache miss
1853321842
%1552 = stablehlo.reshape %1537 : (tensor<1x288xf32>) -> tensor<1x1x288xf32>
Cost: 302

Operation name: stablehlo.dot_general
cache miss
2229247683
%1553 = stablehlo.dot_general %1177, %1399, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 304

Operation name: stablehlo.slice
cache miss
3729170049
%1554 = stablehlo.slice %1553 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 335

Operation name: stablehlo.slice
cache miss
3729170049
%1555 = stablehlo.slice %1504 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 369

Operation name: stablehlo.slice
cache miss
3729170049
%1556 = stablehlo.slice %1484 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 276

Operation name: stablehlo.slice
cache miss
3729170049
%1557 = stablehlo.slice %1464 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 368

Operation name: stablehlo.slice
cache miss
3729170049
%1558 = stablehlo.slice %1444 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 367

Operation name: stablehlo.slice
cache miss
3729170049
%1559 = stablehlo.slice %1424 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 366

Operation name: stablehlo.concatenate
cache miss
3995287944
%1560 = stablehlo.concatenate %1554, %1555, %1556, %1557, %1558, %1559, dim = 1 : (tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<1x288xf32>
Cost: 514

Operation name: stablehlo.slice
cache miss
342624074
%1561 = stablehlo.slice %1553 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 384

Operation name: stablehlo.slice
cache miss
342624074
%1562 = stablehlo.slice %1504 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 587

Operation name: stablehlo.slice
cache miss
342624074
%1563 = stablehlo.slice %1484 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 315

Operation name: stablehlo.slice
cache miss
342624074
%1564 = stablehlo.slice %1464 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 384

Operation name: stablehlo.slice
cache miss
342624074
%1565 = stablehlo.slice %1444 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 397

Operation name: stablehlo.slice
cache miss
342624074
%1566 = stablehlo.slice %1424 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 391

Operation name: stablehlo.concatenate
cache miss
3995287944
%1567 = stablehlo.concatenate %1561, %1562, %1563, %1564, %1565, %1566, dim = 1 : (tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<1x288xf32>
Cost: 480

Operation name: stablehlo.reshape
cache miss
1377738304
%1568 = stablehlo.reshape %1567 : (tensor<1x288xf32>) -> tensor<288xf32>
Cost: 268

Operation name: stablehlo.dot_general
cache miss
1491685315
%1569 = stablehlo.dot_general %1568, %1145, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288x288xf32>) -> tensor<288xf32>
Cost: 239

Operation name: stablehlo.add
cache miss
3847999983
%1570 = stablehlo.add %1549, %1569 : tensor<288xf32>
Cost: 788

Operation name: stablehlo.multiply
cache miss
2511610444
%1571 = stablehlo.multiply %1135, %1570 : tensor<288xf32>
Cost: 493

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 191

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%1572 = stablehlo.reduce(%1571 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<288xf32>, tensor<f32>) -> tensor<f32>
Cost: 409

Operation name: stablehlo.multiply
cache miss
1284973678
%1573 = stablehlo.multiply %1572, %1134 : tensor<f32>
Cost: 234

Operation name: stablehlo.negate
cache miss
391293031
%1574 = stablehlo.negate %1573 : tensor<f32>
Cost: 189

Operation name: stablehlo.multiply
cache miss
1284973678
%1575 = stablehlo.multiply %1574, %1131 : tensor<f32>
Cost: 186

Operation name: stablehlo.divide
cache miss
719102371
%1576 = stablehlo.divide %1575, %cst_5 : tensor<f32>
Cost: 186

Operation name: stablehlo.dot_general
cache miss
1413888672
%1577 = stablehlo.dot_general %1576, %1124, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<f32>, tensor<288xf32>) -> tensor<288xf32>
Cost: 500

Operation name: stablehlo.add
cache miss
3847999983
%1578 = stablehlo.add %1397, %1577 : tensor<288xf32>
Cost: 481

Operation name: stablehlo.add
cache miss
3847999983
%1579 = stablehlo.add %1578, %1577 : tensor<288xf32>
Cost: 480

Operation name: stablehlo.multiply
cache miss
2511610444
%1580 = stablehlo.multiply %1570, %1136 : tensor<288xf32>
Cost: 603

Operation name: stablehlo.multiply
cache miss
2511610444
%1581 = stablehlo.multiply %1126, %1580 : tensor<288xf32>
Cost: 452

Operation name: stablehlo.add
cache miss
3847999983
%1582 = stablehlo.add %1579, %1581 : tensor<288xf32>
Cost: 296

Operation name: stablehlo.dot_general
cache miss
2840382466
%1583 = stablehlo.dot_general %1582, %1122, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288x768xf32>) -> tensor<768xf32>
Cost: 5153

Operation name: stablehlo.multiply
cache miss
4137946040
%1584 = stablehlo.multiply %1119, %1583 : tensor<768xf32>
Cost: 550

Operation name: stablehlo.dot_general
cache miss
1491685315
%1585 = stablehlo.dot_general %1584, %1111, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<768xf32>, tensor<768x288xf32>) -> tensor<288xf32>
Cost: 4173

Operation name: stablehlo.dot_general
cache miss
1743356395
%1586 = stablehlo.dot_general %1584, %1106, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<768xf32>, tensor<288xf32>) -> tensor<768x288xf32>
Cost: 54112

Operation name: stablehlo.reshape
cache miss
1628429499
%1587 = stablehlo.reshape %1586 : (tensor<768x288xf32>) -> tensor<1x768x288xf32>
Cost: 49893

Operation name: stablehlo.multiply
cache miss
4137946040
%1588 = stablehlo.multiply %1583, %1112 : tensor<768xf32>
Cost: 463

Operation name: stablehlo.multiply
cache miss
4137946040
%1589 = stablehlo.multiply %1109, %1588 : tensor<768xf32>
Cost: 381

Operation name: stablehlo.multiply
cache miss
4137946040
%1590 = stablehlo.multiply %1589, %1118 : tensor<768xf32>
Cost: 326

Operation name: stablehlo.negate
cache miss
4256221255
%1591 = stablehlo.negate %1590 : tensor<768xf32>
Cost: 631

Operation name: stablehlo.multiply
cache miss
4137946040
%1592 = stablehlo.multiply %1591, %1114 : tensor<768xf32>
Cost: 612

Operation name: stablehlo.negate
cache miss
4256221255
%1593 = stablehlo.negate %1592 : tensor<768xf32>
Cost: 550

Operation name: stablehlo.multiply
cache miss
4137946040
%1594 = stablehlo.multiply %1588, %1116 : tensor<768xf32>
Cost: 479

Operation name: stablehlo.add
cache miss
389450582
%1595 = stablehlo.add %1593, %1594 : tensor<768xf32>
Cost: 460

Operation name: stablehlo.dot_general
cache miss
1491685315
%1596 = stablehlo.dot_general %1595, %1108, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<768xf32>, tensor<768x288xf32>) -> tensor<288xf32>
Cost: 6722

Operation name: stablehlo.add
cache miss
3847999983
%1597 = stablehlo.add %1585, %1596 : tensor<288xf32>
Cost: 274

Operation name: stablehlo.multiply
cache miss
2511610444
%1598 = stablehlo.multiply %1104, %1597 : tensor<288xf32>
Cost: 274

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 417

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%1599 = stablehlo.reduce(%1598 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<288xf32>, tensor<f32>) -> tensor<f32>
Cost: 349

Operation name: stablehlo.multiply
cache miss
1284973678
%1600 = stablehlo.multiply %1599, %1103 : tensor<f32>
Cost: 259

Operation name: stablehlo.negate
cache miss
391293031
%1601 = stablehlo.negate %1600 : tensor<f32>
Cost: 195

Operation name: stablehlo.multiply
cache miss
1284973678
%1602 = stablehlo.multiply %1601, %1100 : tensor<f32>
Cost: 189

Operation name: stablehlo.divide
cache miss
719102371
%1603 = stablehlo.divide %1602, %cst_5 : tensor<f32>
Cost: 190

Operation name: stablehlo.dot_general
cache miss
1413888672
%1604 = stablehlo.dot_general %1603, %1093, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<f32>, tensor<288xf32>) -> tensor<288xf32>
Cost: 403

Operation name: stablehlo.add
cache miss
3847999983
%1605 = stablehlo.add %1582, %1604 : tensor<288xf32>
Cost: 692

Operation name: stablehlo.add
cache miss
3847999983
%1606 = stablehlo.add %1605, %1604 : tensor<288xf32>
Cost: 284

Operation name: stablehlo.multiply
cache miss
2511610444
%1607 = stablehlo.multiply %1597, %1105 : tensor<288xf32>
Cost: 279

Operation name: stablehlo.multiply
cache miss
2511610444
%1608 = stablehlo.multiply %1095, %1607 : tensor<288xf32>
Cost: 434

Operation name: stablehlo.add
cache miss
3847999983
%1609 = stablehlo.add %1606, %1608 : tensor<288xf32>
Cost: 565

Operation name: stablehlo.dot_general
cache miss
1491685315
%1610 = stablehlo.dot_general %1609, %1091, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288x288xf32>) -> tensor<288xf32>
Cost: 357

Operation name: stablehlo.slice
cache miss
2839717066
%1611 = stablehlo.slice %1610 [0:48] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 233

Operation name: stablehlo.slice
cache miss
4032389970
%1612 = stablehlo.slice %1610 [48:96] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 392

Operation name: stablehlo.slice
cache miss
1375222232
%1613 = stablehlo.slice %1610 [96:144] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 370

Operation name: stablehlo.slice
cache miss
3469783879
%1614 = stablehlo.slice %1610 [144:192] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 278

Operation name: stablehlo.slice
cache miss
1801550340
%1615 = stablehlo.slice %1610 [192:240] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 369

Operation name: stablehlo.slice
cache miss
2798019243
%1616 = stablehlo.slice %1610 [240:288] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 562

Operation name: stablehlo.dot_general
cache miss
597698865
%1617 = stablehlo.dot_general %1616, %1087, contracting_dims = [0] x [1], precision = [DEFAULT, DEFAULT] : (tensor<48xf32>, tensor<2x48xf32>) -> tensor<2xf32>
Cost: 308

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1618 = stablehlo.broadcast_in_dim %1084, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 298

Operation name: stablehlo.multiply
cache miss
806204014
%1619 = stablehlo.multiply %1617, %1618 : tensor<2xf32>
Cost: 394

Operation name: stablehlo.multiply
cache miss
806204014
%1620 = stablehlo.multiply %1619, %1075 : tensor<2xf32>
Cost: 407

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 301

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%1621 = stablehlo.reduce(%1620 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 307

Operation name: stablehlo.negate
cache miss
391293031
%1622 = stablehlo.negate %1621 : tensor<f32>
Cost: 382

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1623 = stablehlo.broadcast_in_dim %1622, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 194

Operation name: stablehlo.divide
cache miss
1531341356
%1624 = stablehlo.divide %1617, %1081 : tensor<2xf32>
Cost: 298

Operation name: stablehlo.add
cache miss
1018830424
%1625 = stablehlo.add %1623, %1624 : tensor<2xf32>
Cost: 193

Operation name: stablehlo.multiply
cache miss
806204014
%1626 = stablehlo.multiply %1625, %1075 : tensor<2xf32>
Cost: 296

Operation name: stablehlo.negate
cache miss
37341164
%1627 = stablehlo.negate %1626 : tensor<2xf32>
Cost: 193

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 191

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%1628 = stablehlo.reduce(%1627 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 213

Operation name: stablehlo.divide
cache miss
719102371
%1629 = stablehlo.divide %1628, %1073 : tensor<f32>
Cost: 192

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1630 = stablehlo.broadcast_in_dim %1629, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 197

Operation name: stablehlo.multiply
cache miss
806204014
%1631 = stablehlo.multiply %1630, %1072 : tensor<2xf32>
Cost: 332

Operation name: stablehlo.add
cache miss
1018830424
%1632 = stablehlo.add %1626, %1631 : tensor<2xf32>
Cost: 331

Operation name: stablehlo.divide
cache miss
1531341356
%1633 = stablehlo.divide %1632, %cst_1 : tensor<2xf32>
Cost: 388

Operation name: stablehlo.dot_general
cache miss
2902940993
%1634 = stablehlo.dot_general %1633, %1066, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<2x48xf32>) -> tensor<48xf32>
Cost: 378

Operation name: stablehlo.dot_general
cache miss
2229247683
%1635 = stablehlo.dot_general %1633, %1063, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 382

Operation name: stablehlo.dot_general
cache miss
2229247683
%1636 = stablehlo.dot_general %1082, %1616, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 308

Operation name: stablehlo.dot_general
cache miss
597698865
%1637 = stablehlo.dot_general %1615, %1061, contracting_dims = [0] x [1], precision = [DEFAULT, DEFAULT] : (tensor<48xf32>, tensor<2x48xf32>) -> tensor<2xf32>
Cost: 402

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1638 = stablehlo.broadcast_in_dim %1058, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 397

Operation name: stablehlo.multiply
cache miss
806204014
%1639 = stablehlo.multiply %1637, %1638 : tensor<2xf32>
Cost: 292

Operation name: stablehlo.multiply
cache miss
806204014
%1640 = stablehlo.multiply %1639, %1049 : tensor<2xf32>
Cost: 399

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 401

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%1641 = stablehlo.reduce(%1640 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 235

Operation name: stablehlo.negate
cache miss
391293031
%1642 = stablehlo.negate %1641 : tensor<f32>
Cost: 415

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1643 = stablehlo.broadcast_in_dim %1642, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 405

Operation name: stablehlo.divide
cache miss
1531341356
%1644 = stablehlo.divide %1637, %1055 : tensor<2xf32>
Cost: 402

Operation name: stablehlo.add
cache miss
1018830424
%1645 = stablehlo.add %1643, %1644 : tensor<2xf32>
Cost: 312

Operation name: stablehlo.multiply
cache miss
806204014
%1646 = stablehlo.multiply %1645, %1049 : tensor<2xf32>
Cost: 390

Operation name: stablehlo.negate
cache miss
37341164
%1647 = stablehlo.negate %1646 : tensor<2xf32>
Cost: 310

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 314

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%1648 = stablehlo.reduce(%1647 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 181

Operation name: stablehlo.divide
cache miss
719102371
%1649 = stablehlo.divide %1648, %1047 : tensor<f32>
Cost: 398

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1650 = stablehlo.broadcast_in_dim %1649, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 472

Operation name: stablehlo.multiply
cache miss
806204014
%1651 = stablehlo.multiply %1650, %1046 : tensor<2xf32>
Cost: 279

Operation name: stablehlo.add
cache miss
1018830424
%1652 = stablehlo.add %1646, %1651 : tensor<2xf32>
Cost: 177

Operation name: stablehlo.divide
cache miss
1531341356
%1653 = stablehlo.divide %1652, %cst_1 : tensor<2xf32>
Cost: 392

Operation name: stablehlo.dot_general
cache miss
2902940993
%1654 = stablehlo.dot_general %1653, %1040, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<2x48xf32>) -> tensor<48xf32>
Cost: 167

Operation name: stablehlo.dot_general
cache miss
2229247683
%1655 = stablehlo.dot_general %1653, %1037, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 303

Operation name: stablehlo.dot_general
cache miss
2229247683
%1656 = stablehlo.dot_general %1056, %1615, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 371

Operation name: stablehlo.dot_general
cache miss
597698865
%1657 = stablehlo.dot_general %1614, %1035, contracting_dims = [0] x [1], precision = [DEFAULT, DEFAULT] : (tensor<48xf32>, tensor<2x48xf32>) -> tensor<2xf32>
Cost: 303

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1658 = stablehlo.broadcast_in_dim %1032, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 279

Operation name: stablehlo.multiply
cache miss
806204014
%1659 = stablehlo.multiply %1657, %1658 : tensor<2xf32>
Cost: 279

Operation name: stablehlo.multiply
cache miss
806204014
%1660 = stablehlo.multiply %1659, %1023 : tensor<2xf32>
Cost: 274

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 386

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%1661 = stablehlo.reduce(%1660 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 275

Operation name: stablehlo.negate
cache miss
391293031
%1662 = stablehlo.negate %1661 : tensor<f32>
Cost: 172

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1663 = stablehlo.broadcast_in_dim %1662, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 301

Operation name: stablehlo.divide
cache miss
1531341356
%1664 = stablehlo.divide %1657, %1029 : tensor<2xf32>
Cost: 274

Operation name: stablehlo.add
cache miss
1018830424
%1665 = stablehlo.add %1663, %1664 : tensor<2xf32>
Cost: 318

Operation name: stablehlo.multiply
cache miss
806204014
%1666 = stablehlo.multiply %1665, %1023 : tensor<2xf32>
Cost: 178

Operation name: stablehlo.negate
cache miss
37341164
%1667 = stablehlo.negate %1666 : tensor<2xf32>
Cost: 384

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 388

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%1668 = stablehlo.reduce(%1667 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 299

Operation name: stablehlo.divide
cache miss
719102371
%1669 = stablehlo.divide %1668, %1021 : tensor<f32>
Cost: 269

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1670 = stablehlo.broadcast_in_dim %1669, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 276

Operation name: stablehlo.multiply
cache miss
806204014
%1671 = stablehlo.multiply %1670, %1020 : tensor<2xf32>
Cost: 171

Operation name: stablehlo.add
cache miss
1018830424
%1672 = stablehlo.add %1666, %1671 : tensor<2xf32>
Cost: 367

Operation name: stablehlo.divide
cache miss
1531341356
%1673 = stablehlo.divide %1672, %cst_1 : tensor<2xf32>
Cost: 304

Operation name: stablehlo.dot_general
cache miss
2902940993
%1674 = stablehlo.dot_general %1673, %1014, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<2x48xf32>) -> tensor<48xf32>
Cost: 169

Operation name: stablehlo.dot_general
cache miss
2229247683
%1675 = stablehlo.dot_general %1673, %1011, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 189

Operation name: stablehlo.dot_general
cache miss
2229247683
%1676 = stablehlo.dot_general %1030, %1614, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 175

Operation name: stablehlo.dot_general
cache miss
597698865
%1677 = stablehlo.dot_general %1613, %1009, contracting_dims = [0] x [1], precision = [DEFAULT, DEFAULT] : (tensor<48xf32>, tensor<2x48xf32>) -> tensor<2xf32>
Cost: 389

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1678 = stablehlo.broadcast_in_dim %1006, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 173

Operation name: stablehlo.multiply
cache miss
806204014
%1679 = stablehlo.multiply %1677, %1678 : tensor<2xf32>
Cost: 172

Operation name: stablehlo.multiply
cache miss
806204014
%1680 = stablehlo.multiply %1679, %997 : tensor<2xf32>
Cost: 213

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 272

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%1681 = stablehlo.reduce(%1680 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 185

Operation name: stablehlo.negate
cache miss
391293031
%1682 = stablehlo.negate %1681 : tensor<f32>
Cost: 272

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1683 = stablehlo.broadcast_in_dim %1682, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 379

Operation name: stablehlo.divide
cache miss
1531341356
%1684 = stablehlo.divide %1677, %1003 : tensor<2xf32>
Cost: 169

Operation name: stablehlo.add
cache miss
1018830424
%1685 = stablehlo.add %1683, %1684 : tensor<2xf32>
Cost: 371

Operation name: stablehlo.multiply
cache miss
806204014
%1686 = stablehlo.multiply %1685, %997 : tensor<2xf32>
Cost: 387

Operation name: stablehlo.negate
cache miss
37341164
%1687 = stablehlo.negate %1686 : tensor<2xf32>
Cost: 411

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 458

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%1688 = stablehlo.reduce(%1687 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 302

Operation name: stablehlo.divide
cache miss
719102371
%1689 = stablehlo.divide %1688, %995 : tensor<f32>
Cost: 171

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1690 = stablehlo.broadcast_in_dim %1689, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 561

Operation name: stablehlo.multiply
cache miss
806204014
%1691 = stablehlo.multiply %1690, %994 : tensor<2xf32>
Cost: 393

Operation name: stablehlo.add
cache miss
1018830424
%1692 = stablehlo.add %1686, %1691 : tensor<2xf32>
Cost: 299

Operation name: stablehlo.divide
cache miss
1531341356
%1693 = stablehlo.divide %1692, %cst_1 : tensor<2xf32>
Cost: 362

Operation name: stablehlo.dot_general
cache miss
2902940993
%1694 = stablehlo.dot_general %1693, %988, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<2x48xf32>) -> tensor<48xf32>
Cost: 288

Operation name: stablehlo.dot_general
cache miss
2229247683
%1695 = stablehlo.dot_general %1693, %985, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 369

Operation name: stablehlo.dot_general
cache miss
2229247683
%1696 = stablehlo.dot_general %1004, %1613, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 178

Operation name: stablehlo.dot_general
cache miss
597698865
%1697 = stablehlo.dot_general %1612, %983, contracting_dims = [0] x [1], precision = [DEFAULT, DEFAULT] : (tensor<48xf32>, tensor<2x48xf32>) -> tensor<2xf32>
Cost: 367

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1698 = stablehlo.broadcast_in_dim %980, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 362

Operation name: stablehlo.multiply
cache miss
806204014
%1699 = stablehlo.multiply %1697, %1698 : tensor<2xf32>
Cost: 278

Operation name: stablehlo.multiply
cache miss
806204014
%1700 = stablehlo.multiply %1699, %971 : tensor<2xf32>
Cost: 360

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 363

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%1701 = stablehlo.reduce(%1700 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 303

Operation name: stablehlo.negate
cache miss
391293031
%1702 = stablehlo.negate %1701 : tensor<f32>
Cost: 273

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1703 = stablehlo.broadcast_in_dim %1702, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 304

Operation name: stablehlo.divide
cache miss
1531341356
%1704 = stablehlo.divide %1697, %977 : tensor<2xf32>
Cost: 325

Operation name: stablehlo.add
cache miss
1018830424
%1705 = stablehlo.add %1703, %1704 : tensor<2xf32>
Cost: 173

Operation name: stablehlo.multiply
cache miss
806204014
%1706 = stablehlo.multiply %1705, %971 : tensor<2xf32>
Cost: 174

Operation name: stablehlo.negate
cache miss
37341164
%1707 = stablehlo.negate %1706 : tensor<2xf32>
Cost: 173

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 174

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%1708 = stablehlo.reduce(%1707 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 172

Operation name: stablehlo.divide
cache miss
719102371
%1709 = stablehlo.divide %1708, %969 : tensor<f32>
Cost: 274

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1710 = stablehlo.broadcast_in_dim %1709, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 171

Operation name: stablehlo.multiply
cache miss
806204014
%1711 = stablehlo.multiply %1710, %968 : tensor<2xf32>
Cost: 297

Operation name: stablehlo.add
cache miss
1018830424
%1712 = stablehlo.add %1706, %1711 : tensor<2xf32>
Cost: 182

Operation name: stablehlo.divide
cache miss
1531341356
%1713 = stablehlo.divide %1712, %cst_1 : tensor<2xf32>
Cost: 307

Operation name: stablehlo.dot_general
cache miss
2902940993
%1714 = stablehlo.dot_general %1713, %962, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<2x48xf32>) -> tensor<48xf32>
Cost: 267

Operation name: stablehlo.dot_general
cache miss
2229247683
%1715 = stablehlo.dot_general %1713, %959, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 175

Operation name: stablehlo.dot_general
cache miss
2229247683
%1716 = stablehlo.dot_general %978, %1612, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 278

Operation name: stablehlo.dot_general
cache miss
597698865
%1717 = stablehlo.dot_general %1611, %957, contracting_dims = [0] x [1], precision = [DEFAULT, DEFAULT] : (tensor<48xf32>, tensor<2x48xf32>) -> tensor<2xf32>
Cost: 170

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1718 = stablehlo.broadcast_in_dim %954, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 303

Operation name: stablehlo.multiply
cache miss
806204014
%1719 = stablehlo.multiply %1717, %1718 : tensor<2xf32>
Cost: 285

Operation name: stablehlo.multiply
cache miss
806204014
%1720 = stablehlo.multiply %1719, %945 : tensor<2xf32>
Cost: 176

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 174

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%1721 = stablehlo.reduce(%1720 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 174

Operation name: stablehlo.negate
cache miss
391293031
%1722 = stablehlo.negate %1721 : tensor<f32>
Cost: 446

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1723 = stablehlo.broadcast_in_dim %1722, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 362

Operation name: stablehlo.divide
cache miss
1531341356
%1724 = stablehlo.divide %1717, %951 : tensor<2xf32>
Cost: 362

Operation name: stablehlo.add
cache miss
1018830424
%1725 = stablehlo.add %1723, %1724 : tensor<2xf32>
Cost: 302

Operation name: stablehlo.multiply
cache miss
806204014
%1726 = stablehlo.multiply %1725, %945 : tensor<2xf32>
Cost: 364

Operation name: stablehlo.negate
cache miss
37341164
%1727 = stablehlo.negate %1726 : tensor<2xf32>
Cost: 271

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 278

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%1728 = stablehlo.reduce(%1727 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 174

Operation name: stablehlo.divide
cache miss
719102371
%1729 = stablehlo.divide %1728, %943 : tensor<f32>
Cost: 301

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1730 = stablehlo.broadcast_in_dim %1729, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 298

Operation name: stablehlo.multiply
cache miss
806204014
%1731 = stablehlo.multiply %1730, %942 : tensor<2xf32>
Cost: 294

Operation name: stablehlo.add
cache miss
1018830424
%1732 = stablehlo.add %1726, %1731 : tensor<2xf32>
Cost: 300

Operation name: stablehlo.divide
cache miss
1531341356
%1733 = stablehlo.divide %1732, %cst_1 : tensor<2xf32>
Cost: 297

Operation name: stablehlo.dot_general
cache miss
2902940993
%1734 = stablehlo.dot_general %1733, %936, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<2x48xf32>) -> tensor<48xf32>
Cost: 290

Operation name: stablehlo.concatenate
cache miss
292037604
%1735 = stablehlo.concatenate %1734, %1714, %1694, %1674, %1654, %1634, dim = 0 : (tensor<48xf32>, tensor<48xf32>, tensor<48xf32>, tensor<48xf32>, tensor<48xf32>, tensor<48xf32>) -> tensor<288xf32>
Cost: 277

Operation name: stablehlo.reshape
cache miss
2392077026
%1736 = stablehlo.reshape %1735 : (tensor<288xf32>) -> tensor<144x2xf32>
Cost: 439

Operation name: stablehlo.dot_general
cache miss
1714835160
%1737 = stablehlo.dot_general %1736, %cst_8, batching_dims = [0] x [0], contracting_dims = [1] x [1], precision = [DEFAULT, DEFAULT] : (tensor<144x2xf32>, tensor<144x2x2xf32>) -> tensor<144x2xf32>
Cost: 774

Operation name: stablehlo.reshape
cache miss
1377738304
%1738 = stablehlo.reshape %1737 : (tensor<144x2xf32>) -> tensor<288xf32>
Cost: 515

Operation name: stablehlo.dot_general
cache miss
1491685315
%1739 = stablehlo.dot_general %1738, %914, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288x288xf32>) -> tensor<288xf32>
Cost: 257

Operation name: stablehlo.dot_general
cache miss
3876455518
%1740 = stablehlo.dot_general %1738, %912, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288xf32>) -> tensor<288x288xf32>
Cost: 21813

Operation name: stablehlo.reshape
cache miss
552616300
%1741 = stablehlo.reshape %1740 : (tensor<288x288xf32>) -> tensor<1x288x288xf32>
Cost: 26588

Operation name: stablehlo.dot_general
cache miss
2229247683
%1742 = stablehlo.dot_general %1733, %933, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 483

Operation name: stablehlo.slice
cache miss
3729170049
%1743 = stablehlo.slice %1742 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 388

Operation name: stablehlo.slice
cache miss
3729170049
%1744 = stablehlo.slice %1715 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 303

Operation name: stablehlo.slice
cache miss
3729170049
%1745 = stablehlo.slice %1695 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 175

Operation name: stablehlo.slice
cache miss
3729170049
%1746 = stablehlo.slice %1675 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 175

Operation name: stablehlo.slice
cache miss
3729170049
%1747 = stablehlo.slice %1655 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 174

Operation name: stablehlo.slice
cache miss
3729170049
%1748 = stablehlo.slice %1635 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 307

Operation name: stablehlo.concatenate
cache miss
3995287944
%1749 = stablehlo.concatenate %1743, %1744, %1745, %1746, %1747, %1748, dim = 1 : (tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<1x288xf32>
Cost: 542

Operation name: stablehlo.slice
cache miss
342624074
%1750 = stablehlo.slice %1742 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 296

Operation name: stablehlo.slice
cache miss
342624074
%1751 = stablehlo.slice %1715 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 277

Operation name: stablehlo.slice
cache miss
342624074
%1752 = stablehlo.slice %1695 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 370

Operation name: stablehlo.slice
cache miss
342624074
%1753 = stablehlo.slice %1675 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 409

Operation name: stablehlo.slice
cache miss
342624074
%1754 = stablehlo.slice %1655 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 345

Operation name: stablehlo.slice
cache miss
342624074
%1755 = stablehlo.slice %1635 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 304

Operation name: stablehlo.concatenate
cache miss
3995287944
%1756 = stablehlo.concatenate %1750, %1751, %1752, %1753, %1754, %1755, dim = 1 : (tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<1x288xf32>
Cost: 531

Operation name: stablehlo.reshape
cache miss
2392077026
%1757 = stablehlo.reshape %1756 : (tensor<1x288xf32>) -> tensor<144x2xf32>
Cost: 686

Operation name: stablehlo.dot_general
cache miss
1714835160
%1758 = stablehlo.dot_general %1757, %cst_8, batching_dims = [0] x [0], contracting_dims = [1] x [1], precision = [DEFAULT, DEFAULT] : (tensor<144x2xf32>, tensor<144x2x2xf32>) -> tensor<144x2xf32>
Cost: 473

Operation name: stablehlo.reshape
cache miss
1377738304
%1759 = stablehlo.reshape %1758 : (tensor<144x2xf32>) -> tensor<288xf32>
Cost: 278

Operation name: stablehlo.dot_general
cache miss
1491685315
%1760 = stablehlo.dot_general %1759, %917, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288x288xf32>) -> tensor<288xf32>
Cost: 344

Operation name: stablehlo.add
cache miss
3847999983
%1761 = stablehlo.add %1739, %1760 : tensor<288xf32>
Cost: 495

Operation name: stablehlo.dot_general
cache miss
3876455518
%1762 = stablehlo.dot_general %1759, %912, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288xf32>) -> tensor<288x288xf32>
Cost: 23942

Operation name: stablehlo.reshape
cache miss
552616300
%1763 = stablehlo.reshape %1762 : (tensor<288x288xf32>) -> tensor<1x288x288xf32>
Cost: 30892

Operation name: stablehlo.reshape
cache miss
1853321842
%1764 = stablehlo.reshape %1749 : (tensor<1x288xf32>) -> tensor<1x1x288xf32>
Cost: 533

Operation name: stablehlo.dot_general
cache miss
2229247683
%1765 = stablehlo.dot_general %952, %1611, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 333

Operation name: stablehlo.slice
cache miss
3729170049
%1766 = stablehlo.slice %1765 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 369

Operation name: stablehlo.slice
cache miss
3729170049
%1767 = stablehlo.slice %1716 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 276

Operation name: stablehlo.slice
cache miss
3729170049
%1768 = stablehlo.slice %1696 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 342

Operation name: stablehlo.slice
cache miss
3729170049
%1769 = stablehlo.slice %1676 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 172

Operation name: stablehlo.slice
cache miss
3729170049
%1770 = stablehlo.slice %1656 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 174

Operation name: stablehlo.slice
cache miss
3729170049
%1771 = stablehlo.slice %1636 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 338

Operation name: stablehlo.concatenate
cache miss
3995287944
%1772 = stablehlo.concatenate %1766, %1767, %1768, %1769, %1770, %1771, dim = 1 : (tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<1x288xf32>
Cost: 280

Operation name: stablehlo.slice
cache miss
342624074
%1773 = stablehlo.slice %1765 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 343

Operation name: stablehlo.slice
cache miss
342624074
%1774 = stablehlo.slice %1716 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 278

Operation name: stablehlo.slice
cache miss
342624074
%1775 = stablehlo.slice %1696 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 176

Operation name: stablehlo.slice
cache miss
342624074
%1776 = stablehlo.slice %1676 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 174

Operation name: stablehlo.slice
cache miss
342624074
%1777 = stablehlo.slice %1656 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 176

Operation name: stablehlo.slice
cache miss
342624074
%1778 = stablehlo.slice %1636 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 274

Operation name: stablehlo.concatenate
cache miss
3995287944
%1779 = stablehlo.concatenate %1773, %1774, %1775, %1776, %1777, %1778, dim = 1 : (tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<1x288xf32>
Cost: 450

Operation name: stablehlo.reshape
cache miss
1377738304
%1780 = stablehlo.reshape %1779 : (tensor<1x288xf32>) -> tensor<288xf32>
Cost: 406

Operation name: stablehlo.dot_general
cache miss
1491685315
%1781 = stablehlo.dot_general %1780, %920, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288x288xf32>) -> tensor<288xf32>
Cost: 384

Operation name: stablehlo.add
cache miss
3847999983
%1782 = stablehlo.add %1761, %1781 : tensor<288xf32>
Cost: 358

Operation name: stablehlo.multiply
cache miss
2511610444
%1783 = stablehlo.multiply %910, %1782 : tensor<288xf32>
Cost: 265

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 290

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%1784 = stablehlo.reduce(%1783 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<288xf32>, tensor<f32>) -> tensor<f32>
Cost: 396

Operation name: stablehlo.multiply
cache miss
1284973678
%1785 = stablehlo.multiply %1784, %909 : tensor<f32>
Cost: 530

Operation name: stablehlo.negate
cache miss
391293031
%1786 = stablehlo.negate %1785 : tensor<f32>
Cost: 293

Operation name: stablehlo.multiply
cache miss
1284973678
%1787 = stablehlo.multiply %1786, %906 : tensor<f32>
Cost: 302

Operation name: stablehlo.divide
cache miss
719102371
%1788 = stablehlo.divide %1787, %cst_5 : tensor<f32>
Cost: 194

Operation name: stablehlo.dot_general
cache miss
1413888672
%1789 = stablehlo.dot_general %1788, %899, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<f32>, tensor<288xf32>) -> tensor<288xf32>
Cost: 555

Operation name: stablehlo.add
cache miss
3847999983
%1790 = stablehlo.add %1609, %1789 : tensor<288xf32>
Cost: 618

Operation name: stablehlo.add
cache miss
3847999983
%1791 = stablehlo.add %1790, %1789 : tensor<288xf32>
Cost: 511

Operation name: stablehlo.multiply
cache miss
2511610444
%1792 = stablehlo.multiply %1782, %911 : tensor<288xf32>
Cost: 555

Operation name: stablehlo.multiply
cache miss
2511610444
%1793 = stablehlo.multiply %901, %1792 : tensor<288xf32>
Cost: 415

Operation name: stablehlo.add
cache miss
3847999983
%1794 = stablehlo.add %1791, %1793 : tensor<288xf32>
Cost: 404

Operation name: stablehlo.dot_general
cache miss
2840382466
%1795 = stablehlo.dot_general %1794, %897, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288x768xf32>) -> tensor<768xf32>
Cost: 6651

Operation name: stablehlo.multiply
cache miss
4137946040
%1796 = stablehlo.multiply %894, %1795 : tensor<768xf32>
Cost: 480

Operation name: stablehlo.dot_general
cache miss
1491685315
%1797 = stablehlo.dot_general %1796, %886, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<768xf32>, tensor<768x288xf32>) -> tensor<288xf32>
Cost: 6093

Operation name: stablehlo.dot_general
cache miss
1743356395
%1798 = stablehlo.dot_general %1796, %881, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<768xf32>, tensor<288xf32>) -> tensor<768x288xf32>
Cost: 54484

Operation name: stablehlo.reshape
cache miss
1628429499
%1799 = stablehlo.reshape %1798 : (tensor<768x288xf32>) -> tensor<1x768x288xf32>
Cost: 43935

Operation name: stablehlo.multiply
cache miss
4137946040
%1800 = stablehlo.multiply %1795, %887 : tensor<768xf32>
Cost: 534

Operation name: stablehlo.multiply
cache miss
4137946040
%1801 = stablehlo.multiply %884, %1800 : tensor<768xf32>
Cost: 466

Operation name: stablehlo.multiply
cache miss
4137946040
%1802 = stablehlo.multiply %1801, %893 : tensor<768xf32>
Cost: 475

Operation name: stablehlo.negate
cache miss
4256221255
%1803 = stablehlo.negate %1802 : tensor<768xf32>
Cost: 572

Operation name: stablehlo.multiply
cache miss
4137946040
%1804 = stablehlo.multiply %1803, %889 : tensor<768xf32>
Cost: 508

Operation name: stablehlo.negate
cache miss
4256221255
%1805 = stablehlo.negate %1804 : tensor<768xf32>
Cost: 552

Operation name: stablehlo.multiply
cache miss
4137946040
%1806 = stablehlo.multiply %1800, %891 : tensor<768xf32>
Cost: 457

Operation name: stablehlo.add
cache miss
389450582
%1807 = stablehlo.add %1805, %1806 : tensor<768xf32>
Cost: 504

Operation name: stablehlo.dot_general
cache miss
1491685315
%1808 = stablehlo.dot_general %1807, %883, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<768xf32>, tensor<768x288xf32>) -> tensor<288xf32>
Cost: 5222

Operation name: stablehlo.add
cache miss
3847999983
%1809 = stablehlo.add %1797, %1808 : tensor<288xf32>
Cost: 401

Operation name: stablehlo.multiply
cache miss
2511610444
%1810 = stablehlo.multiply %879, %1809 : tensor<288xf32>
Cost: 386

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 324

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%1811 = stablehlo.reduce(%1810 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<288xf32>, tensor<f32>) -> tensor<f32>
Cost: 458

Operation name: stablehlo.multiply
cache miss
1284973678
%1812 = stablehlo.multiply %1811, %878 : tensor<f32>
Cost: 193

Operation name: stablehlo.negate
cache miss
391293031
%1813 = stablehlo.negate %1812 : tensor<f32>
Cost: 196

Operation name: stablehlo.multiply
cache miss
1284973678
%1814 = stablehlo.multiply %1813, %875 : tensor<f32>
Cost: 567

Operation name: stablehlo.divide
cache miss
719102371
%1815 = stablehlo.divide %1814, %cst_5 : tensor<f32>
Cost: 326

Operation name: stablehlo.dot_general
cache miss
1413888672
%1816 = stablehlo.dot_general %1815, %868, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<f32>, tensor<288xf32>) -> tensor<288xf32>
Cost: 610

Operation name: stablehlo.add
cache miss
3847999983
%1817 = stablehlo.add %1794, %1816 : tensor<288xf32>
Cost: 529

Operation name: stablehlo.add
cache miss
3847999983
%1818 = stablehlo.add %1817, %1816 : tensor<288xf32>
Cost: 532

Operation name: stablehlo.multiply
cache miss
2511610444
%1819 = stablehlo.multiply %1809, %880 : tensor<288xf32>
Cost: 464

Operation name: stablehlo.multiply
cache miss
2511610444
%1820 = stablehlo.multiply %870, %1819 : tensor<288xf32>
Cost: 264

Operation name: stablehlo.add
cache miss
3847999983
%1821 = stablehlo.add %1818, %1820 : tensor<288xf32>
Cost: 434

Operation name: stablehlo.dot_general
cache miss
1491685315
%1822 = stablehlo.dot_general %1821, %866, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288x288xf32>) -> tensor<288xf32>
Cost: 230

Operation name: stablehlo.slice
cache miss
2839717066
%1823 = stablehlo.slice %1822 [0:48] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 174

Operation name: stablehlo.slice
cache miss
4032389970
%1824 = stablehlo.slice %1822 [48:96] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 388

Operation name: stablehlo.slice
cache miss
1375222232
%1825 = stablehlo.slice %1822 [96:144] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 368

Operation name: stablehlo.slice
cache miss
3469783879
%1826 = stablehlo.slice %1822 [144:192] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 276

Operation name: stablehlo.slice
cache miss
1801550340
%1827 = stablehlo.slice %1822 [192:240] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 172

Operation name: stablehlo.slice
cache miss
2798019243
%1828 = stablehlo.slice %1822 [240:288] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 310

Operation name: stablehlo.dot_general
cache miss
597698865
%1829 = stablehlo.dot_general %1828, %862, contracting_dims = [0] x [1], precision = [DEFAULT, DEFAULT] : (tensor<48xf32>, tensor<2x48xf32>) -> tensor<2xf32>
Cost: 312

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1830 = stablehlo.broadcast_in_dim %859, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 386

Operation name: stablehlo.multiply
cache miss
806204014
%1831 = stablehlo.multiply %1829, %1830 : tensor<2xf32>
Cost: 410

Operation name: stablehlo.multiply
cache miss
806204014
%1832 = stablehlo.multiply %1831, %850 : tensor<2xf32>
Cost: 258

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 404

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%1833 = stablehlo.reduce(%1832 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 399

Operation name: stablehlo.negate
cache miss
391293031
%1834 = stablehlo.negate %1833 : tensor<f32>
Cost: 450

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1835 = stablehlo.broadcast_in_dim %1834, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 324

Operation name: stablehlo.divide
cache miss
1531341356
%1836 = stablehlo.divide %1829, %856 : tensor<2xf32>
Cost: 293

Operation name: stablehlo.add
cache miss
1018830424
%1837 = stablehlo.add %1835, %1836 : tensor<2xf32>
Cost: 298

Operation name: stablehlo.multiply
cache miss
806204014
%1838 = stablehlo.multiply %1837, %850 : tensor<2xf32>
Cost: 324

Operation name: stablehlo.negate
cache miss
37341164
%1839 = stablehlo.negate %1838 : tensor<2xf32>
Cost: 323

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 303

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%1840 = stablehlo.reduce(%1839 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 307

Operation name: stablehlo.divide
cache miss
719102371
%1841 = stablehlo.divide %1840, %848 : tensor<f32>
Cost: 190

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1842 = stablehlo.broadcast_in_dim %1841, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 320

Operation name: stablehlo.multiply
cache miss
806204014
%1843 = stablehlo.multiply %1842, %847 : tensor<2xf32>
Cost: 324

Operation name: stablehlo.add
cache miss
1018830424
%1844 = stablehlo.add %1838, %1843 : tensor<2xf32>
Cost: 239

Operation name: stablehlo.divide
cache miss
1531341356
%1845 = stablehlo.divide %1844, %cst_1 : tensor<2xf32>
Cost: 297

Operation name: stablehlo.dot_general
cache miss
2902940993
%1846 = stablehlo.dot_general %1845, %841, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<2x48xf32>) -> tensor<48xf32>
Cost: 168

Operation name: stablehlo.dot_general
cache miss
2229247683
%1847 = stablehlo.dot_general %1845, %838, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 176

Operation name: stablehlo.dot_general
cache miss
2229247683
%1848 = stablehlo.dot_general %857, %1828, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 392

Operation name: stablehlo.dot_general
cache miss
597698865
%1849 = stablehlo.dot_general %1827, %836, contracting_dims = [0] x [1], precision = [DEFAULT, DEFAULT] : (tensor<48xf32>, tensor<2x48xf32>) -> tensor<2xf32>
Cost: 392

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1850 = stablehlo.broadcast_in_dim %833, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 392

Operation name: stablehlo.multiply
cache miss
806204014
%1851 = stablehlo.multiply %1849, %1850 : tensor<2xf32>
Cost: 389

Operation name: stablehlo.multiply
cache miss
806204014
%1852 = stablehlo.multiply %1851, %824 : tensor<2xf32>
Cost: 289

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 421

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%1853 = stablehlo.reduce(%1852 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 390

Operation name: stablehlo.negate
cache miss
391293031
%1854 = stablehlo.negate %1853 : tensor<f32>
Cost: 333

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1855 = stablehlo.broadcast_in_dim %1854, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 404

Operation name: stablehlo.divide
cache miss
1531341356
%1856 = stablehlo.divide %1849, %830 : tensor<2xf32>
Cost: 284

Operation name: stablehlo.add
cache miss
1018830424
%1857 = stablehlo.add %1855, %1856 : tensor<2xf32>
Cost: 406

Operation name: stablehlo.multiply
cache miss
806204014
%1858 = stablehlo.multiply %1857, %824 : tensor<2xf32>
Cost: 297

Operation name: stablehlo.negate
cache miss
37341164
%1859 = stablehlo.negate %1858 : tensor<2xf32>
Cost: 285

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 410

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%1860 = stablehlo.reduce(%1859 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 282

Operation name: stablehlo.divide
cache miss
719102371
%1861 = stablehlo.divide %1860, %822 : tensor<f32>
Cost: 182

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1862 = stablehlo.broadcast_in_dim %1861, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 178

Operation name: stablehlo.multiply
cache miss
806204014
%1863 = stablehlo.multiply %1862, %821 : tensor<2xf32>
Cost: 403

Operation name: stablehlo.add
cache miss
1018830424
%1864 = stablehlo.add %1858, %1863 : tensor<2xf32>
Cost: 382

Operation name: stablehlo.divide
cache miss
1531341356
%1865 = stablehlo.divide %1864, %cst_1 : tensor<2xf32>
Cost: 380

Operation name: stablehlo.dot_general
cache miss
2902940993
%1866 = stablehlo.dot_general %1865, %815, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<2x48xf32>) -> tensor<48xf32>
Cost: 168

Operation name: stablehlo.dot_general
cache miss
2229247683
%1867 = stablehlo.dot_general %1865, %812, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 280

Operation name: stablehlo.dot_general
cache miss
2229247683
%1868 = stablehlo.dot_general %831, %1827, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 377

Operation name: stablehlo.dot_general
cache miss
597698865
%1869 = stablehlo.dot_general %1826, %810, contracting_dims = [0] x [1], precision = [DEFAULT, DEFAULT] : (tensor<48xf32>, tensor<2x48xf32>) -> tensor<2xf32>
Cost: 173

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1870 = stablehlo.broadcast_in_dim %807, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 284

Operation name: stablehlo.multiply
cache miss
806204014
%1871 = stablehlo.multiply %1869, %1870 : tensor<2xf32>
Cost: 364

Operation name: stablehlo.multiply
cache miss
806204014
%1872 = stablehlo.multiply %1871, %798 : tensor<2xf32>
Cost: 272

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 178

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%1873 = stablehlo.reduce(%1872 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 303

Operation name: stablehlo.negate
cache miss
391293031
%1874 = stablehlo.negate %1873 : tensor<f32>
Cost: 170

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1875 = stablehlo.broadcast_in_dim %1874, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 365

Operation name: stablehlo.divide
cache miss
1531341356
%1876 = stablehlo.divide %1869, %804 : tensor<2xf32>
Cost: 470

Operation name: stablehlo.add
cache miss
1018830424
%1877 = stablehlo.add %1875, %1876 : tensor<2xf32>
Cost: 272

Operation name: stablehlo.multiply
cache miss
806204014
%1878 = stablehlo.multiply %1877, %798 : tensor<2xf32>
Cost: 361

Operation name: stablehlo.negate
cache miss
37341164
%1879 = stablehlo.negate %1878 : tensor<2xf32>
Cost: 338

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 361

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%1880 = stablehlo.reduce(%1879 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 181

Operation name: stablehlo.divide
cache miss
719102371
%1881 = stablehlo.divide %1880, %796 : tensor<f32>
Cost: 274

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1882 = stablehlo.broadcast_in_dim %1881, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 323

Operation name: stablehlo.multiply
cache miss
806204014
%1883 = stablehlo.multiply %1882, %795 : tensor<2xf32>
Cost: 339

Operation name: stablehlo.add
cache miss
1018830424
%1884 = stablehlo.add %1878, %1883 : tensor<2xf32>
Cost: 272

Operation name: stablehlo.divide
cache miss
1531341356
%1885 = stablehlo.divide %1884, %cst_1 : tensor<2xf32>
Cost: 237

Operation name: stablehlo.dot_general
cache miss
2902940993
%1886 = stablehlo.dot_general %1885, %789, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<2x48xf32>) -> tensor<48xf32>
Cost: 265

Operation name: stablehlo.dot_general
cache miss
2229247683
%1887 = stablehlo.dot_general %1885, %786, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 369

Operation name: stablehlo.dot_general
cache miss
2229247683
%1888 = stablehlo.dot_general %805, %1826, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 375

Operation name: stablehlo.dot_general
cache miss
597698865
%1889 = stablehlo.dot_general %1825, %784, contracting_dims = [0] x [1], precision = [DEFAULT, DEFAULT] : (tensor<48xf32>, tensor<2x48xf32>) -> tensor<2xf32>
Cost: 387

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1890 = stablehlo.broadcast_in_dim %781, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 364

Operation name: stablehlo.multiply
cache miss
806204014
%1891 = stablehlo.multiply %1889, %1890 : tensor<2xf32>
Cost: 177

Operation name: stablehlo.multiply
cache miss
806204014
%1892 = stablehlo.multiply %1891, %772 : tensor<2xf32>
Cost: 384

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 362

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%1893 = stablehlo.reduce(%1892 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 366

Operation name: stablehlo.negate
cache miss
391293031
%1894 = stablehlo.negate %1893 : tensor<f32>
Cost: 280

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1895 = stablehlo.broadcast_in_dim %1894, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 361

Operation name: stablehlo.divide
cache miss
1531341356
%1896 = stablehlo.divide %1889, %778 : tensor<2xf32>
Cost: 364

Operation name: stablehlo.add
cache miss
1018830424
%1897 = stablehlo.add %1895, %1896 : tensor<2xf32>
Cost: 362

Operation name: stablehlo.multiply
cache miss
806204014
%1898 = stablehlo.multiply %1897, %772 : tensor<2xf32>
Cost: 363

Operation name: stablehlo.negate
cache miss
37341164
%1899 = stablehlo.negate %1898 : tensor<2xf32>
Cost: 359

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 458

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%1900 = stablehlo.reduce(%1899 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 366

Operation name: stablehlo.divide
cache miss
719102371
%1901 = stablehlo.divide %1900, %770 : tensor<f32>
Cost: 270

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1902 = stablehlo.broadcast_in_dim %1901, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 170

Operation name: stablehlo.multiply
cache miss
806204014
%1903 = stablehlo.multiply %1902, %769 : tensor<2xf32>
Cost: 296

Operation name: stablehlo.add
cache miss
1018830424
%1904 = stablehlo.add %1898, %1903 : tensor<2xf32>
Cost: 173

Operation name: stablehlo.divide
cache miss
1531341356
%1905 = stablehlo.divide %1904, %cst_1 : tensor<2xf32>
Cost: 370

Operation name: stablehlo.dot_general
cache miss
2902940993
%1906 = stablehlo.dot_general %1905, %763, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<2x48xf32>) -> tensor<48xf32>
Cost: 324

Operation name: stablehlo.dot_general
cache miss
2229247683
%1907 = stablehlo.dot_general %1905, %760, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 288

Operation name: stablehlo.dot_general
cache miss
2229247683
%1908 = stablehlo.dot_general %779, %1825, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 176

Operation name: stablehlo.dot_general
cache miss
597698865
%1909 = stablehlo.dot_general %1824, %758, contracting_dims = [0] x [1], precision = [DEFAULT, DEFAULT] : (tensor<48xf32>, tensor<2x48xf32>) -> tensor<2xf32>
Cost: 273

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1910 = stablehlo.broadcast_in_dim %755, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 277

Operation name: stablehlo.multiply
cache miss
806204014
%1911 = stablehlo.multiply %1909, %1910 : tensor<2xf32>
Cost: 365

Operation name: stablehlo.multiply
cache miss
806204014
%1912 = stablehlo.multiply %1911, %746 : tensor<2xf32>
Cost: 272

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 389

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%1913 = stablehlo.reduce(%1912 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 273

Operation name: stablehlo.negate
cache miss
391293031
%1914 = stablehlo.negate %1913 : tensor<f32>
Cost: 277

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1915 = stablehlo.broadcast_in_dim %1914, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 171

Operation name: stablehlo.divide
cache miss
1531341356
%1916 = stablehlo.divide %1909, %752 : tensor<2xf32>
Cost: 277

Operation name: stablehlo.add
cache miss
1018830424
%1917 = stablehlo.add %1915, %1916 : tensor<2xf32>
Cost: 272

Operation name: stablehlo.multiply
cache miss
806204014
%1918 = stablehlo.multiply %1917, %746 : tensor<2xf32>
Cost: 365

Operation name: stablehlo.negate
cache miss
37341164
%1919 = stablehlo.negate %1918 : tensor<2xf32>
Cost: 276

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 374

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%1920 = stablehlo.reduce(%1919 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 369

Operation name: stablehlo.divide
cache miss
719102371
%1921 = stablehlo.divide %1920, %744 : tensor<f32>
Cost: 175

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1922 = stablehlo.broadcast_in_dim %1921, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 271

Operation name: stablehlo.multiply
cache miss
806204014
%1923 = stablehlo.multiply %1922, %743 : tensor<2xf32>
Cost: 431

Operation name: stablehlo.add
cache miss
1018830424
%1924 = stablehlo.add %1918, %1923 : tensor<2xf32>
Cost: 173

Operation name: stablehlo.divide
cache miss
1531341356
%1925 = stablehlo.divide %1924, %cst_1 : tensor<2xf32>
Cost: 363

Operation name: stablehlo.dot_general
cache miss
2902940993
%1926 = stablehlo.dot_general %1925, %737, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<2x48xf32>) -> tensor<48xf32>
Cost: 266

Operation name: stablehlo.dot_general
cache miss
2229247683
%1927 = stablehlo.dot_general %1925, %734, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 310

Operation name: stablehlo.dot_general
cache miss
2229247683
%1928 = stablehlo.dot_general %753, %1824, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 177

Operation name: stablehlo.dot_general
cache miss
597698865
%1929 = stablehlo.dot_general %1823, %732, contracting_dims = [0] x [1], precision = [DEFAULT, DEFAULT] : (tensor<48xf32>, tensor<2x48xf32>) -> tensor<2xf32>
Cost: 285

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1930 = stablehlo.broadcast_in_dim %729, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 360

Operation name: stablehlo.multiply
cache miss
806204014
%1931 = stablehlo.multiply %1929, %1930 : tensor<2xf32>
Cost: 275

Operation name: stablehlo.multiply
cache miss
806204014
%1932 = stablehlo.multiply %1931, %720 : tensor<2xf32>
Cost: 171

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 271

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%1933 = stablehlo.reduce(%1932 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 367

Operation name: stablehlo.negate
cache miss
391293031
%1934 = stablehlo.negate %1933 : tensor<f32>
Cost: 276

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1935 = stablehlo.broadcast_in_dim %1934, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 382

Operation name: stablehlo.divide
cache miss
1531341356
%1936 = stablehlo.divide %1929, %726 : tensor<2xf32>
Cost: 274

Operation name: stablehlo.add
cache miss
1018830424
%1937 = stablehlo.add %1935, %1936 : tensor<2xf32>
Cost: 275

Operation name: stablehlo.multiply
cache miss
806204014
%1938 = stablehlo.multiply %1937, %720 : tensor<2xf32>
Cost: 364

Operation name: stablehlo.negate
cache miss
37341164
%1939 = stablehlo.negate %1938 : tensor<2xf32>
Cost: 362

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 368

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%1940 = stablehlo.reduce(%1939 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 364

Operation name: stablehlo.divide
cache miss
719102371
%1941 = stablehlo.divide %1940, %718 : tensor<f32>
Cost: 276

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%1942 = stablehlo.broadcast_in_dim %1941, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 454

Operation name: stablehlo.multiply
cache miss
806204014
%1943 = stablehlo.multiply %1942, %717 : tensor<2xf32>
Cost: 291

Operation name: stablehlo.add
cache miss
1018830424
%1944 = stablehlo.add %1938, %1943 : tensor<2xf32>
Cost: 358

Operation name: stablehlo.divide
cache miss
1531341356
%1945 = stablehlo.divide %1944, %cst_1 : tensor<2xf32>
Cost: 362

Operation name: stablehlo.dot_general
cache miss
2902940993
%1946 = stablehlo.dot_general %1945, %711, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<2x48xf32>) -> tensor<48xf32>
Cost: 501

Operation name: stablehlo.concatenate
cache miss
292037604
%1947 = stablehlo.concatenate %1946, %1926, %1906, %1886, %1866, %1846, dim = 0 : (tensor<48xf32>, tensor<48xf32>, tensor<48xf32>, tensor<48xf32>, tensor<48xf32>, tensor<48xf32>) -> tensor<288xf32>
Cost: 413

Operation name: stablehlo.reshape
cache miss
2392077026
%1948 = stablehlo.reshape %1947 : (tensor<288xf32>) -> tensor<144x2xf32>
Cost: 510

Operation name: stablehlo.dot_general
cache miss
1714835160
%1949 = stablehlo.dot_general %1948, %cst_8, batching_dims = [0] x [0], contracting_dims = [1] x [1], precision = [DEFAULT, DEFAULT] : (tensor<144x2xf32>, tensor<144x2x2xf32>) -> tensor<144x2xf32>
Cost: 401

Operation name: stablehlo.reshape
cache miss
1377738304
%1950 = stablehlo.reshape %1949 : (tensor<144x2xf32>) -> tensor<288xf32>
Cost: 517

Operation name: stablehlo.dot_general
cache miss
1491685315
%1951 = stablehlo.dot_general %1950, %689, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288x288xf32>) -> tensor<288xf32>
Cost: 233

Operation name: stablehlo.dot_general
cache miss
3876455518
%1952 = stablehlo.dot_general %1950, %687, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288xf32>) -> tensor<288x288xf32>
Cost: 28441

Operation name: stablehlo.reshape
cache miss
552616300
%1953 = stablehlo.reshape %1952 : (tensor<288x288xf32>) -> tensor<1x288x288xf32>
Cost: 18333

Operation name: stablehlo.dot_general
cache miss
2229247683
%1954 = stablehlo.dot_general %1945, %708, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 372

Operation name: stablehlo.slice
cache miss
3729170049
%1955 = stablehlo.slice %1954 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 391

Operation name: stablehlo.slice
cache miss
3729170049
%1956 = stablehlo.slice %1927 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 277

Operation name: stablehlo.slice
cache miss
3729170049
%1957 = stablehlo.slice %1907 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 281

Operation name: stablehlo.slice
cache miss
3729170049
%1958 = stablehlo.slice %1887 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 373

Operation name: stablehlo.slice
cache miss
3729170049
%1959 = stablehlo.slice %1867 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 512

Operation name: stablehlo.slice
cache miss
3729170049
%1960 = stablehlo.slice %1847 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 277

Operation name: stablehlo.concatenate
cache miss
3995287944
%1961 = stablehlo.concatenate %1955, %1956, %1957, %1958, %1959, %1960, dim = 1 : (tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<1x288xf32>
Cost: 414

Operation name: stablehlo.slice
cache miss
342624074
%1962 = stablehlo.slice %1954 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 175

Operation name: stablehlo.slice
cache miss
342624074
%1963 = stablehlo.slice %1927 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 175

Operation name: stablehlo.slice
cache miss
342624074
%1964 = stablehlo.slice %1907 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 282

Operation name: stablehlo.slice
cache miss
342624074
%1965 = stablehlo.slice %1887 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 372

Operation name: stablehlo.slice
cache miss
342624074
%1966 = stablehlo.slice %1867 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 275

Operation name: stablehlo.slice
cache miss
342624074
%1967 = stablehlo.slice %1847 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 176

Operation name: stablehlo.concatenate
cache miss
3995287944
%1968 = stablehlo.concatenate %1962, %1963, %1964, %1965, %1966, %1967, dim = 1 : (tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<1x288xf32>
Cost: 415

Operation name: stablehlo.reshape
cache miss
2392077026
%1969 = stablehlo.reshape %1968 : (tensor<1x288xf32>) -> tensor<144x2xf32>
Cost: 391

Operation name: stablehlo.dot_general
cache miss
1714835160
%1970 = stablehlo.dot_general %1969, %cst_8, batching_dims = [0] x [0], contracting_dims = [1] x [1], precision = [DEFAULT, DEFAULT] : (tensor<144x2xf32>, tensor<144x2x2xf32>) -> tensor<144x2xf32>
Cost: 597

Operation name: stablehlo.reshape
cache miss
1377738304
%1971 = stablehlo.reshape %1970 : (tensor<144x2xf32>) -> tensor<288xf32>
Cost: 394

Operation name: stablehlo.dot_general
cache miss
1491685315
%1972 = stablehlo.dot_general %1971, %692, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288x288xf32>) -> tensor<288xf32>
Cost: 597

Operation name: stablehlo.add
cache miss
3847999983
%1973 = stablehlo.add %1951, %1972 : tensor<288xf32>
Cost: 620

Operation name: stablehlo.dot_general
cache miss
3876455518
%1974 = stablehlo.dot_general %1971, %687, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288xf32>) -> tensor<288x288xf32>
Cost: 45245

Operation name: stablehlo.reshape
cache miss
552616300
%1975 = stablehlo.reshape %1974 : (tensor<288x288xf32>) -> tensor<1x288x288xf32>
Cost: 27544

Operation name: stablehlo.reshape
cache miss
1853321842
%1976 = stablehlo.reshape %1961 : (tensor<1x288xf32>) -> tensor<1x1x288xf32>
Cost: 433

Operation name: stablehlo.dot_general
cache miss
2229247683
%1977 = stablehlo.dot_general %727, %1823, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 372

Operation name: stablehlo.slice
cache miss
3729170049
%1978 = stablehlo.slice %1977 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 276

Operation name: stablehlo.slice
cache miss
3729170049
%1979 = stablehlo.slice %1928 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 373

Operation name: stablehlo.slice
cache miss
3729170049
%1980 = stablehlo.slice %1908 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 370

Operation name: stablehlo.slice
cache miss
3729170049
%1981 = stablehlo.slice %1888 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 369

Operation name: stablehlo.slice
cache miss
3729170049
%1982 = stablehlo.slice %1868 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 295

Operation name: stablehlo.slice
cache miss
3729170049
%1983 = stablehlo.slice %1848 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 277

Operation name: stablehlo.concatenate
cache miss
3995287944
%1984 = stablehlo.concatenate %1978, %1979, %1980, %1981, %1982, %1983, dim = 1 : (tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<1x288xf32>
Cost: 504

Operation name: stablehlo.slice
cache miss
342624074
%1985 = stablehlo.slice %1977 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 366

Operation name: stablehlo.slice
cache miss
342624074
%1986 = stablehlo.slice %1928 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 275

Operation name: stablehlo.slice
cache miss
342624074
%1987 = stablehlo.slice %1908 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 447

Operation name: stablehlo.slice
cache miss
342624074
%1988 = stablehlo.slice %1888 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 293

Operation name: stablehlo.slice
cache miss
342624074
%1989 = stablehlo.slice %1868 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 174

Operation name: stablehlo.slice
cache miss
342624074
%1990 = stablehlo.slice %1848 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 276

Operation name: stablehlo.concatenate
cache miss
3995287944
%1991 = stablehlo.concatenate %1985, %1986, %1987, %1988, %1989, %1990, dim = 1 : (tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<1x288xf32>
Cost: 774

Operation name: stablehlo.reshape
cache miss
1377738304
%1992 = stablehlo.reshape %1991 : (tensor<1x288xf32>) -> tensor<288xf32>
Cost: 408

Operation name: stablehlo.dot_general
cache miss
1491685315
%1993 = stablehlo.dot_general %1992, %695, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288x288xf32>) -> tensor<288xf32>
Cost: 353

Operation name: stablehlo.add
cache miss
3847999983
%1994 = stablehlo.add %1973, %1993 : tensor<288xf32>
Cost: 517

Operation name: stablehlo.multiply
cache miss
2511610444
%1995 = stablehlo.multiply %685, %1994 : tensor<288xf32>
Cost: 549

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 296

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%1996 = stablehlo.reduce(%1995 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<288xf32>, tensor<f32>) -> tensor<f32>
Cost: 293

Operation name: stablehlo.multiply
cache miss
1284973678
%1997 = stablehlo.multiply %1996, %684 : tensor<f32>
Cost: 298

Operation name: stablehlo.negate
cache miss
391293031
%1998 = stablehlo.negate %1997 : tensor<f32>
Cost: 193

Operation name: stablehlo.multiply
cache miss
1284973678
%1999 = stablehlo.multiply %1998, %681 : tensor<f32>
Cost: 193

Operation name: stablehlo.divide
cache miss
719102371
%2000 = stablehlo.divide %1999, %cst_5 : tensor<f32>
Cost: 406

Operation name: stablehlo.dot_general
cache miss
1413888672
%2001 = stablehlo.dot_general %2000, %674, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<f32>, tensor<288xf32>) -> tensor<288xf32>
Cost: 546

Operation name: stablehlo.add
cache miss
3847999983
%2002 = stablehlo.add %1821, %2001 : tensor<288xf32>
Cost: 270

Operation name: stablehlo.add
cache miss
3847999983
%2003 = stablehlo.add %2002, %2001 : tensor<288xf32>
Cost: 526

Operation name: stablehlo.multiply
cache miss
2511610444
%2004 = stablehlo.multiply %1994, %686 : tensor<288xf32>
Cost: 518

Operation name: stablehlo.multiply
cache miss
2511610444
%2005 = stablehlo.multiply %676, %2004 : tensor<288xf32>
Cost: 427

Operation name: stablehlo.add
cache miss
3847999983
%2006 = stablehlo.add %2003, %2005 : tensor<288xf32>
Cost: 418

Operation name: stablehlo.dot_general
cache miss
2840382466
%2007 = stablehlo.dot_general %2006, %672, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288x768xf32>) -> tensor<768xf32>
Cost: 8781

Operation name: stablehlo.multiply
cache miss
4137946040
%2008 = stablehlo.multiply %669, %2007 : tensor<768xf32>
Cost: 591

Operation name: stablehlo.dot_general
cache miss
1491685315
%2009 = stablehlo.dot_general %2008, %661, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<768xf32>, tensor<768x288xf32>) -> tensor<288xf32>
Cost: 5912

Operation name: stablehlo.dot_general
cache miss
1743356395
%2010 = stablehlo.dot_general %2008, %656, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<768xf32>, tensor<288xf32>) -> tensor<768x288xf32>
Cost: 89148

Operation name: stablehlo.reshape
cache miss
1628429499
%2011 = stablehlo.reshape %2010 : (tensor<768x288xf32>) -> tensor<1x768x288xf32>
Cost: 71385

Operation name: stablehlo.multiply
cache miss
4137946040
%2012 = stablehlo.multiply %2007, %662 : tensor<768xf32>
Cost: 301

Operation name: stablehlo.multiply
cache miss
4137946040
%2013 = stablehlo.multiply %659, %2012 : tensor<768xf32>
Cost: 569

Operation name: stablehlo.multiply
cache miss
4137946040
%2014 = stablehlo.multiply %2013, %668 : tensor<768xf32>
Cost: 570

Operation name: stablehlo.negate
cache miss
4256221255
%2015 = stablehlo.negate %2014 : tensor<768xf32>
Cost: 426

Operation name: stablehlo.multiply
cache miss
4137946040
%2016 = stablehlo.multiply %2015, %664 : tensor<768xf32>
Cost: 626

Operation name: stablehlo.negate
cache miss
4256221255
%2017 = stablehlo.negate %2016 : tensor<768xf32>
Cost: 420

Operation name: stablehlo.multiply
cache miss
4137946040
%2018 = stablehlo.multiply %2012, %666 : tensor<768xf32>
Cost: 488

Operation name: stablehlo.add
cache miss
389450582
%2019 = stablehlo.add %2017, %2018 : tensor<768xf32>
Cost: 300

Operation name: stablehlo.dot_general
cache miss
1491685315
%2020 = stablehlo.dot_general %2019, %658, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<768xf32>, tensor<768x288xf32>) -> tensor<288xf32>
Cost: 3844

Operation name: stablehlo.add
cache miss
3847999983
%2021 = stablehlo.add %2009, %2020 : tensor<288xf32>
Cost: 268

Operation name: stablehlo.multiply
cache miss
2511610444
%2022 = stablehlo.multiply %654, %2021 : tensor<288xf32>
Cost: 420

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 265

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%2023 = stablehlo.reduce(%2022 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<288xf32>, tensor<f32>) -> tensor<f32>
Cost: 296

Operation name: stablehlo.multiply
cache miss
1284973678
%2024 = stablehlo.multiply %2023, %653 : tensor<f32>
Cost: 311

Operation name: stablehlo.negate
cache miss
391293031
%2025 = stablehlo.negate %2024 : tensor<f32>
Cost: 194

Operation name: stablehlo.multiply
cache miss
1284973678
%2026 = stablehlo.multiply %2025, %650 : tensor<f32>
Cost: 305

Operation name: stablehlo.divide
cache miss
719102371
%2027 = stablehlo.divide %2026, %cst_5 : tensor<f32>
Cost: 197

Operation name: stablehlo.dot_general
cache miss
1413888672
%2028 = stablehlo.dot_general %2027, %643, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<f32>, tensor<288xf32>) -> tensor<288xf32>
Cost: 512

Operation name: stablehlo.add
cache miss
3847999983
%2029 = stablehlo.add %2006, %2028 : tensor<288xf32>
Cost: 553

Operation name: stablehlo.add
cache miss
3847999983
%2030 = stablehlo.add %2029, %2028 : tensor<288xf32>
Cost: 416

Operation name: stablehlo.multiply
cache miss
2511610444
%2031 = stablehlo.multiply %2021, %655 : tensor<288xf32>
Cost: 464

Operation name: stablehlo.multiply
cache miss
2511610444
%2032 = stablehlo.multiply %645, %2031 : tensor<288xf32>
Cost: 407

Operation name: stablehlo.add
cache miss
3847999983
%2033 = stablehlo.add %2030, %2032 : tensor<288xf32>
Cost: 410

Operation name: stablehlo.dot_general
cache miss
1491685315
%2034 = stablehlo.dot_general %2033, %641, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288x288xf32>) -> tensor<288xf32>
Cost: 356

Operation name: stablehlo.slice
cache miss
2839717066
%2035 = stablehlo.slice %2034 [0:48] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 276

Operation name: stablehlo.slice
cache miss
4032389970
%2036 = stablehlo.slice %2034 [48:96] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 395

Operation name: stablehlo.slice
cache miss
1375222232
%2037 = stablehlo.slice %2034 [96:144] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 282

Operation name: stablehlo.slice
cache miss
3469783879
%2038 = stablehlo.slice %2034 [144:192] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 280

Operation name: stablehlo.slice
cache miss
1801550340
%2039 = stablehlo.slice %2034 [192:240] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 176

Operation name: stablehlo.slice
cache miss
2798019243
%2040 = stablehlo.slice %2034 [240:288] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 174

Operation name: stablehlo.dot_general
cache miss
597698865
%2041 = stablehlo.dot_general %2040, %637, contracting_dims = [0] x [1], precision = [DEFAULT, DEFAULT] : (tensor<48xf32>, tensor<2x48xf32>) -> tensor<2xf32>
Cost: 293

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%2042 = stablehlo.broadcast_in_dim %634, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 298

Operation name: stablehlo.multiply
cache miss
806204014
%2043 = stablehlo.multiply %2041, %2042 : tensor<2xf32>
Cost: 409

Operation name: stablehlo.multiply
cache miss
806204014
%2044 = stablehlo.multiply %2043, %625 : tensor<2xf32>
Cost: 386

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 390

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%2045 = stablehlo.reduce(%2044 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 394

Operation name: stablehlo.negate
cache miss
391293031
%2046 = stablehlo.negate %2045 : tensor<f32>
Cost: 297

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%2047 = stablehlo.broadcast_in_dim %2046, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 557

Operation name: stablehlo.divide
cache miss
1531341356
%2048 = stablehlo.divide %2041, %631 : tensor<2xf32>
Cost: 406

Operation name: stablehlo.add
cache miss
1018830424
%2049 = stablehlo.add %2047, %2048 : tensor<2xf32>
Cost: 300

Operation name: stablehlo.multiply
cache miss
806204014
%2050 = stablehlo.multiply %2049, %625 : tensor<2xf32>
Cost: 388

Operation name: stablehlo.negate
cache miss
37341164
%2051 = stablehlo.negate %2050 : tensor<2xf32>
Cost: 327

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 326

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%2052 = stablehlo.reduce(%2051 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 505

Operation name: stablehlo.divide
cache miss
719102371
%2053 = stablehlo.divide %2052, %623 : tensor<f32>
Cost: 311

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%2054 = stablehlo.broadcast_in_dim %2053, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 404

Operation name: stablehlo.multiply
cache miss
806204014
%2055 = stablehlo.multiply %2054, %622 : tensor<2xf32>
Cost: 391

Operation name: stablehlo.add
cache miss
1018830424
%2056 = stablehlo.add %2050, %2055 : tensor<2xf32>
Cost: 393

Operation name: stablehlo.divide
cache miss
1531341356
%2057 = stablehlo.divide %2056, %cst_1 : tensor<2xf32>
Cost: 199

Operation name: stablehlo.dot_general
cache miss
2902940993
%2058 = stablehlo.dot_general %2057, %616, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<2x48xf32>) -> tensor<48xf32>
Cost: 352

Operation name: stablehlo.dot_general
cache miss
2229247683
%2059 = stablehlo.dot_general %2057, %613, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 281

Operation name: stablehlo.dot_general
cache miss
2229247683
%2060 = stablehlo.dot_general %632, %2040, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 374

Operation name: stablehlo.dot_general
cache miss
597698865
%2061 = stablehlo.dot_general %2039, %611, contracting_dims = [0] x [1], precision = [DEFAULT, DEFAULT] : (tensor<48xf32>, tensor<2x48xf32>) -> tensor<2xf32>
Cost: 382

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%2062 = stablehlo.broadcast_in_dim %608, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 290

Operation name: stablehlo.multiply
cache miss
806204014
%2063 = stablehlo.multiply %2061, %2062 : tensor<2xf32>
Cost: 386

Operation name: stablehlo.multiply
cache miss
806204014
%2064 = stablehlo.multiply %2063, %599 : tensor<2xf32>
Cost: 412

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 428

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%2065 = stablehlo.reduce(%2064 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 289

Operation name: stablehlo.negate
cache miss
391293031
%2066 = stablehlo.negate %2065 : tensor<f32>
Cost: 526

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%2067 = stablehlo.broadcast_in_dim %2066, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 281

Operation name: stablehlo.divide
cache miss
1531341356
%2068 = stablehlo.divide %2061, %605 : tensor<2xf32>
Cost: 376

Operation name: stablehlo.add
cache miss
1018830424
%2069 = stablehlo.add %2067, %2068 : tensor<2xf32>
Cost: 179

Operation name: stablehlo.multiply
cache miss
806204014
%2070 = stablehlo.multiply %2069, %599 : tensor<2xf32>
Cost: 379

Operation name: stablehlo.negate
cache miss
37341164
%2071 = stablehlo.negate %2070 : tensor<2xf32>
Cost: 281

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 452

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%2072 = stablehlo.reduce(%2071 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 376

Operation name: stablehlo.divide
cache miss
719102371
%2073 = stablehlo.divide %2072, %597 : tensor<f32>
Cost: 384

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%2074 = stablehlo.broadcast_in_dim %2073, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 380

Operation name: stablehlo.multiply
cache miss
806204014
%2075 = stablehlo.multiply %2074, %596 : tensor<2xf32>
Cost: 379

Operation name: stablehlo.add
cache miss
1018830424
%2076 = stablehlo.add %2070, %2075 : tensor<2xf32>
Cost: 352

Operation name: stablehlo.divide
cache miss
1531341356
%2077 = stablehlo.divide %2076, %cst_1 : tensor<2xf32>
Cost: 178

Operation name: stablehlo.dot_general
cache miss
2902940993
%2078 = stablehlo.dot_general %2077, %590, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<2x48xf32>) -> tensor<48xf32>
Cost: 269

Operation name: stablehlo.dot_general
cache miss
2229247683
%2079 = stablehlo.dot_general %2077, %587, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 284

Operation name: stablehlo.dot_general
cache miss
2229247683
%2080 = stablehlo.dot_general %606, %2039, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 380

Operation name: stablehlo.dot_general
cache miss
597698865
%2081 = stablehlo.dot_general %2038, %585, contracting_dims = [0] x [1], precision = [DEFAULT, DEFAULT] : (tensor<48xf32>, tensor<2x48xf32>) -> tensor<2xf32>
Cost: 381

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%2082 = stablehlo.broadcast_in_dim %582, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 380

Operation name: stablehlo.multiply
cache miss
806204014
%2083 = stablehlo.multiply %2081, %2082 : tensor<2xf32>
Cost: 276

Operation name: stablehlo.multiply
cache miss
806204014
%2084 = stablehlo.multiply %2083, %573 : tensor<2xf32>
Cost: 174

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 174

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%2085 = stablehlo.reduce(%2084 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 366

Operation name: stablehlo.negate
cache miss
391293031
%2086 = stablehlo.negate %2085 : tensor<f32>
Cost: 174

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%2087 = stablehlo.broadcast_in_dim %2086, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 278

Operation name: stablehlo.divide
cache miss
1531341356
%2088 = stablehlo.divide %2081, %579 : tensor<2xf32>
Cost: 279

Operation name: stablehlo.add
cache miss
1018830424
%2089 = stablehlo.add %2087, %2088 : tensor<2xf32>
Cost: 375

Operation name: stablehlo.multiply
cache miss
806204014
%2090 = stablehlo.multiply %2089, %573 : tensor<2xf32>
Cost: 347

Operation name: stablehlo.negate
cache miss
37341164
%2091 = stablehlo.negate %2090 : tensor<2xf32>
Cost: 364

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 289

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%2092 = stablehlo.reduce(%2091 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 171

Operation name: stablehlo.divide
cache miss
719102371
%2093 = stablehlo.divide %2092, %571 : tensor<f32>
Cost: 172

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%2094 = stablehlo.broadcast_in_dim %2093, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 170

Operation name: stablehlo.multiply
cache miss
806204014
%2095 = stablehlo.multiply %2094, %570 : tensor<2xf32>
Cost: 170

Operation name: stablehlo.add
cache miss
1018830424
%2096 = stablehlo.add %2090, %2095 : tensor<2xf32>
Cost: 270

Operation name: stablehlo.divide
cache miss
1531341356
%2097 = stablehlo.divide %2096, %cst_1 : tensor<2xf32>
Cost: 367

Operation name: stablehlo.dot_general
cache miss
2902940993
%2098 = stablehlo.dot_general %2097, %564, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<2x48xf32>) -> tensor<48xf32>
Cost: 367

Operation name: stablehlo.dot_general
cache miss
2229247683
%2099 = stablehlo.dot_general %2097, %561, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 377

Operation name: stablehlo.dot_general
cache miss
2229247683
%2100 = stablehlo.dot_general %580, %2038, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 282

Operation name: stablehlo.dot_general
cache miss
597698865
%2101 = stablehlo.dot_general %2037, %559, contracting_dims = [0] x [1], precision = [DEFAULT, DEFAULT] : (tensor<48xf32>, tensor<2x48xf32>) -> tensor<2xf32>
Cost: 180

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%2102 = stablehlo.broadcast_in_dim %556, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 281

Operation name: stablehlo.multiply
cache miss
806204014
%2103 = stablehlo.multiply %2101, %2102 : tensor<2xf32>
Cost: 274

Operation name: stablehlo.multiply
cache miss
806204014
%2104 = stablehlo.multiply %2103, %547 : tensor<2xf32>
Cost: 184

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 272

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%2105 = stablehlo.reduce(%2104 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 366

Operation name: stablehlo.negate
cache miss
391293031
%2106 = stablehlo.negate %2105 : tensor<f32>
Cost: 273

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%2107 = stablehlo.broadcast_in_dim %2106, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 367

Operation name: stablehlo.divide
cache miss
1531341356
%2108 = stablehlo.divide %2101, %553 : tensor<2xf32>
Cost: 269

Operation name: stablehlo.add
cache miss
1018830424
%2109 = stablehlo.add %2107, %2108 : tensor<2xf32>
Cost: 272

Operation name: stablehlo.multiply
cache miss
806204014
%2110 = stablehlo.multiply %2109, %547 : tensor<2xf32>
Cost: 367

Operation name: stablehlo.negate
cache miss
37341164
%2111 = stablehlo.negate %2110 : tensor<2xf32>
Cost: 362

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 272

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%2112 = stablehlo.reduce(%2111 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 281

Operation name: stablehlo.divide
cache miss
719102371
%2113 = stablehlo.divide %2112, %545 : tensor<f32>
Cost: 367

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%2114 = stablehlo.broadcast_in_dim %2113, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 292

Operation name: stablehlo.multiply
cache miss
806204014
%2115 = stablehlo.multiply %2114, %544 : tensor<2xf32>
Cost: 394

Operation name: stablehlo.add
cache miss
1018830424
%2116 = stablehlo.add %2110, %2115 : tensor<2xf32>
Cost: 271

Operation name: stablehlo.divide
cache miss
1531341356
%2117 = stablehlo.divide %2116, %cst_1 : tensor<2xf32>
Cost: 271

Operation name: stablehlo.dot_general
cache miss
2902940993
%2118 = stablehlo.dot_general %2117, %538, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<2x48xf32>) -> tensor<48xf32>
Cost: 263

Operation name: stablehlo.dot_general
cache miss
2229247683
%2119 = stablehlo.dot_general %2117, %535, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 372

Operation name: stablehlo.dot_general
cache miss
2229247683
%2120 = stablehlo.dot_general %554, %2037, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 375

Operation name: stablehlo.dot_general
cache miss
597698865
%2121 = stablehlo.dot_general %2036, %533, contracting_dims = [0] x [1], precision = [DEFAULT, DEFAULT] : (tensor<48xf32>, tensor<2x48xf32>) -> tensor<2xf32>
Cost: 273

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%2122 = stablehlo.broadcast_in_dim %530, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 407

Operation name: stablehlo.multiply
cache miss
806204014
%2123 = stablehlo.multiply %2121, %2122 : tensor<2xf32>
Cost: 173

Operation name: stablehlo.multiply
cache miss
806204014
%2124 = stablehlo.multiply %2123, %521 : tensor<2xf32>
Cost: 363

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 276

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%2125 = stablehlo.reduce(%2124 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 177

Operation name: stablehlo.negate
cache miss
391293031
%2126 = stablehlo.negate %2125 : tensor<f32>
Cost: 173

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%2127 = stablehlo.broadcast_in_dim %2126, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 173

Operation name: stablehlo.divide
cache miss
1531341356
%2128 = stablehlo.divide %2121, %527 : tensor<2xf32>
Cost: 363

Operation name: stablehlo.add
cache miss
1018830424
%2129 = stablehlo.add %2127, %2128 : tensor<2xf32>
Cost: 445

Operation name: stablehlo.multiply
cache miss
806204014
%2130 = stablehlo.multiply %2129, %521 : tensor<2xf32>
Cost: 362

Operation name: stablehlo.negate
cache miss
37341164
%2131 = stablehlo.negate %2130 : tensor<2xf32>
Cost: 368

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 363

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%2132 = stablehlo.reduce(%2131 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 363

Operation name: stablehlo.divide
cache miss
719102371
%2133 = stablehlo.divide %2132, %519 : tensor<f32>
Cost: 272

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%2134 = stablehlo.broadcast_in_dim %2133, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 170

Operation name: stablehlo.multiply
cache miss
806204014
%2135 = stablehlo.multiply %2134, %518 : tensor<2xf32>
Cost: 274

Operation name: stablehlo.add
cache miss
1018830424
%2136 = stablehlo.add %2130, %2135 : tensor<2xf32>
Cost: 388

Operation name: stablehlo.divide
cache miss
1531341356
%2137 = stablehlo.divide %2136, %cst_1 : tensor<2xf32>
Cost: 298

Operation name: stablehlo.dot_general
cache miss
2902940993
%2138 = stablehlo.dot_general %2137, %512, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<2x48xf32>) -> tensor<48xf32>
Cost: 354

Operation name: stablehlo.dot_general
cache miss
2229247683
%2139 = stablehlo.dot_general %2137, %509, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 191

Operation name: stablehlo.dot_general
cache miss
2229247683
%2140 = stablehlo.dot_general %528, %2036, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 280

Operation name: stablehlo.dot_general
cache miss
597698865
%2141 = stablehlo.dot_general %2035, %507, contracting_dims = [0] x [1], precision = [DEFAULT, DEFAULT] : (tensor<48xf32>, tensor<2x48xf32>) -> tensor<2xf32>
Cost: 272

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%2142 = stablehlo.broadcast_in_dim %504, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 429

Operation name: stablehlo.multiply
cache miss
806204014
%2143 = stablehlo.multiply %2141, %2142 : tensor<2xf32>
Cost: 367

Operation name: stablehlo.multiply
cache miss
806204014
%2144 = stablehlo.multiply %2143, %495 : tensor<2xf32>
Cost: 269

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 274

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%2145 = stablehlo.reduce(%2144 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 275

Operation name: stablehlo.negate
cache miss
391293031
%2146 = stablehlo.negate %2145 : tensor<f32>
Cost: 463

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%2147 = stablehlo.broadcast_in_dim %2146, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 364

Operation name: stablehlo.divide
cache miss
1531341356
%2148 = stablehlo.divide %2141, %501 : tensor<2xf32>
Cost: 271

Operation name: stablehlo.add
cache miss
1018830424
%2149 = stablehlo.add %2147, %2148 : tensor<2xf32>
Cost: 172

Operation name: stablehlo.multiply
cache miss
806204014
%2150 = stablehlo.multiply %2149, %495 : tensor<2xf32>
Cost: 277

Operation name: stablehlo.negate
cache miss
37341164
%2151 = stablehlo.negate %2150 : tensor<2xf32>
Cost: 275

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 276

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%2152 = stablehlo.reduce(%2151 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 279

Operation name: stablehlo.divide
cache miss
719102371
%2153 = stablehlo.divide %2152, %493 : tensor<f32>
Cost: 170

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%2154 = stablehlo.broadcast_in_dim %2153, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 273

Operation name: stablehlo.multiply
cache miss
806204014
%2155 = stablehlo.multiply %2154, %492 : tensor<2xf32>
Cost: 172

Operation name: stablehlo.add
cache miss
1018830424
%2156 = stablehlo.add %2150, %2155 : tensor<2xf32>
Cost: 366

Operation name: stablehlo.divide
cache miss
1531341356
%2157 = stablehlo.divide %2156, %cst_1 : tensor<2xf32>
Cost: 280

Operation name: stablehlo.dot_general
cache miss
2902940993
%2158 = stablehlo.dot_general %2157, %486, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<2x48xf32>) -> tensor<48xf32>
Cost: 171

Operation name: stablehlo.concatenate
cache miss
292037604
%2159 = stablehlo.concatenate %2158, %2138, %2118, %2098, %2078, %2058, dim = 0 : (tensor<48xf32>, tensor<48xf32>, tensor<48xf32>, tensor<48xf32>, tensor<48xf32>, tensor<48xf32>) -> tensor<288xf32>
Cost: 514

Operation name: stablehlo.reshape
cache miss
2392077026
%2160 = stablehlo.reshape %2159 : (tensor<288xf32>) -> tensor<144x2xf32>
Cost: 508

Operation name: stablehlo.dot_general
cache miss
1714835160
%2161 = stablehlo.dot_general %2160, %cst_8, batching_dims = [0] x [0], contracting_dims = [1] x [1], precision = [DEFAULT, DEFAULT] : (tensor<144x2xf32>, tensor<144x2x2xf32>) -> tensor<144x2xf32>
Cost: 396

Operation name: stablehlo.reshape
cache miss
1377738304
%2162 = stablehlo.reshape %2161 : (tensor<144x2xf32>) -> tensor<288xf32>
Cost: 492

Operation name: stablehlo.dot_general
cache miss
1491685315
%2163 = stablehlo.dot_general %2162, %464, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288x288xf32>) -> tensor<288xf32>
Cost: 454

Operation name: stablehlo.dot_general
cache miss
3876455518
%2164 = stablehlo.dot_general %2162, %462, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288xf32>) -> tensor<288x288xf32>
Cost: 22744

Operation name: stablehlo.reshape
cache miss
552616300
%2165 = stablehlo.reshape %2164 : (tensor<288x288xf32>) -> tensor<1x288x288xf32>
Cost: 31042

Operation name: stablehlo.dot_general
cache miss
2229247683
%2166 = stablehlo.dot_general %2157, %483, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 278

Operation name: stablehlo.slice
cache miss
3729170049
%2167 = stablehlo.slice %2166 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 370

Operation name: stablehlo.slice
cache miss
3729170049
%2168 = stablehlo.slice %2139 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 277

Operation name: stablehlo.slice
cache miss
3729170049
%2169 = stablehlo.slice %2119 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 280

Operation name: stablehlo.slice
cache miss
3729170049
%2170 = stablehlo.slice %2099 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 369

Operation name: stablehlo.slice
cache miss
3729170049
%2171 = stablehlo.slice %2079 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 374

Operation name: stablehlo.slice
cache miss
3729170049
%2172 = stablehlo.slice %2059 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 277

Operation name: stablehlo.concatenate
cache miss
3995287944
%2173 = stablehlo.concatenate %2167, %2168, %2169, %2170, %2171, %2172, dim = 1 : (tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<1x288xf32>
Cost: 422

Operation name: stablehlo.slice
cache miss
342624074
%2174 = stablehlo.slice %2166 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 279

Operation name: stablehlo.slice
cache miss
342624074
%2175 = stablehlo.slice %2139 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 278

Operation name: stablehlo.slice
cache miss
342624074
%2176 = stablehlo.slice %2119 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 172

Operation name: stablehlo.slice
cache miss
342624074
%2177 = stablehlo.slice %2099 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 370

Operation name: stablehlo.slice
cache miss
342624074
%2178 = stablehlo.slice %2079 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 278

Operation name: stablehlo.slice
cache miss
342624074
%2179 = stablehlo.slice %2059 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 173

Operation name: stablehlo.concatenate
cache miss
3995287944
%2180 = stablehlo.concatenate %2174, %2175, %2176, %2177, %2178, %2179, dim = 1 : (tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<1x288xf32>
Cost: 481

Operation name: stablehlo.reshape
cache miss
2392077026
%2181 = stablehlo.reshape %2180 : (tensor<1x288xf32>) -> tensor<144x2xf32>
Cost: 301

Operation name: stablehlo.dot_general
cache miss
1714835160
%2182 = stablehlo.dot_general %2181, %cst_8, batching_dims = [0] x [0], contracting_dims = [1] x [1], precision = [DEFAULT, DEFAULT] : (tensor<144x2xf32>, tensor<144x2x2xf32>) -> tensor<144x2xf32>
Cost: 645

Operation name: stablehlo.reshape
cache miss
1377738304
%2183 = stablehlo.reshape %2182 : (tensor<144x2xf32>) -> tensor<288xf32>
Cost: 396

Operation name: stablehlo.dot_general
cache miss
1491685315
%2184 = stablehlo.dot_general %2183, %467, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288x288xf32>) -> tensor<288xf32>
Cost: 341

Operation name: stablehlo.add
cache miss
3847999983
%2185 = stablehlo.add %2163, %2184 : tensor<288xf32>
Cost: 400

Operation name: stablehlo.dot_general
cache miss
3876455518
%2186 = stablehlo.dot_general %2183, %462, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288xf32>) -> tensor<288x288xf32>
Cost: 36137

Operation name: stablehlo.reshape
cache miss
552616300
%2187 = stablehlo.reshape %2186 : (tensor<288x288xf32>) -> tensor<1x288x288xf32>
Cost: 35833

Operation name: stablehlo.reshape
cache miss
1853321842
%2188 = stablehlo.reshape %2173 : (tensor<1x288xf32>) -> tensor<1x1x288xf32>
Cost: 683

Operation name: stablehlo.dot_general
cache miss
2229247683
%2189 = stablehlo.dot_general %502, %2035, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 448

Operation name: stablehlo.slice
cache miss
3729170049
%2190 = stablehlo.slice %2189 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 367

Operation name: stablehlo.slice
cache miss
3729170049
%2191 = stablehlo.slice %2140 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 477

Operation name: stablehlo.slice
cache miss
3729170049
%2192 = stablehlo.slice %2120 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 278

Operation name: stablehlo.slice
cache miss
3729170049
%2193 = stablehlo.slice %2100 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 370

Operation name: stablehlo.slice
cache miss
3729170049
%2194 = stablehlo.slice %2080 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 277

Operation name: stablehlo.slice
cache miss
3729170049
%2195 = stablehlo.slice %2060 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 400

Operation name: stablehlo.concatenate
cache miss
3995287944
%2196 = stablehlo.concatenate %2190, %2191, %2192, %2193, %2194, %2195, dim = 1 : (tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<1x288xf32>
Cost: 412

Operation name: stablehlo.slice
cache miss
342624074
%2197 = stablehlo.slice %2189 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 389

Operation name: stablehlo.slice
cache miss
342624074
%2198 = stablehlo.slice %2140 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 278

Operation name: stablehlo.slice
cache miss
342624074
%2199 = stablehlo.slice %2120 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 175

Operation name: stablehlo.slice
cache miss
342624074
%2200 = stablehlo.slice %2100 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 173

Operation name: stablehlo.slice
cache miss
342624074
%2201 = stablehlo.slice %2080 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 277

Operation name: stablehlo.slice
cache miss
342624074
%2202 = stablehlo.slice %2060 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 470

Operation name: stablehlo.concatenate
cache miss
3995287944
%2203 = stablehlo.concatenate %2197, %2198, %2199, %2200, %2201, %2202, dim = 1 : (tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<1x288xf32>
Cost: 286

Operation name: stablehlo.reshape
cache miss
1377738304
%2204 = stablehlo.reshape %2203 : (tensor<1x288xf32>) -> tensor<288xf32>
Cost: 423

Operation name: stablehlo.dot_general
cache miss
1491685315
%2205 = stablehlo.dot_general %2204, %470, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288x288xf32>) -> tensor<288xf32>
Cost: 347

Operation name: stablehlo.add
cache miss
3847999983
%2206 = stablehlo.add %2185, %2205 : tensor<288xf32>
Cost: 291

Operation name: stablehlo.multiply
cache miss
2511610444
%2207 = stablehlo.multiply %460, %2206 : tensor<288xf32>
Cost: 508

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 192

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%2208 = stablehlo.reduce(%2207 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<288xf32>, tensor<f32>) -> tensor<f32>
Cost: 190

Operation name: stablehlo.multiply
cache miss
1284973678
%2209 = stablehlo.multiply %2208, %459 : tensor<f32>
Cost: 198

Operation name: stablehlo.negate
cache miss
391293031
%2210 = stablehlo.negate %2209 : tensor<f32>
Cost: 397

Operation name: stablehlo.multiply
cache miss
1284973678
%2211 = stablehlo.multiply %2210, %456 : tensor<f32>
Cost: 299

Operation name: stablehlo.divide
cache miss
719102371
%2212 = stablehlo.divide %2211, %cst_5 : tensor<f32>
Cost: 468

Operation name: stablehlo.dot_general
cache miss
1413888672
%2213 = stablehlo.dot_general %2212, %449, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<f32>, tensor<288xf32>) -> tensor<288xf32>
Cost: 550

Operation name: stablehlo.add
cache miss
3847999983
%2214 = stablehlo.add %2033, %2213 : tensor<288xf32>
Cost: 531

Operation name: stablehlo.add
cache miss
3847999983
%2215 = stablehlo.add %2214, %2213 : tensor<288xf32>
Cost: 531

Operation name: stablehlo.multiply
cache miss
2511610444
%2216 = stablehlo.multiply %2206, %461 : tensor<288xf32>
Cost: 440

Operation name: stablehlo.multiply
cache miss
2511610444
%2217 = stablehlo.multiply %451, %2216 : tensor<288xf32>
Cost: 431

Operation name: stablehlo.add
cache miss
3847999983
%2218 = stablehlo.add %2215, %2217 : tensor<288xf32>
Cost: 534

Operation name: stablehlo.dot_general
cache miss
2840382466
%2219 = stablehlo.dot_general %2218, %447, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288x768xf32>) -> tensor<768xf32>
Cost: 5997

Operation name: stablehlo.multiply
cache miss
4137946040
%2220 = stablehlo.multiply %444, %2219 : tensor<768xf32>
Cost: 551

Operation name: stablehlo.dot_general
cache miss
1491685315
%2221 = stablehlo.dot_general %2220, %436, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<768xf32>, tensor<768x288xf32>) -> tensor<288xf32>
Cost: 3732

Operation name: stablehlo.dot_general
cache miss
1743356395
%2222 = stablehlo.dot_general %2220, %431, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<768xf32>, tensor<288xf32>) -> tensor<768x288xf32>
Cost: 93788

Operation name: stablehlo.reshape
cache miss
1628429499
%2223 = stablehlo.reshape %2222 : (tensor<768x288xf32>) -> tensor<1x768x288xf32>
Cost: 73684

Operation name: stablehlo.multiply
cache miss
4137946040
%2224 = stablehlo.multiply %2219, %437 : tensor<768xf32>
Cost: 582

Operation name: stablehlo.multiply
cache miss
4137946040
%2225 = stablehlo.multiply %434, %2224 : tensor<768xf32>
Cost: 686

Operation name: stablehlo.multiply
cache miss
4137946040
%2226 = stablehlo.multiply %2225, %443 : tensor<768xf32>
Cost: 327

Operation name: stablehlo.negate
cache miss
4256221255
%2227 = stablehlo.negate %2226 : tensor<768xf32>
Cost: 290

Operation name: stablehlo.multiply
cache miss
4137946040
%2228 = stablehlo.multiply %2227, %439 : tensor<768xf32>
Cost: 300

Operation name: stablehlo.negate
cache miss
4256221255
%2229 = stablehlo.negate %2228 : tensor<768xf32>
Cost: 453

Operation name: stablehlo.multiply
cache miss
4137946040
%2230 = stablehlo.multiply %2224, %441 : tensor<768xf32>
Cost: 487

Operation name: stablehlo.add
cache miss
389450582
%2231 = stablehlo.add %2229, %2230 : tensor<768xf32>
Cost: 482

Operation name: stablehlo.dot_general
cache miss
1491685315
%2232 = stablehlo.dot_general %2231, %433, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<768xf32>, tensor<768x288xf32>) -> tensor<288xf32>
Cost: 5959

Operation name: stablehlo.add
cache miss
3847999983
%2233 = stablehlo.add %2221, %2232 : tensor<288xf32>
Cost: 268

Operation name: stablehlo.multiply
cache miss
2511610444
%2234 = stablehlo.multiply %429, %2233 : tensor<288xf32>
Cost: 436

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 397

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%2235 = stablehlo.reduce(%2234 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<288xf32>, tensor<f32>) -> tensor<f32>
Cost: 401

Operation name: stablehlo.multiply
cache miss
1284973678
%2236 = stablehlo.multiply %2235, %428 : tensor<f32>
Cost: 298

Operation name: stablehlo.negate
cache miss
391293031
%2237 = stablehlo.negate %2236 : tensor<f32>
Cost: 399

Operation name: stablehlo.multiply
cache miss
1284973678
%2238 = stablehlo.multiply %2237, %425 : tensor<f32>
Cost: 405

Operation name: stablehlo.divide
cache miss
719102371
%2239 = stablehlo.divide %2238, %cst_5 : tensor<f32>
Cost: 291

Operation name: stablehlo.dot_general
cache miss
1413888672
%2240 = stablehlo.dot_general %2239, %418, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<f32>, tensor<288xf32>) -> tensor<288xf32>
Cost: 407

Operation name: stablehlo.add
cache miss
3847999983
%2241 = stablehlo.add %2218, %2240 : tensor<288xf32>
Cost: 677

Operation name: stablehlo.add
cache miss
3847999983
%2242 = stablehlo.add %2241, %2240 : tensor<288xf32>
Cost: 446

Operation name: stablehlo.multiply
cache miss
2511610444
%2243 = stablehlo.multiply %2233, %430 : tensor<288xf32>
Cost: 542

Operation name: stablehlo.multiply
cache miss
2511610444
%2244 = stablehlo.multiply %420, %2243 : tensor<288xf32>
Cost: 514

Operation name: stablehlo.add
cache miss
3847999983
%2245 = stablehlo.add %2242, %2244 : tensor<288xf32>
Cost: 422

Operation name: stablehlo.dot_general
cache miss
1491685315
%2246 = stablehlo.dot_general %2245, %416, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288x288xf32>) -> tensor<288xf32>
Cost: 464

Operation name: stablehlo.slice
cache miss
2839717066
%2247 = stablehlo.slice %2246 [0:48] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 422

Operation name: stablehlo.slice
cache miss
4032389970
%2248 = stablehlo.slice %2246 [48:96] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 275

Operation name: stablehlo.slice
cache miss
1375222232
%2249 = stablehlo.slice %2246 [96:144] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 275

Operation name: stablehlo.slice
cache miss
3469783879
%2250 = stablehlo.slice %2246 [144:192] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 446

Operation name: stablehlo.slice
cache miss
1801550340
%2251 = stablehlo.slice %2246 [192:240] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 459

Operation name: stablehlo.slice
cache miss
2798019243
%2252 = stablehlo.slice %2246 [240:288] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 273

Operation name: stablehlo.dot_general
cache miss
597698865
%2253 = stablehlo.dot_general %2252, %412, contracting_dims = [0] x [1], precision = [DEFAULT, DEFAULT] : (tensor<48xf32>, tensor<2x48xf32>) -> tensor<2xf32>
Cost: 452

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%2254 = stablehlo.broadcast_in_dim %409, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 296

Operation name: stablehlo.multiply
cache miss
806204014
%2255 = stablehlo.multiply %2253, %2254 : tensor<2xf32>
Cost: 310

Operation name: stablehlo.multiply
cache miss
806204014
%2256 = stablehlo.multiply %2255, %400 : tensor<2xf32>
Cost: 293

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 305

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%2257 = stablehlo.reduce(%2256 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 298

Operation name: stablehlo.negate
cache miss
391293031
%2258 = stablehlo.negate %2257 : tensor<f32>
Cost: 549

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%2259 = stablehlo.broadcast_in_dim %2258, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 394

Operation name: stablehlo.divide
cache miss
1531341356
%2260 = stablehlo.divide %2253, %406 : tensor<2xf32>
Cost: 327

Operation name: stablehlo.add
cache miss
1018830424
%2261 = stablehlo.add %2259, %2260 : tensor<2xf32>
Cost: 321

Operation name: stablehlo.multiply
cache miss
806204014
%2262 = stablehlo.multiply %2261, %400 : tensor<2xf32>
Cost: 288

Operation name: stablehlo.negate
cache miss
37341164
%2263 = stablehlo.negate %2262 : tensor<2xf32>
Cost: 358

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 292

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%2264 = stablehlo.reduce(%2263 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 400

Operation name: stablehlo.divide
cache miss
719102371
%2265 = stablehlo.divide %2264, %398 : tensor<f32>
Cost: 296

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%2266 = stablehlo.broadcast_in_dim %2265, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 388

Operation name: stablehlo.multiply
cache miss
806204014
%2267 = stablehlo.multiply %2266, %397 : tensor<2xf32>
Cost: 396

Operation name: stablehlo.add
cache miss
1018830424
%2268 = stablehlo.add %2262, %2267 : tensor<2xf32>
Cost: 319

Operation name: stablehlo.divide
cache miss
1531341356
%2269 = stablehlo.divide %2268, %cst_1 : tensor<2xf32>
Cost: 422

Operation name: stablehlo.dot_general
cache miss
2902940993
%2270 = stablehlo.dot_general %2269, %391, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<2x48xf32>) -> tensor<48xf32>
Cost: 326

Operation name: stablehlo.dot_general
cache miss
2229247683
%2271 = stablehlo.dot_general %2269, %388, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 366

Operation name: stablehlo.dot_general
cache miss
2229247683
%2272 = stablehlo.dot_general %407, %2252, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 341

Operation name: stablehlo.dot_general
cache miss
597698865
%2273 = stablehlo.dot_general %2251, %386, contracting_dims = [0] x [1], precision = [DEFAULT, DEFAULT] : (tensor<48xf32>, tensor<2x48xf32>) -> tensor<2xf32>
Cost: 293

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%2274 = stablehlo.broadcast_in_dim %383, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 301

Operation name: stablehlo.multiply
cache miss
806204014
%2275 = stablehlo.multiply %2273, %2274 : tensor<2xf32>
Cost: 292

Operation name: stablehlo.multiply
cache miss
806204014
%2276 = stablehlo.multiply %2275, %374 : tensor<2xf32>
Cost: 301

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 395

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%2277 = stablehlo.reduce(%2276 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 305

Operation name: stablehlo.negate
cache miss
391293031
%2278 = stablehlo.negate %2277 : tensor<f32>
Cost: 411

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%2279 = stablehlo.broadcast_in_dim %2278, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 386

Operation name: stablehlo.divide
cache miss
1531341356
%2280 = stablehlo.divide %2273, %380 : tensor<2xf32>
Cost: 383

Operation name: stablehlo.add
cache miss
1018830424
%2281 = stablehlo.add %2279, %2280 : tensor<2xf32>
Cost: 375

Operation name: stablehlo.multiply
cache miss
806204014
%2282 = stablehlo.multiply %2281, %374 : tensor<2xf32>
Cost: 283

Operation name: stablehlo.negate
cache miss
37341164
%2283 = stablehlo.negate %2282 : tensor<2xf32>
Cost: 374

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 378

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%2284 = stablehlo.reduce(%2283 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 182

Operation name: stablehlo.divide
cache miss
719102371
%2285 = stablehlo.divide %2284, %372 : tensor<f32>
Cost: 312

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%2286 = stablehlo.broadcast_in_dim %2285, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 311

Operation name: stablehlo.multiply
cache miss
806204014
%2287 = stablehlo.multiply %2286, %371 : tensor<2xf32>
Cost: 283

Operation name: stablehlo.add
cache miss
1018830424
%2288 = stablehlo.add %2282, %2287 : tensor<2xf32>
Cost: 402

Operation name: stablehlo.divide
cache miss
1531341356
%2289 = stablehlo.divide %2288, %cst_1 : tensor<2xf32>
Cost: 310

Operation name: stablehlo.dot_general
cache miss
2902940993
%2290 = stablehlo.dot_general %2289, %365, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<2x48xf32>) -> tensor<48xf32>
Cost: 264

Operation name: stablehlo.dot_general
cache miss
2229247683
%2291 = stablehlo.dot_general %2289, %362, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 281

Operation name: stablehlo.dot_general
cache miss
2229247683
%2292 = stablehlo.dot_general %381, %2251, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 376

Operation name: stablehlo.dot_general
cache miss
597698865
%2293 = stablehlo.dot_general %2250, %360, contracting_dims = [0] x [1], precision = [DEFAULT, DEFAULT] : (tensor<48xf32>, tensor<2x48xf32>) -> tensor<2xf32>
Cost: 481

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%2294 = stablehlo.broadcast_in_dim %357, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 285

Operation name: stablehlo.multiply
cache miss
806204014
%2295 = stablehlo.multiply %2293, %2294 : tensor<2xf32>
Cost: 282

Operation name: stablehlo.multiply
cache miss
806204014
%2296 = stablehlo.multiply %2295, %348 : tensor<2xf32>
Cost: 178

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 173

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%2297 = stablehlo.reduce(%2296 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 364

Operation name: stablehlo.negate
cache miss
391293031
%2298 = stablehlo.negate %2297 : tensor<f32>
Cost: 329

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%2299 = stablehlo.broadcast_in_dim %2298, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 274

Operation name: stablehlo.divide
cache miss
1531341356
%2300 = stablehlo.divide %2293, %354 : tensor<2xf32>
Cost: 278

Operation name: stablehlo.add
cache miss
1018830424
%2301 = stablehlo.add %2299, %2300 : tensor<2xf32>
Cost: 360

Operation name: stablehlo.multiply
cache miss
806204014
%2302 = stablehlo.multiply %2301, %348 : tensor<2xf32>
Cost: 271

Operation name: stablehlo.negate
cache miss
37341164
%2303 = stablehlo.negate %2302 : tensor<2xf32>
Cost: 171

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 395

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%2304 = stablehlo.reduce(%2303 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 362

Operation name: stablehlo.divide
cache miss
719102371
%2305 = stablehlo.divide %2304, %346 : tensor<f32>
Cost: 276

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%2306 = stablehlo.broadcast_in_dim %2305, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 272

Operation name: stablehlo.multiply
cache miss
806204014
%2307 = stablehlo.multiply %2306, %345 : tensor<2xf32>
Cost: 366

Operation name: stablehlo.add
cache miss
1018830424
%2308 = stablehlo.add %2302, %2307 : tensor<2xf32>
Cost: 438

Operation name: stablehlo.divide
cache miss
1531341356
%2309 = stablehlo.divide %2308, %cst_1 : tensor<2xf32>
Cost: 362

Operation name: stablehlo.dot_general
cache miss
2902940993
%2310 = stablehlo.dot_general %2309, %339, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<2x48xf32>) -> tensor<48xf32>
Cost: 358

Operation name: stablehlo.dot_general
cache miss
2229247683
%2311 = stablehlo.dot_general %2309, %336, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 343

Operation name: stablehlo.dot_general
cache miss
2229247683
%2312 = stablehlo.dot_general %355, %2250, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 370

Operation name: stablehlo.dot_general
cache miss
597698865
%2313 = stablehlo.dot_general %2249, %334, contracting_dims = [0] x [1], precision = [DEFAULT, DEFAULT] : (tensor<48xf32>, tensor<2x48xf32>) -> tensor<2xf32>
Cost: 367

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%2314 = stablehlo.broadcast_in_dim %331, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 271

Operation name: stablehlo.multiply
cache miss
806204014
%2315 = stablehlo.multiply %2313, %2314 : tensor<2xf32>
Cost: 170

Operation name: stablehlo.multiply
cache miss
806204014
%2316 = stablehlo.multiply %2315, %322 : tensor<2xf32>
Cost: 383

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 173

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%2317 = stablehlo.reduce(%2316 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 219

Operation name: stablehlo.negate
cache miss
391293031
%2318 = stablehlo.negate %2317 : tensor<f32>
Cost: 274

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%2319 = stablehlo.broadcast_in_dim %2318, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 364

Operation name: stablehlo.divide
cache miss
1531341356
%2320 = stablehlo.divide %2313, %328 : tensor<2xf32>
Cost: 179

Operation name: stablehlo.add
cache miss
1018830424
%2321 = stablehlo.add %2319, %2320 : tensor<2xf32>
Cost: 376

Operation name: stablehlo.multiply
cache miss
806204014
%2322 = stablehlo.multiply %2321, %322 : tensor<2xf32>
Cost: 360

Operation name: stablehlo.negate
cache miss
37341164
%2323 = stablehlo.negate %2322 : tensor<2xf32>
Cost: 393

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 368

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%2324 = stablehlo.reduce(%2323 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 171

Operation name: stablehlo.divide
cache miss
719102371
%2325 = stablehlo.divide %2324, %320 : tensor<f32>
Cost: 281

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%2326 = stablehlo.broadcast_in_dim %2325, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 365

Operation name: stablehlo.multiply
cache miss
806204014
%2327 = stablehlo.multiply %2326, %319 : tensor<2xf32>
Cost: 270

Operation name: stablehlo.add
cache miss
1018830424
%2328 = stablehlo.add %2322, %2327 : tensor<2xf32>
Cost: 439

Operation name: stablehlo.divide
cache miss
1531341356
%2329 = stablehlo.divide %2328, %cst_1 : tensor<2xf32>
Cost: 358

Operation name: stablehlo.dot_general
cache miss
2902940993
%2330 = stablehlo.dot_general %2329, %313, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<2x48xf32>) -> tensor<48xf32>
Cost: 310

Operation name: stablehlo.dot_general
cache miss
2229247683
%2331 = stablehlo.dot_general %2329, %310, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 174

Operation name: stablehlo.dot_general
cache miss
2229247683
%2332 = stablehlo.dot_general %329, %2249, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 179

Operation name: stablehlo.dot_general
cache miss
597698865
%2333 = stablehlo.dot_general %2248, %308, contracting_dims = [0] x [1], precision = [DEFAULT, DEFAULT] : (tensor<48xf32>, tensor<2x48xf32>) -> tensor<2xf32>
Cost: 369

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%2334 = stablehlo.broadcast_in_dim %305, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 364

Operation name: stablehlo.multiply
cache miss
806204014
%2335 = stablehlo.multiply %2333, %2334 : tensor<2xf32>
Cost: 385

Operation name: stablehlo.multiply
cache miss
806204014
%2336 = stablehlo.multiply %2335, %296 : tensor<2xf32>
Cost: 270

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 279

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%2337 = stablehlo.reduce(%2336 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 364

Operation name: stablehlo.negate
cache miss
391293031
%2338 = stablehlo.negate %2337 : tensor<f32>
Cost: 174

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%2339 = stablehlo.broadcast_in_dim %2338, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 279

Operation name: stablehlo.divide
cache miss
1531341356
%2340 = stablehlo.divide %2333, %302 : tensor<2xf32>
Cost: 170

Operation name: stablehlo.add
cache miss
1018830424
%2341 = stablehlo.add %2339, %2340 : tensor<2xf32>
Cost: 366

Operation name: stablehlo.multiply
cache miss
806204014
%2342 = stablehlo.multiply %2341, %296 : tensor<2xf32>
Cost: 280

Operation name: stablehlo.negate
cache miss
37341164
%2343 = stablehlo.negate %2342 : tensor<2xf32>
Cost: 362

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 275

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%2344 = stablehlo.reduce(%2343 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 170

Operation name: stablehlo.divide
cache miss
719102371
%2345 = stablehlo.divide %2344, %294 : tensor<f32>
Cost: 277

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%2346 = stablehlo.broadcast_in_dim %2345, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 280

Operation name: stablehlo.multiply
cache miss
806204014
%2347 = stablehlo.multiply %2346, %293 : tensor<2xf32>
Cost: 274

Operation name: stablehlo.add
cache miss
1018830424
%2348 = stablehlo.add %2342, %2347 : tensor<2xf32>
Cost: 186

Operation name: stablehlo.divide
cache miss
1531341356
%2349 = stablehlo.divide %2348, %cst_1 : tensor<2xf32>
Cost: 272

Operation name: stablehlo.dot_general
cache miss
2902940993
%2350 = stablehlo.dot_general %2349, %287, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<2x48xf32>) -> tensor<48xf32>
Cost: 335

Operation name: stablehlo.dot_general
cache miss
2229247683
%2351 = stablehlo.dot_general %2349, %284, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 420

Operation name: stablehlo.dot_general
cache miss
2229247683
%2352 = stablehlo.dot_general %303, %2248, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 269

Operation name: stablehlo.dot_general
cache miss
597698865
%2353 = stablehlo.dot_general %2247, %282, contracting_dims = [0] x [1], precision = [DEFAULT, DEFAULT] : (tensor<48xf32>, tensor<2x48xf32>) -> tensor<2xf32>
Cost: 369

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%2354 = stablehlo.broadcast_in_dim %279, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 273

Operation name: stablehlo.multiply
cache miss
806204014
%2355 = stablehlo.multiply %2353, %2354 : tensor<2xf32>
Cost: 277

Operation name: stablehlo.multiply
cache miss
806204014
%2356 = stablehlo.multiply %2355, %270 : tensor<2xf32>
Cost: 278

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 358

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%2357 = stablehlo.reduce(%2356 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 367

Operation name: stablehlo.negate
cache miss
391293031
%2358 = stablehlo.negate %2357 : tensor<f32>
Cost: 413

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%2359 = stablehlo.broadcast_in_dim %2358, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 172

Operation name: stablehlo.divide
cache miss
1531341356
%2360 = stablehlo.divide %2353, %276 : tensor<2xf32>
Cost: 276

Operation name: stablehlo.add
cache miss
1018830424
%2361 = stablehlo.add %2359, %2360 : tensor<2xf32>
Cost: 368

Operation name: stablehlo.multiply
cache miss
806204014
%2362 = stablehlo.multiply %2361, %270 : tensor<2xf32>
Cost: 369

Operation name: stablehlo.negate
cache miss
37341164
%2363 = stablehlo.negate %2362 : tensor<2xf32>
Cost: 175

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 386

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%2364 = stablehlo.reduce(%2363 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 372

Operation name: stablehlo.divide
cache miss
719102371
%2365 = stablehlo.divide %2364, %268 : tensor<f32>
Cost: 171

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%2366 = stablehlo.broadcast_in_dim %2365, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 274

Operation name: stablehlo.multiply
cache miss
806204014
%2367 = stablehlo.multiply %2366, %267 : tensor<2xf32>
Cost: 379

Operation name: stablehlo.add
cache miss
1018830424
%2368 = stablehlo.add %2362, %2367 : tensor<2xf32>
Cost: 363

Operation name: stablehlo.divide
cache miss
1531341356
%2369 = stablehlo.divide %2368, %cst_1 : tensor<2xf32>
Cost: 271

Operation name: stablehlo.dot_general
cache miss
2902940993
%2370 = stablehlo.dot_general %2369, %261, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<2x48xf32>) -> tensor<48xf32>
Cost: 167

Operation name: stablehlo.concatenate
cache miss
292037604
%2371 = stablehlo.concatenate %2370, %2350, %2330, %2310, %2290, %2270, dim = 0 : (tensor<48xf32>, tensor<48xf32>, tensor<48xf32>, tensor<48xf32>, tensor<48xf32>, tensor<48xf32>) -> tensor<288xf32>
Cost: 271

Operation name: stablehlo.reshape
cache miss
2392077026
%2372 = stablehlo.reshape %2371 : (tensor<288xf32>) -> tensor<144x2xf32>
Cost: 531

Operation name: stablehlo.dot_general
cache miss
1714835160
%2373 = stablehlo.dot_general %2372, %cst_8, batching_dims = [0] x [0], contracting_dims = [1] x [1], precision = [DEFAULT, DEFAULT] : (tensor<144x2xf32>, tensor<144x2x2xf32>) -> tensor<144x2xf32>
Cost: 593

Operation name: stablehlo.reshape
cache miss
1377738304
%2374 = stablehlo.reshape %2373 : (tensor<144x2xf32>) -> tensor<288xf32>
Cost: 518

Operation name: stablehlo.dot_general
cache miss
1491685315
%2375 = stablehlo.dot_general %2374, %239, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288x288xf32>) -> tensor<288xf32>
Cost: 462

Operation name: stablehlo.dot_general
cache miss
3876455518
%2376 = stablehlo.dot_general %2374, %237, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288xf32>) -> tensor<288x288xf32>
Cost: 30959

Operation name: stablehlo.reshape
cache miss
552616300
%2377 = stablehlo.reshape %2376 : (tensor<288x288xf32>) -> tensor<1x288x288xf32>
Cost: 27315

Operation name: stablehlo.dot_general
cache miss
2229247683
%2378 = stablehlo.dot_general %2369, %258, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 290

Operation name: stablehlo.slice
cache miss
3729170049
%2379 = stablehlo.slice %2378 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 373

Operation name: stablehlo.slice
cache miss
3729170049
%2380 = stablehlo.slice %2351 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 532

Operation name: stablehlo.slice
cache miss
3729170049
%2381 = stablehlo.slice %2331 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 278

Operation name: stablehlo.slice
cache miss
3729170049
%2382 = stablehlo.slice %2311 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 449

Operation name: stablehlo.slice
cache miss
3729170049
%2383 = stablehlo.slice %2291 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 381

Operation name: stablehlo.slice
cache miss
3729170049
%2384 = stablehlo.slice %2271 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 465

Operation name: stablehlo.concatenate
cache miss
3995287944
%2385 = stablehlo.concatenate %2379, %2380, %2381, %2382, %2383, %2384, dim = 1 : (tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<1x288xf32>
Cost: 411

Operation name: stablehlo.slice
cache miss
342624074
%2386 = stablehlo.slice %2378 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 276

Operation name: stablehlo.slice
cache miss
342624074
%2387 = stablehlo.slice %2351 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 367

Operation name: stablehlo.slice
cache miss
342624074
%2388 = stablehlo.slice %2331 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 278

Operation name: stablehlo.slice
cache miss
342624074
%2389 = stablehlo.slice %2311 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 383

Operation name: stablehlo.slice
cache miss
342624074
%2390 = stablehlo.slice %2291 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 279

Operation name: stablehlo.slice
cache miss
342624074
%2391 = stablehlo.slice %2271 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 278

Operation name: stablehlo.concatenate
cache miss
3995287944
%2392 = stablehlo.concatenate %2386, %2387, %2388, %2389, %2390, %2391, dim = 1 : (tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<1x288xf32>
Cost: 555

Operation name: stablehlo.reshape
cache miss
2392077026
%2393 = stablehlo.reshape %2392 : (tensor<1x288xf32>) -> tensor<144x2xf32>
Cost: 284

Operation name: stablehlo.dot_general
cache miss
1714835160
%2394 = stablehlo.dot_general %2393, %cst_8, batching_dims = [0] x [0], contracting_dims = [1] x [1], precision = [DEFAULT, DEFAULT] : (tensor<144x2xf32>, tensor<144x2x2xf32>) -> tensor<144x2xf32>
Cost: 386

Operation name: stablehlo.reshape
cache miss
1377738304
%2395 = stablehlo.reshape %2394 : (tensor<144x2xf32>) -> tensor<288xf32>
Cost: 408

Operation name: stablehlo.dot_general
cache miss
1491685315
%2396 = stablehlo.dot_general %2395, %242, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288x288xf32>) -> tensor<288xf32>
Cost: 235

Operation name: stablehlo.add
cache miss
3847999983
%2397 = stablehlo.add %2375, %2396 : tensor<288xf32>
Cost: 544

Operation name: stablehlo.dot_general
cache miss
3876455518
%2398 = stablehlo.dot_general %2395, %237, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288xf32>) -> tensor<288x288xf32>
Cost: 16728

Operation name: stablehlo.reshape
cache miss
552616300
%2399 = stablehlo.reshape %2398 : (tensor<288x288xf32>) -> tensor<1x288x288xf32>
Cost: 44147

Operation name: stablehlo.reshape
cache miss
1853321842
%2400 = stablehlo.reshape %2385 : (tensor<1x288xf32>) -> tensor<1x1x288xf32>
Cost: 417

Operation name: stablehlo.dot_general
cache miss
2229247683
%2401 = stablehlo.dot_general %277, %2247, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 374

Operation name: stablehlo.slice
cache miss
3729170049
%2402 = stablehlo.slice %2401 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 408

Operation name: stablehlo.slice
cache miss
3729170049
%2403 = stablehlo.slice %2352 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 364

Operation name: stablehlo.slice
cache miss
3729170049
%2404 = stablehlo.slice %2332 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 276

Operation name: stablehlo.slice
cache miss
3729170049
%2405 = stablehlo.slice %2312 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 273

Operation name: stablehlo.slice
cache miss
3729170049
%2406 = stablehlo.slice %2292 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 276

Operation name: stablehlo.slice
cache miss
3729170049
%2407 = stablehlo.slice %2272 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 282

Operation name: stablehlo.concatenate
cache miss
3995287944
%2408 = stablehlo.concatenate %2402, %2403, %2404, %2405, %2406, %2407, dim = 1 : (tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<1x288xf32>
Cost: 405

Operation name: stablehlo.slice
cache miss
342624074
%2409 = stablehlo.slice %2401 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 365

Operation name: stablehlo.slice
cache miss
342624074
%2410 = stablehlo.slice %2352 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 276

Operation name: stablehlo.slice
cache miss
342624074
%2411 = stablehlo.slice %2332 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 367

Operation name: stablehlo.slice
cache miss
342624074
%2412 = stablehlo.slice %2312 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 546

Operation name: stablehlo.slice
cache miss
342624074
%2413 = stablehlo.slice %2292 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 276

Operation name: stablehlo.slice
cache miss
342624074
%2414 = stablehlo.slice %2272 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 366

Operation name: stablehlo.concatenate
cache miss
3995287944
%2415 = stablehlo.concatenate %2409, %2410, %2411, %2412, %2413, %2414, dim = 1 : (tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<1x288xf32>
Cost: 539

Operation name: stablehlo.reshape
cache miss
1377738304
%2416 = stablehlo.reshape %2415 : (tensor<1x288xf32>) -> tensor<288xf32>
Cost: 516

Operation name: stablehlo.dot_general
cache miss
1491685315
%2417 = stablehlo.dot_general %2416, %245, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288x288xf32>) -> tensor<288xf32>
Cost: 223

Operation name: stablehlo.add
cache miss
3847999983
%2418 = stablehlo.add %2397, %2417 : tensor<288xf32>
Cost: 269

Operation name: stablehlo.multiply
cache miss
2511610444
%2419 = stablehlo.multiply %235, %2418 : tensor<288xf32>
Cost: 638

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 300

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%2420 = stablehlo.reduce(%2419 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<288xf32>, tensor<f32>) -> tensor<f32>
Cost: 298

Operation name: stablehlo.multiply
cache miss
1284973678
%2421 = stablehlo.multiply %2420, %234 : tensor<f32>
Cost: 305

Operation name: stablehlo.negate
cache miss
391293031
%2422 = stablehlo.negate %2421 : tensor<f32>
Cost: 516

Operation name: stablehlo.multiply
cache miss
1284973678
%2423 = stablehlo.multiply %2422, %231 : tensor<f32>
Cost: 302

Operation name: stablehlo.divide
cache miss
719102371
%2424 = stablehlo.divide %2423, %cst_5 : tensor<f32>
Cost: 427

Operation name: stablehlo.dot_general
cache miss
1413888672
%2425 = stablehlo.dot_general %2424, %224, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<f32>, tensor<288xf32>) -> tensor<288xf32>
Cost: 401

Operation name: stablehlo.add
cache miss
3847999983
%2426 = stablehlo.add %2245, %2425 : tensor<288xf32>
Cost: 427

Operation name: stablehlo.add
cache miss
3847999983
%2427 = stablehlo.add %2426, %2425 : tensor<288xf32>
Cost: 562

Operation name: stablehlo.multiply
cache miss
2511610444
%2428 = stablehlo.multiply %2418, %236 : tensor<288xf32>
Cost: 686

Operation name: stablehlo.multiply
cache miss
2511610444
%2429 = stablehlo.multiply %226, %2428 : tensor<288xf32>
Cost: 518

Operation name: stablehlo.add
cache miss
3847999983
%2430 = stablehlo.add %2427, %2429 : tensor<288xf32>
Cost: 510

Operation name: stablehlo.dot_general
cache miss
2840382466
%2431 = stablehlo.dot_general %2430, %222, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288x768xf32>) -> tensor<768xf32>
Cost: 7096

Operation name: stablehlo.multiply
cache miss
4137946040
%2432 = stablehlo.multiply %219, %2431 : tensor<768xf32>
Cost: 520

Operation name: stablehlo.dot_general
cache miss
1491685315
%2433 = stablehlo.dot_general %2432, %211, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<768xf32>, tensor<768x288xf32>) -> tensor<288xf32>
Cost: 4694

Operation name: stablehlo.dot_general
cache miss
1743356395
%2434 = stablehlo.dot_general %2432, %206, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<768xf32>, tensor<288xf32>) -> tensor<768x288xf32>
Cost: 74790

Operation name: stablehlo.reshape
cache miss
1628429499
%2435 = stablehlo.reshape %2434 : (tensor<768x288xf32>) -> tensor<1x768x288xf32>
Cost: 60563

Operation name: stablehlo.concatenate
cache miss
2658928759
%2436 = stablehlo.concatenate %2435, %2223, %2011, %1799, %1587, %1375, dim = 0 : (tensor<1x768x288xf32>, tensor<1x768x288xf32>, tensor<1x768x288xf32>, tensor<1x768x288xf32>, tensor<1x768x288xf32>, tensor<1x768x288xf32>) -> tensor<6x768x288xf32>
Cost: 174238

Operation name: stablehlo.multiply
cache miss
4137946040
%2437 = stablehlo.multiply %2431, %212 : tensor<768xf32>
Cost: 293

Operation name: stablehlo.multiply
cache miss
4137946040
%2438 = stablehlo.multiply %209, %2437 : tensor<768xf32>
Cost: 442

Operation name: stablehlo.multiply
cache miss
4137946040
%2439 = stablehlo.multiply %2438, %218 : tensor<768xf32>
Cost: 424

Operation name: stablehlo.negate
cache miss
4256221255
%2440 = stablehlo.negate %2439 : tensor<768xf32>
Cost: 280

Operation name: stablehlo.multiply
cache miss
4137946040
%2441 = stablehlo.multiply %2440, %214 : tensor<768xf32>
Cost: 288

Operation name: stablehlo.negate
cache miss
4256221255
%2442 = stablehlo.negate %2441 : tensor<768xf32>
Cost: 713

Operation name: stablehlo.multiply
cache miss
4137946040
%2443 = stablehlo.multiply %2437, %216 : tensor<768xf32>
Cost: 497

Operation name: stablehlo.add
cache miss
389450582
%2444 = stablehlo.add %2442, %2443 : tensor<768xf32>
Cost: 300

Operation name: stablehlo.dot_general
cache miss
1491685315
%2445 = stablehlo.dot_general %2444, %208, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<768xf32>, tensor<768x288xf32>) -> tensor<288xf32>
Cost: 4642

Operation name: stablehlo.add
cache miss
3847999983
%2446 = stablehlo.add %2433, %2445 : tensor<288xf32>
Cost: 404

Operation name: stablehlo.multiply
cache miss
2511610444
%2447 = stablehlo.multiply %204, %2446 : tensor<288xf32>
Cost: 580

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 382

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%2448 = stablehlo.reduce(%2447 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<288xf32>, tensor<f32>) -> tensor<f32>
Cost: 308

Operation name: stablehlo.multiply
cache miss
1284973678
%2449 = stablehlo.multiply %2448, %203 : tensor<f32>
Cost: 301

Operation name: stablehlo.negate
cache miss
391293031
%2450 = stablehlo.negate %2449 : tensor<f32>
Cost: 398

Operation name: stablehlo.multiply
cache miss
1284973678
%2451 = stablehlo.multiply %2450, %200 : tensor<f32>
Cost: 385

Operation name: stablehlo.divide
cache miss
719102371
%2452 = stablehlo.divide %2451, %cst_5 : tensor<f32>
Cost: 295

Operation name: stablehlo.dot_general
cache miss
1413888672
%2453 = stablehlo.dot_general %2452, %193, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<f32>, tensor<288xf32>) -> tensor<288xf32>
Cost: 570

Operation name: stablehlo.add
cache miss
3847999983
%2454 = stablehlo.add %2430, %2453 : tensor<288xf32>
Cost: 541

Operation name: stablehlo.add
cache miss
3847999983
%2455 = stablehlo.add %2454, %2453 : tensor<288xf32>
Cost: 383

Operation name: stablehlo.multiply
cache miss
2511610444
%2456 = stablehlo.multiply %2446, %205 : tensor<288xf32>
Cost: 624

Operation name: stablehlo.multiply
cache miss
2511610444
%2457 = stablehlo.multiply %195, %2456 : tensor<288xf32>
Cost: 645

Operation name: stablehlo.add
cache miss
3847999983
%2458 = stablehlo.add %2455, %2457 : tensor<288xf32>
Cost: 402

Operation name: stablehlo.dot_general
cache miss
1491685315
%2459 = stablehlo.dot_general %2458, %191, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288x288xf32>) -> tensor<288xf32>
Cost: 478

Operation name: stablehlo.slice
cache miss
2839717066
%2460 = stablehlo.slice %2459 [0:48] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 371

Operation name: stablehlo.slice
cache miss
4032389970
%2461 = stablehlo.slice %2459 [48:96] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 368

Operation name: stablehlo.slice
cache miss
1375222232
%2462 = stablehlo.slice %2459 [96:144] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 276

Operation name: stablehlo.slice
cache miss
3469783879
%2463 = stablehlo.slice %2459 [144:192] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 461

Operation name: stablehlo.slice
cache miss
1801550340
%2464 = stablehlo.slice %2459 [192:240] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 366

Operation name: stablehlo.slice
cache miss
2798019243
%2465 = stablehlo.slice %2459 [240:288] : (tensor<288xf32>) -> tensor<48xf32>
Cost: 388

Operation name: stablehlo.dot_general
cache miss
597698865
%2466 = stablehlo.dot_general %2465, %187, contracting_dims = [0] x [1], precision = [DEFAULT, DEFAULT] : (tensor<48xf32>, tensor<2x48xf32>) -> tensor<2xf32>
Cost: 466

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%2467 = stablehlo.broadcast_in_dim %184, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 388

Operation name: stablehlo.multiply
cache miss
806204014
%2468 = stablehlo.multiply %2466, %2467 : tensor<2xf32>
Cost: 289

Operation name: stablehlo.multiply
cache miss
806204014
%2469 = stablehlo.multiply %2468, %175 : tensor<2xf32>
Cost: 389

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 304

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%2470 = stablehlo.reduce(%2469 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 300

Operation name: stablehlo.negate
cache miss
391293031
%2471 = stablehlo.negate %2470 : tensor<f32>
Cost: 290

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%2472 = stablehlo.broadcast_in_dim %2471, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 295

Operation name: stablehlo.divide
cache miss
1531341356
%2473 = stablehlo.divide %2466, %181 : tensor<2xf32>
Cost: 205

Operation name: stablehlo.add
cache miss
1018830424
%2474 = stablehlo.add %2472, %2473 : tensor<2xf32>
Cost: 494

Operation name: stablehlo.multiply
cache miss
806204014
%2475 = stablehlo.multiply %2474, %175 : tensor<2xf32>
Cost: 405

Operation name: stablehlo.negate
cache miss
37341164
%2476 = stablehlo.negate %2475 : tensor<2xf32>
Cost: 382

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 463

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%2477 = stablehlo.reduce(%2476 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 301

Operation name: stablehlo.divide
cache miss
719102371
%2478 = stablehlo.divide %2477, %173 : tensor<f32>
Cost: 197

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%2479 = stablehlo.broadcast_in_dim %2478, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 194

Operation name: stablehlo.multiply
cache miss
806204014
%2480 = stablehlo.multiply %2479, %172 : tensor<2xf32>
Cost: 383

Operation name: stablehlo.add
cache miss
1018830424
%2481 = stablehlo.add %2475, %2480 : tensor<2xf32>
Cost: 299

Operation name: stablehlo.divide
cache miss
1531341356
%2482 = stablehlo.divide %2481, %cst_1 : tensor<2xf32>
Cost: 397

Operation name: stablehlo.dot_general
cache miss
2902940993
%2483 = stablehlo.dot_general %2482, %166, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<2x48xf32>) -> tensor<48xf32>
Cost: 289

Operation name: stablehlo.dot_general
cache miss
2229247683
%2484 = stablehlo.dot_general %2482, %163, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 276

Operation name: stablehlo.dot_general
cache miss
2229247683
%2485 = stablehlo.dot_general %182, %2465, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 369

Operation name: stablehlo.dot_general
cache miss
597698865
%2486 = stablehlo.dot_general %2464, %161, contracting_dims = [0] x [1], precision = [DEFAULT, DEFAULT] : (tensor<48xf32>, tensor<2x48xf32>) -> tensor<2xf32>
Cost: 493

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%2487 = stablehlo.broadcast_in_dim %158, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 293

Operation name: stablehlo.multiply
cache miss
806204014
%2488 = stablehlo.multiply %2486, %2487 : tensor<2xf32>
Cost: 281

Operation name: stablehlo.multiply
cache miss
806204014
%2489 = stablehlo.multiply %2488, %149 : tensor<2xf32>
Cost: 191

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 243

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%2490 = stablehlo.reduce(%2489 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 327

Operation name: stablehlo.negate
cache miss
391293031
%2491 = stablehlo.negate %2490 : tensor<f32>
Cost: 197

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%2492 = stablehlo.broadcast_in_dim %2491, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 298

Operation name: stablehlo.divide
cache miss
1531341356
%2493 = stablehlo.divide %2486, %155 : tensor<2xf32>
Cost: 303

Operation name: stablehlo.add
cache miss
1018830424
%2494 = stablehlo.add %2492, %2493 : tensor<2xf32>
Cost: 179

Operation name: stablehlo.multiply
cache miss
806204014
%2495 = stablehlo.multiply %2494, %149 : tensor<2xf32>
Cost: 376

Operation name: stablehlo.negate
cache miss
37341164
%2496 = stablehlo.negate %2495 : tensor<2xf32>
Cost: 298

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 180

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%2497 = stablehlo.reduce(%2496 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 282

Operation name: stablehlo.divide
cache miss
719102371
%2498 = stablehlo.divide %2497, %147 : tensor<f32>
Cost: 311

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%2499 = stablehlo.broadcast_in_dim %2498, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 287

Operation name: stablehlo.multiply
cache miss
806204014
%2500 = stablehlo.multiply %2499, %146 : tensor<2xf32>
Cost: 373

Operation name: stablehlo.add
cache miss
1018830424
%2501 = stablehlo.add %2495, %2500 : tensor<2xf32>
Cost: 281

Operation name: stablehlo.divide
cache miss
1531341356
%2502 = stablehlo.divide %2501, %cst_1 : tensor<2xf32>
Cost: 284

Operation name: stablehlo.dot_general
cache miss
2902940993
%2503 = stablehlo.dot_general %2502, %140, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<2x48xf32>) -> tensor<48xf32>
Cost: 293

Operation name: stablehlo.dot_general
cache miss
2229247683
%2504 = stablehlo.dot_general %2502, %137, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 390

Operation name: stablehlo.dot_general
cache miss
2229247683
%2505 = stablehlo.dot_general %156, %2464, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 280

Operation name: stablehlo.dot_general
cache miss
597698865
%2506 = stablehlo.dot_general %2463, %135, contracting_dims = [0] x [1], precision = [DEFAULT, DEFAULT] : (tensor<48xf32>, tensor<2x48xf32>) -> tensor<2xf32>
Cost: 364

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%2507 = stablehlo.broadcast_in_dim %132, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 379

Operation name: stablehlo.multiply
cache miss
806204014
%2508 = stablehlo.multiply %2506, %2507 : tensor<2xf32>
Cost: 291

Operation name: stablehlo.multiply
cache miss
806204014
%2509 = stablehlo.multiply %2508, %123 : tensor<2xf32>
Cost: 272

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 273

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%2510 = stablehlo.reduce(%2509 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 439

Operation name: stablehlo.negate
cache miss
391293031
%2511 = stablehlo.negate %2510 : tensor<f32>
Cost: 277

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%2512 = stablehlo.broadcast_in_dim %2511, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 269

Operation name: stablehlo.divide
cache miss
1531341356
%2513 = stablehlo.divide %2506, %129 : tensor<2xf32>
Cost: 278

Operation name: stablehlo.add
cache miss
1018830424
%2514 = stablehlo.add %2512, %2513 : tensor<2xf32>
Cost: 271

Operation name: stablehlo.multiply
cache miss
806204014
%2515 = stablehlo.multiply %2514, %123 : tensor<2xf32>
Cost: 178

Operation name: stablehlo.negate
cache miss
37341164
%2516 = stablehlo.negate %2515 : tensor<2xf32>
Cost: 363

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 364

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%2517 = stablehlo.reduce(%2516 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 275

Operation name: stablehlo.divide
cache miss
719102371
%2518 = stablehlo.divide %2517, %121 : tensor<f32>
Cost: 391

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%2519 = stablehlo.broadcast_in_dim %2518, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 271

Operation name: stablehlo.multiply
cache miss
806204014
%2520 = stablehlo.multiply %2519, %120 : tensor<2xf32>
Cost: 435

Operation name: stablehlo.add
cache miss
1018830424
%2521 = stablehlo.add %2515, %2520 : tensor<2xf32>
Cost: 500

Operation name: stablehlo.divide
cache miss
1531341356
%2522 = stablehlo.divide %2521, %cst_1 : tensor<2xf32>
Cost: 306

Operation name: stablehlo.dot_general
cache miss
2902940993
%2523 = stablehlo.dot_general %2522, %114, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<2x48xf32>) -> tensor<48xf32>
Cost: 267

Operation name: stablehlo.dot_general
cache miss
2229247683
%2524 = stablehlo.dot_general %2522, %111, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 376

Operation name: stablehlo.dot_general
cache miss
2229247683
%2525 = stablehlo.dot_general %130, %2463, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 281

Operation name: stablehlo.dot_general
cache miss
597698865
%2526 = stablehlo.dot_general %2462, %109, contracting_dims = [0] x [1], precision = [DEFAULT, DEFAULT] : (tensor<48xf32>, tensor<2x48xf32>) -> tensor<2xf32>
Cost: 171

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%2527 = stablehlo.broadcast_in_dim %106, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 275

Operation name: stablehlo.multiply
cache miss
806204014
%2528 = stablehlo.multiply %2526, %2527 : tensor<2xf32>
Cost: 171

Operation name: stablehlo.multiply
cache miss
806204014
%2529 = stablehlo.multiply %2528, %97 : tensor<2xf32>
Cost: 289

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 236

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%2530 = stablehlo.reduce(%2529 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 366

Operation name: stablehlo.negate
cache miss
391293031
%2531 = stablehlo.negate %2530 : tensor<f32>
Cost: 365

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%2532 = stablehlo.broadcast_in_dim %2531, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 293

Operation name: stablehlo.divide
cache miss
1531341356
%2533 = stablehlo.divide %2526, %103 : tensor<2xf32>
Cost: 410

Operation name: stablehlo.add
cache miss
1018830424
%2534 = stablehlo.add %2532, %2533 : tensor<2xf32>
Cost: 485

Operation name: stablehlo.multiply
cache miss
806204014
%2535 = stablehlo.multiply %2534, %97 : tensor<2xf32>
Cost: 363

Operation name: stablehlo.negate
cache miss
37341164
%2536 = stablehlo.negate %2535 : tensor<2xf32>
Cost: 362

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 176

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%2537 = stablehlo.reduce(%2536 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 274

Operation name: stablehlo.divide
cache miss
719102371
%2538 = stablehlo.divide %2537, %95 : tensor<f32>
Cost: 273

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%2539 = stablehlo.broadcast_in_dim %2538, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 172

Operation name: stablehlo.multiply
cache miss
806204014
%2540 = stablehlo.multiply %2539, %94 : tensor<2xf32>
Cost: 270

Operation name: stablehlo.add
cache miss
1018830424
%2541 = stablehlo.add %2535, %2540 : tensor<2xf32>
Cost: 271

Operation name: stablehlo.divide
cache miss
1531341356
%2542 = stablehlo.divide %2541, %cst_1 : tensor<2xf32>
Cost: 269

Operation name: stablehlo.dot_general
cache miss
2902940993
%2543 = stablehlo.dot_general %2542, %88, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<2x48xf32>) -> tensor<48xf32>
Cost: 279

Operation name: stablehlo.dot_general
cache miss
2229247683
%2544 = stablehlo.dot_general %2542, %85, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 297

Operation name: stablehlo.dot_general
cache miss
2229247683
%2545 = stablehlo.dot_general %104, %2462, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 456

Operation name: stablehlo.dot_general
cache miss
597698865
%2546 = stablehlo.dot_general %2461, %83, contracting_dims = [0] x [1], precision = [DEFAULT, DEFAULT] : (tensor<48xf32>, tensor<2x48xf32>) -> tensor<2xf32>
Cost: 178

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%2547 = stablehlo.broadcast_in_dim %80, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 362

Operation name: stablehlo.multiply
cache miss
806204014
%2548 = stablehlo.multiply %2546, %2547 : tensor<2xf32>
Cost: 275

Operation name: stablehlo.multiply
cache miss
806204014
%2549 = stablehlo.multiply %2548, %71 : tensor<2xf32>
Cost: 272

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 368

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%2550 = stablehlo.reduce(%2549 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 282

Operation name: stablehlo.negate
cache miss
391293031
%2551 = stablehlo.negate %2550 : tensor<f32>
Cost: 362

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%2552 = stablehlo.broadcast_in_dim %2551, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 301

Operation name: stablehlo.divide
cache miss
1531341356
%2553 = stablehlo.divide %2546, %77 : tensor<2xf32>
Cost: 274

Operation name: stablehlo.add
cache miss
1018830424
%2554 = stablehlo.add %2552, %2553 : tensor<2xf32>
Cost: 273

Operation name: stablehlo.multiply
cache miss
806204014
%2555 = stablehlo.multiply %2554, %71 : tensor<2xf32>
Cost: 272

Operation name: stablehlo.negate
cache miss
37341164
%2556 = stablehlo.negate %2555 : tensor<2xf32>
Cost: 386

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 366

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%2557 = stablehlo.reduce(%2556 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 280

Operation name: stablehlo.divide
cache miss
719102371
%2558 = stablehlo.divide %2557, %69 : tensor<f32>
Cost: 273

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%2559 = stablehlo.broadcast_in_dim %2558, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 466

Operation name: stablehlo.multiply
cache miss
806204014
%2560 = stablehlo.multiply %2559, %68 : tensor<2xf32>
Cost: 271

Operation name: stablehlo.add
cache miss
1018830424
%2561 = stablehlo.add %2555, %2560 : tensor<2xf32>
Cost: 172

Operation name: stablehlo.divide
cache miss
1531341356
%2562 = stablehlo.divide %2561, %cst_1 : tensor<2xf32>
Cost: 270

Operation name: stablehlo.dot_general
cache miss
2902940993
%2563 = stablehlo.dot_general %2562, %62, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<2x48xf32>) -> tensor<48xf32>
Cost: 360

Operation name: stablehlo.dot_general
cache miss
2229247683
%2564 = stablehlo.dot_general %2562, %59, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 401

Operation name: stablehlo.dot_general
cache miss
2229247683
%2565 = stablehlo.dot_general %78, %2461, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 374

Operation name: stablehlo.dot_general
cache miss
597698865
%2566 = stablehlo.dot_general %2460, %57, contracting_dims = [0] x [1], precision = [DEFAULT, DEFAULT] : (tensor<48xf32>, tensor<2x48xf32>) -> tensor<2xf32>
Cost: 279

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%2567 = stablehlo.broadcast_in_dim %54, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 274

Operation name: stablehlo.multiply
cache miss
806204014
%2568 = stablehlo.multiply %2566, %2567 : tensor<2xf32>
Cost: 364

Operation name: stablehlo.multiply
cache miss
806204014
%2569 = stablehlo.multiply %2568, %45 : tensor<2xf32>
Cost: 271

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 169

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%2570 = stablehlo.reduce(%2569 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 365

Operation name: stablehlo.negate
cache miss
391293031
%2571 = stablehlo.negate %2570 : tensor<f32>
Cost: 284

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%2572 = stablehlo.broadcast_in_dim %2571, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 365

Operation name: stablehlo.divide
cache miss
1531341356
%2573 = stablehlo.divide %2566, %51 : tensor<2xf32>
Cost: 278

Operation name: stablehlo.add
cache miss
1018830424
%2574 = stablehlo.add %2572, %2573 : tensor<2xf32>
Cost: 298

Operation name: stablehlo.multiply
cache miss
806204014
%2575 = stablehlo.multiply %2574, %45 : tensor<2xf32>
Cost: 383

Operation name: stablehlo.negate
cache miss
37341164
%2576 = stablehlo.negate %2575 : tensor<2xf32>
Cost: 367

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 382

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%2577 = stablehlo.reduce(%2576 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<2xf32>, tensor<f32>) -> tensor<f32>
Cost: 299

Operation name: stablehlo.divide
cache miss
719102371
%2578 = stablehlo.divide %2577, %43 : tensor<f32>
Cost: 276

Operation name: stablehlo.broadcast_in_dim
cache miss
1703569158
%2579 = stablehlo.broadcast_in_dim %2578, dims = [] : (tensor<f32>) -> tensor<2xf32>
Cost: 369

Operation name: stablehlo.multiply
cache miss
806204014
%2580 = stablehlo.multiply %2579, %42 : tensor<2xf32>
Cost: 296

Operation name: stablehlo.add
cache miss
1018830424
%2581 = stablehlo.add %2575, %2580 : tensor<2xf32>
Cost: 297

Operation name: stablehlo.divide
cache miss
1531341356
%2582 = stablehlo.divide %2581, %cst_1 : tensor<2xf32>
Cost: 360

Operation name: stablehlo.dot_general
cache miss
2902940993
%2583 = stablehlo.dot_general %2582, %36, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<2x48xf32>) -> tensor<48xf32>
Cost: 352

Operation name: stablehlo.concatenate
cache miss
292037604
%2584 = stablehlo.concatenate %2583, %2563, %2543, %2523, %2503, %2483, dim = 0 : (tensor<48xf32>, tensor<48xf32>, tensor<48xf32>, tensor<48xf32>, tensor<48xf32>, tensor<48xf32>) -> tensor<288xf32>
Cost: 416

Operation name: stablehlo.reshape
cache miss
2392077026
%2585 = stablehlo.reshape %2584 : (tensor<288xf32>) -> tensor<144x2xf32>
Cost: 284

Operation name: stablehlo.dot_general
cache miss
1714835160
%2586 = stablehlo.dot_general %2585, %cst_8, batching_dims = [0] x [0], contracting_dims = [1] x [1], precision = [DEFAULT, DEFAULT] : (tensor<144x2xf32>, tensor<144x2x2xf32>) -> tensor<144x2xf32>
Cost: 580

Operation name: stablehlo.reshape
cache miss
1377738304
%2587 = stablehlo.reshape %2586 : (tensor<144x2xf32>) -> tensor<288xf32>
Cost: 276

Operation name: stablehlo.dot_general
cache miss
1491685315
%2588 = stablehlo.dot_general %2587, %14, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288x288xf32>) -> tensor<288xf32>
Cost: 452

Operation name: stablehlo.dot_general
cache miss
3876455518
%2589 = stablehlo.dot_general %2587, %12, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288xf32>) -> tensor<288x288xf32>
Cost: 46460

Operation name: stablehlo.reshape
cache miss
552616300
%2590 = stablehlo.reshape %2589 : (tensor<288x288xf32>) -> tensor<1x288x288xf32>
Cost: 20474

Operation name: stablehlo.concatenate
cache miss
1946357965
%2591 = stablehlo.concatenate %2590, %2377, %2165, %1953, %1741, %1529, dim = 0 : (tensor<1x288x288xf32>, tensor<1x288x288xf32>, tensor<1x288x288xf32>, tensor<1x288x288xf32>, tensor<1x288x288xf32>, tensor<1x288x288xf32>) -> tensor<6x288x288xf32>
Cost: 146689

Operation name: stablehlo.dot_general
cache miss
2229247683
%2592 = stablehlo.dot_general %2582, %33, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 176

Operation name: stablehlo.slice
cache miss
3729170049
%2593 = stablehlo.slice %2592 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 372

Operation name: stablehlo.slice
cache miss
3729170049
%2594 = stablehlo.slice %2564 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 302

Operation name: stablehlo.slice
cache miss
3729170049
%2595 = stablehlo.slice %2544 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 387

Operation name: stablehlo.slice
cache miss
3729170049
%2596 = stablehlo.slice %2524 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 307

Operation name: stablehlo.slice
cache miss
3729170049
%2597 = stablehlo.slice %2504 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 285

Operation name: stablehlo.slice
cache miss
3729170049
%2598 = stablehlo.slice %2484 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 177

Operation name: stablehlo.concatenate
cache miss
3995287944
%2599 = stablehlo.concatenate %2593, %2594, %2595, %2596, %2597, %2598, dim = 1 : (tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<1x288xf32>
Cost: 405

Operation name: stablehlo.slice
cache miss
342624074
%2600 = stablehlo.slice %2592 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 300

Operation name: stablehlo.slice
cache miss
342624074
%2601 = stablehlo.slice %2564 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 368

Operation name: stablehlo.slice
cache miss
342624074
%2602 = stablehlo.slice %2544 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 389

Operation name: stablehlo.slice
cache miss
342624074
%2603 = stablehlo.slice %2524 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 277

Operation name: stablehlo.slice
cache miss
342624074
%2604 = stablehlo.slice %2504 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 283

Operation name: stablehlo.slice
cache miss
342624074
%2605 = stablehlo.slice %2484 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 281

Operation name: stablehlo.concatenate
cache miss
3995287944
%2606 = stablehlo.concatenate %2600, %2601, %2602, %2603, %2604, %2605, dim = 1 : (tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<1x288xf32>
Cost: 684

Operation name: stablehlo.reshape
cache miss
2392077026
%2607 = stablehlo.reshape %2606 : (tensor<1x288xf32>) -> tensor<144x2xf32>
Cost: 278

Operation name: stablehlo.dot_general
cache miss
1714835160
%2608 = stablehlo.dot_general %2607, %cst_8, batching_dims = [0] x [0], contracting_dims = [1] x [1], precision = [DEFAULT, DEFAULT] : (tensor<144x2xf32>, tensor<144x2x2xf32>) -> tensor<144x2xf32>
Cost: 592

Operation name: stablehlo.reshape
cache miss
1377738304
%2609 = stablehlo.reshape %2608 : (tensor<144x2xf32>) -> tensor<288xf32>
Cost: 421

Operation name: stablehlo.dot_general
cache miss
1491685315
%2610 = stablehlo.dot_general %2609, %17, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288x288xf32>) -> tensor<288xf32>
Cost: 357

Operation name: stablehlo.add
cache miss
3847999983
%2611 = stablehlo.add %2588, %2610 : tensor<288xf32>
Cost: 543

Operation name: stablehlo.dot_general
cache miss
3876455518
%2612 = stablehlo.dot_general %2609, %12, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288xf32>) -> tensor<288x288xf32>
Cost: 40251

Operation name: stablehlo.reshape
cache miss
552616300
%2613 = stablehlo.reshape %2612 : (tensor<288x288xf32>) -> tensor<1x288x288xf32>
Cost: 34753

Operation name: stablehlo.concatenate
cache miss
1946357965
%2614 = stablehlo.concatenate %2613, %2399, %2187, %1975, %1763, %1551, dim = 0 : (tensor<1x288x288xf32>, tensor<1x288x288xf32>, tensor<1x288x288xf32>, tensor<1x288x288xf32>, tensor<1x288x288xf32>, tensor<1x288x288xf32>) -> tensor<6x288x288xf32>
Cost: 127325

Operation name: stablehlo.reshape
cache miss
1853321842
%2615 = stablehlo.reshape %2599 : (tensor<1x288xf32>) -> tensor<1x1x288xf32>
Cost: 459

Operation name: stablehlo.concatenate
cache miss
1648206339
%2616 = stablehlo.concatenate %2615, %2400, %2188, %1976, %1764, %1552, dim = 0 : (tensor<1x1x288xf32>, tensor<1x1x288xf32>, tensor<1x1x288xf32>, tensor<1x1x288xf32>, tensor<1x1x288xf32>, tensor<1x1x288xf32>) -> tensor<6x1x288xf32>
Cost: 511

Operation name: stablehlo.dot_general
cache miss
2229247683
%2617 = stablehlo.dot_general %52, %2460, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<2xf32>, tensor<48xf32>) -> tensor<2x48xf32>
Cost: 176

Operation name: stablehlo.slice
cache miss
3729170049
%2618 = stablehlo.slice %2617 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 301

Operation name: stablehlo.slice
cache miss
3729170049
%2619 = stablehlo.slice %2565 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 277

Operation name: stablehlo.slice
cache miss
3729170049
%2620 = stablehlo.slice %2545 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 453

Operation name: stablehlo.slice
cache miss
3729170049
%2621 = stablehlo.slice %2525 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 173

Operation name: stablehlo.slice
cache miss
3729170049
%2622 = stablehlo.slice %2505 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 371

Operation name: stablehlo.slice
cache miss
3729170049
%2623 = stablehlo.slice %2485 [0:1, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 193

Operation name: stablehlo.concatenate
cache miss
3995287944
%2624 = stablehlo.concatenate %2618, %2619, %2620, %2621, %2622, %2623, dim = 1 : (tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<1x288xf32>
Cost: 280

Operation name: stablehlo.slice
cache miss
342624074
%2625 = stablehlo.slice %2617 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 174

Operation name: stablehlo.slice
cache miss
342624074
%2626 = stablehlo.slice %2565 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 177

Operation name: stablehlo.slice
cache miss
342624074
%2627 = stablehlo.slice %2545 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 175

Operation name: stablehlo.slice
cache miss
342624074
%2628 = stablehlo.slice %2525 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 173

Operation name: stablehlo.slice
cache miss
342624074
%2629 = stablehlo.slice %2505 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 371

Operation name: stablehlo.slice
cache miss
342624074
%2630 = stablehlo.slice %2485 [1:2, 0:48] : (tensor<2x48xf32>) -> tensor<1x48xf32>
Cost: 375

Operation name: stablehlo.concatenate
cache miss
3995287944
%2631 = stablehlo.concatenate %2625, %2626, %2627, %2628, %2629, %2630, dim = 1 : (tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>, tensor<1x48xf32>) -> tensor<1x288xf32>
Cost: 423

Operation name: stablehlo.reshape
cache miss
1377738304
%2632 = stablehlo.reshape %2631 : (tensor<1x288xf32>) -> tensor<288xf32>
Cost: 403

Operation name: stablehlo.dot_general
cache miss
1491685315
%2633 = stablehlo.dot_general %2632, %20, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288x288xf32>) -> tensor<288xf32>
Cost: 222

Operation name: stablehlo.add
cache miss
3847999983
%2634 = stablehlo.add %2611, %2633 : tensor<288xf32>
Cost: 442

Operation name: stablehlo.multiply
cache miss
2511610444
%2635 = stablehlo.multiply %10, %2634 : tensor<288xf32>
Cost: 407

Operation name: stablehlo.add
cache miss
1937139428
%2733 = stablehlo.add %arg14, %arg15 : tensor<f32>
Cost: 193

Operation name: stablehlo.return
Cost: 0

Operation name: stablehlo.reduce
cache miss
555889745
%2636 = stablehlo.reduce(%2635 init: %cst_2) applies stablehlo.add across dimensions = [0] : (tensor<288xf32>, tensor<f32>) -> tensor<f32>
Cost: 411

Operation name: stablehlo.multiply
cache miss
1284973678
%2637 = stablehlo.multiply %2636, %9 : tensor<f32>
Cost: 324

Operation name: stablehlo.negate
cache miss
391293031
%2638 = stablehlo.negate %2637 : tensor<f32>
Cost: 405

Operation name: stablehlo.multiply
cache miss
1284973678
%2639 = stablehlo.multiply %2638, %6 : tensor<f32>
Cost: 389

Operation name: stablehlo.divide
cache miss
719102371
%2640 = stablehlo.divide %2639, %cst_5 : tensor<f32>
Cost: 300

Operation name: stablehlo.dot_general
cache miss
1413888672
%2641 = stablehlo.dot_general %2640, %arg0, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<f32>, tensor<288xf32>) -> tensor<288xf32>
Cost: 421

Operation name: stablehlo.add
cache miss
3847999983
%2642 = stablehlo.add %2458, %2641 : tensor<288xf32>
Cost: 403

Operation name: stablehlo.add
cache miss
3847999983
%2643 = stablehlo.add %2642, %2641 : tensor<288xf32>
Cost: 396

Operation name: stablehlo.multiply
cache miss
2511610444
%2644 = stablehlo.multiply %2634, %11 : tensor<288xf32>
Cost: 612

Operation name: stablehlo.multiply
cache miss
2511610444
%2645 = stablehlo.multiply %1, %2644 : tensor<288xf32>
Cost: 497

Operation name: stablehlo.add
cache miss
3847999983
%2646 = stablehlo.add %2643, %2645 : tensor<288xf32>
Cost: 579

Operation name: stablehlo.multiply
cache miss
2511610444
%2647 = stablehlo.multiply %2644, %arg0 : tensor<288xf32>
Cost: 512

Operation name: stablehlo.reshape
cache miss
2960881163
%2648 = stablehlo.reshape %2647 : (tensor<288xf32>) -> tensor<1x288xf32>
Cost: 406

Operation name: stablehlo.dot_general
cache miss
3876455518
%2649 = stablehlo.dot_general %2632, %12, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288xf32>) -> tensor<288x288xf32>
Cost: 31256

Operation name: stablehlo.reshape
cache miss
552616300
%2650 = stablehlo.reshape %2649 : (tensor<288x288xf32>) -> tensor<1x288x288xf32>
Cost: 35992

Operation name: stablehlo.reshape
cache miss
1853321842
%2651 = stablehlo.reshape %2624 : (tensor<1x288xf32>) -> tensor<1x1x288xf32>
Cost: 284

Operation name: stablehlo.dot_general
cache miss
3876455518
%2652 = stablehlo.dot_general %2458, %189, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288xf32>) -> tensor<288x288xf32>
Cost: 20769

Operation name: stablehlo.reshape
cache miss
552616300
%2653 = stablehlo.reshape %2652 : (tensor<288x288xf32>) -> tensor<1x288x288xf32>
Cost: 23236

Operation name: stablehlo.multiply
cache miss
2511610444
%2654 = stablehlo.multiply %2456, %193 : tensor<288xf32>
Cost: 565

Operation name: stablehlo.reshape
cache miss
2960881163
%2655 = stablehlo.reshape %2654 : (tensor<288xf32>) -> tensor<1x288xf32>
Cost: 537

Operation name: stablehlo.dot_general
cache miss
1743356395
%2656 = stablehlo.dot_general %2444, %206, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<768xf32>, tensor<288xf32>) -> tensor<768x288xf32>
Cost: 43343

Operation name: stablehlo.reshape
cache miss
1628429499
%2657 = stablehlo.reshape %2656 : (tensor<768x288xf32>) -> tensor<1x768x288xf32>
Cost: 43131

Operation name: stablehlo.dot_general
cache miss
9312040
%2658 = stablehlo.dot_general %2430, %220, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<768xf32>) -> tensor<288x768xf32>
Cost: 77044

Operation name: stablehlo.reshape
cache miss
3198799925
%2659 = stablehlo.reshape %2658 : (tensor<288x768xf32>) -> tensor<1x288x768xf32>
Cost: 59048

Operation name: stablehlo.multiply
cache miss
2511610444
%2660 = stablehlo.multiply %2428, %224 : tensor<288xf32>
Cost: 262

Operation name: stablehlo.reshape
cache miss
2960881163
%2661 = stablehlo.reshape %2660 : (tensor<288xf32>) -> tensor<1x288xf32>
Cost: 407

Operation name: stablehlo.dot_general
cache miss
3876455518
%2662 = stablehlo.dot_general %2416, %237, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288xf32>) -> tensor<288x288xf32>
Cost: 37940

Operation name: stablehlo.reshape
cache miss
552616300
%2663 = stablehlo.reshape %2662 : (tensor<288x288xf32>) -> tensor<1x288x288xf32>
Cost: 32198

Operation name: stablehlo.reshape
cache miss
1853321842
%2664 = stablehlo.reshape %2408 : (tensor<1x288xf32>) -> tensor<1x1x288xf32>
Cost: 319

Operation name: stablehlo.dot_general
cache miss
3876455518
%2665 = stablehlo.dot_general %2245, %414, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288xf32>) -> tensor<288x288xf32>
Cost: 43586

Operation name: stablehlo.reshape
cache miss
552616300
%2666 = stablehlo.reshape %2665 : (tensor<288x288xf32>) -> tensor<1x288x288xf32>
Cost: 43744

Operation name: stablehlo.multiply
cache miss
2511610444
%2667 = stablehlo.multiply %2243, %418 : tensor<288xf32>
Cost: 426

Operation name: stablehlo.reshape
cache miss
2960881163
%2668 = stablehlo.reshape %2667 : (tensor<288xf32>) -> tensor<1x288xf32>
Cost: 543

Operation name: stablehlo.dot_general
cache miss
1743356395
%2669 = stablehlo.dot_general %2231, %431, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<768xf32>, tensor<288xf32>) -> tensor<768x288xf32>
Cost: 73132

Operation name: stablehlo.reshape
cache miss
1628429499
%2670 = stablehlo.reshape %2669 : (tensor<768x288xf32>) -> tensor<1x768x288xf32>
Cost: 65431

Operation name: stablehlo.dot_general
cache miss
9312040
%2671 = stablehlo.dot_general %2218, %445, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<768xf32>) -> tensor<288x768xf32>
Cost: 54435

Operation name: stablehlo.reshape
cache miss
3198799925
%2672 = stablehlo.reshape %2671 : (tensor<288x768xf32>) -> tensor<1x288x768xf32>
Cost: 50261

Operation name: stablehlo.multiply
cache miss
2511610444
%2673 = stablehlo.multiply %2216, %449 : tensor<288xf32>
Cost: 412

Operation name: stablehlo.reshape
cache miss
2960881163
%2674 = stablehlo.reshape %2673 : (tensor<288xf32>) -> tensor<1x288xf32>
Cost: 575

Operation name: stablehlo.dot_general
cache miss
3876455518
%2675 = stablehlo.dot_general %2204, %462, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288xf32>) -> tensor<288x288xf32>
Cost: 21879

Operation name: stablehlo.reshape
cache miss
552616300
%2676 = stablehlo.reshape %2675 : (tensor<288x288xf32>) -> tensor<1x288x288xf32>
Cost: 15664

Operation name: stablehlo.reshape
cache miss
1853321842
%2677 = stablehlo.reshape %2196 : (tensor<1x288xf32>) -> tensor<1x1x288xf32>
Cost: 281

Operation name: stablehlo.dot_general
cache miss
3876455518
%2678 = stablehlo.dot_general %2033, %639, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288xf32>) -> tensor<288x288xf32>
Cost: 15958

Operation name: stablehlo.reshape
cache miss
552616300
%2679 = stablehlo.reshape %2678 : (tensor<288x288xf32>) -> tensor<1x288x288xf32>
Cost: 41044

Operation name: stablehlo.multiply
cache miss
2511610444
%2680 = stablehlo.multiply %2031, %643 : tensor<288xf32>
Cost: 545

Operation name: stablehlo.reshape
cache miss
2960881163
%2681 = stablehlo.reshape %2680 : (tensor<288xf32>) -> tensor<1x288xf32>
Cost: 441

Operation name: stablehlo.dot_general
cache miss
1743356395
%2682 = stablehlo.dot_general %2019, %656, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<768xf32>, tensor<288xf32>) -> tensor<768x288xf32>
Cost: 39760

Operation name: stablehlo.reshape
cache miss
1628429499
%2683 = stablehlo.reshape %2682 : (tensor<768x288xf32>) -> tensor<1x768x288xf32>
Cost: 71890

Operation name: stablehlo.dot_general
cache miss
9312040
%2684 = stablehlo.dot_general %2006, %670, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<768xf32>) -> tensor<288x768xf32>
Cost: 55933

Operation name: stablehlo.reshape
cache miss
3198799925
%2685 = stablehlo.reshape %2684 : (tensor<288x768xf32>) -> tensor<1x288x768xf32>
Cost: 65277

Operation name: stablehlo.multiply
cache miss
2511610444
%2686 = stablehlo.multiply %2004, %674 : tensor<288xf32>
Cost: 388

Operation name: stablehlo.reshape
cache miss
2960881163
%2687 = stablehlo.reshape %2686 : (tensor<288xf32>) -> tensor<1x288xf32>
Cost: 482

Operation name: stablehlo.dot_general
cache miss
3876455518
%2688 = stablehlo.dot_general %1992, %687, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288xf32>) -> tensor<288x288xf32>
Cost: 40011

Operation name: stablehlo.reshape
cache miss
552616300
%2689 = stablehlo.reshape %2688 : (tensor<288x288xf32>) -> tensor<1x288x288xf32>
Cost: 17776

Operation name: stablehlo.reshape
cache miss
1853321842
%2690 = stablehlo.reshape %1984 : (tensor<1x288xf32>) -> tensor<1x1x288xf32>
Cost: 300

Operation name: stablehlo.dot_general
cache miss
3876455518
%2691 = stablehlo.dot_general %1821, %864, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288xf32>) -> tensor<288x288xf32>
Cost: 29367

Operation name: stablehlo.reshape
cache miss
552616300
%2692 = stablehlo.reshape %2691 : (tensor<288x288xf32>) -> tensor<1x288x288xf32>
Cost: 37632

Operation name: stablehlo.multiply
cache miss
2511610444
%2693 = stablehlo.multiply %1819, %868 : tensor<288xf32>
Cost: 516

Operation name: stablehlo.reshape
cache miss
2960881163
%2694 = stablehlo.reshape %2693 : (tensor<288xf32>) -> tensor<1x288xf32>
Cost: 601

Operation name: stablehlo.dot_general
cache miss
1743356395
%2695 = stablehlo.dot_general %1807, %881, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<768xf32>, tensor<288xf32>) -> tensor<768x288xf32>
Cost: 66264

Operation name: stablehlo.reshape
cache miss
1628429499
%2696 = stablehlo.reshape %2695 : (tensor<768x288xf32>) -> tensor<1x768x288xf32>
Cost: 63621

Operation name: stablehlo.dot_general
cache miss
9312040
%2697 = stablehlo.dot_general %1794, %895, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<768xf32>) -> tensor<288x768xf32>
Cost: 72141

Operation name: stablehlo.reshape
cache miss
3198799925
%2698 = stablehlo.reshape %2697 : (tensor<288x768xf32>) -> tensor<1x288x768xf32>
Cost: 57307

Operation name: stablehlo.multiply
cache miss
2511610444
%2699 = stablehlo.multiply %1792, %899 : tensor<288xf32>
Cost: 409

Operation name: stablehlo.reshape
cache miss
2960881163
%2700 = stablehlo.reshape %2699 : (tensor<288xf32>) -> tensor<1x288xf32>
Cost: 414

Operation name: stablehlo.dot_general
cache miss
3876455518
%2701 = stablehlo.dot_general %1780, %912, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288xf32>) -> tensor<288x288xf32>
Cost: 19845

Operation name: stablehlo.reshape
cache miss
552616300
%2702 = stablehlo.reshape %2701 : (tensor<288x288xf32>) -> tensor<1x288x288xf32>
Cost: 20940

Operation name: stablehlo.reshape
cache miss
1853321842
%2703 = stablehlo.reshape %1772 : (tensor<1x288xf32>) -> tensor<1x1x288xf32>
Cost: 324

Operation name: stablehlo.dot_general
cache miss
3876455518
%2704 = stablehlo.dot_general %1609, %1089, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288xf32>) -> tensor<288x288xf32>
Cost: 21483

Operation name: stablehlo.reshape
cache miss
552616300
%2705 = stablehlo.reshape %2704 : (tensor<288x288xf32>) -> tensor<1x288x288xf32>
Cost: 25854

Operation name: stablehlo.multiply
cache miss
2511610444
%2706 = stablehlo.multiply %1607, %1093 : tensor<288xf32>
Cost: 444

Operation name: stablehlo.reshape
cache miss
2960881163
%2707 = stablehlo.reshape %2706 : (tensor<288xf32>) -> tensor<1x288xf32>
Cost: 268

Operation name: stablehlo.dot_general
cache miss
1743356395
%2708 = stablehlo.dot_general %1595, %1106, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<768xf32>, tensor<288xf32>) -> tensor<768x288xf32>
Cost: 49552

Operation name: stablehlo.reshape
cache miss
1628429499
%2709 = stablehlo.reshape %2708 : (tensor<768x288xf32>) -> tensor<1x768x288xf32>
Cost: 56410

Operation name: stablehlo.dot_general
cache miss
9312040
%2710 = stablehlo.dot_general %1582, %1120, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<768xf32>) -> tensor<288x768xf32>
Cost: 69218

Operation name: stablehlo.reshape
cache miss
3198799925
%2711 = stablehlo.reshape %2710 : (tensor<288x768xf32>) -> tensor<1x288x768xf32>
Cost: 39944

Operation name: stablehlo.multiply
cache miss
2511610444
%2712 = stablehlo.multiply %1580, %1124 : tensor<288xf32>
Cost: 433

Operation name: stablehlo.reshape
cache miss
2960881163
%2713 = stablehlo.reshape %2712 : (tensor<288xf32>) -> tensor<1x288xf32>
Cost: 605

Operation name: stablehlo.concatenate
cache miss
2777730774
%2714 = stablehlo.concatenate %2648, %2661, %2674, %2687, %2700, %2713, dim = 0 : (tensor<1x288xf32>, tensor<1x288xf32>, tensor<1x288xf32>, tensor<1x288xf32>, tensor<1x288xf32>, tensor<1x288xf32>) -> tensor<6x288xf32>
Cost: 1436

Operation name: stablehlo.dot_general
cache miss
3876455518
%2715 = stablehlo.dot_general %1568, %1137, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288xf32>) -> tensor<288x288xf32>
Cost: 33806

Operation name: stablehlo.reshape
cache miss
552616300
%2716 = stablehlo.reshape %2715 : (tensor<288x288xf32>) -> tensor<1x288x288xf32>
Cost: 34342

Operation name: stablehlo.concatenate
cache miss
1946357965
%2717 = stablehlo.concatenate %2650, %2663, %2676, %2689, %2702, %2716, dim = 0 : (tensor<1x288x288xf32>, tensor<1x288x288xf32>, tensor<1x288x288xf32>, tensor<1x288x288xf32>, tensor<1x288x288xf32>, tensor<1x288x288xf32>) -> tensor<6x288x288xf32>
Cost: 160843

Operation name: stablehlo.reshape
cache miss
1853321842
%2718 = stablehlo.reshape %1560 : (tensor<1x288xf32>) -> tensor<1x1x288xf32>
Cost: 380

Operation name: stablehlo.concatenate
cache miss
1648206339
%2719 = stablehlo.concatenate %2651, %2664, %2677, %2690, %2703, %2718, dim = 0 : (tensor<1x1x288xf32>, tensor<1x1x288xf32>, tensor<1x1x288xf32>, tensor<1x1x288xf32>, tensor<1x1x288xf32>, tensor<1x1x288xf32>) -> tensor<6x1x288xf32>
Cost: 469

Operation name: stablehlo.dot_general
cache miss
3876455518
%2720 = stablehlo.dot_general %1397, %1314, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<288xf32>) -> tensor<288x288xf32>
Cost: 59896

Operation name: stablehlo.reshape
cache miss
552616300
%2721 = stablehlo.reshape %2720 : (tensor<288x288xf32>) -> tensor<1x288x288xf32>
Cost: 40905

Operation name: stablehlo.concatenate
cache miss
1946357965
%2722 = stablehlo.concatenate %2653, %2666, %2679, %2692, %2705, %2721, dim = 0 : (tensor<1x288x288xf32>, tensor<1x288x288xf32>, tensor<1x288x288xf32>, tensor<1x288x288xf32>, tensor<1x288x288xf32>,Jax2 rev (Array([-6.86408228e-08,  4.21906954e-08,  1.17837608e-07,  3.79193438e-07,
        1.87490897e-07,  7.23556695e-07,  9.79631523e-08, -3.31773066e-07,
        4.02368215e-07,  1.08053335e-07,  4.93508821e-07, -9.26528276e-09,
        1.70167880e-08, -6.06286108e-07, -5.95512006e-07, -1.93254607e-07,
        4.33923589e-07, -1.08318190e-08, -7.33497387e-08,  3.32936082e-07,
       -4.18106119e-08, -7.68011432e-07,  7.67988183e-07,  1.37110973e-07,
       -7.53293477e-07, -5.19659011e-07, -5.47181457e-07, -4.74780194e-07,
       -1.09416681e-06, -1.77187204e-07,  1.16568863e-07,  4.33848328e-07,
       -2.21843933e-07,  2.23926861e-07, -6.71994258e-07,  4.99407633e-07,
       -8.38962748e-08,  5.74726812e-07,  2.11452402e-07,  4.06514822e-08,
        1.64278333e-07,  1.50903460e-07, -5.40260503e-09,  8.70453079e-08,
        1.61527964e-07, -2.25055473e-07, -2.47880685e-08,  1.20487258e-07,
        6.23215158e-07,  9.19801195e-08,  2.56129454e-07,  5.02109970e-07,
        6.70257805e-08,  1.04174653e-07, -4.15205648e-08,  1.87939335e-07,
        4.38711652e-07, -2.48181465e-07,  2.32253015e-07, -4.64785302e-07,
       -3.03568100e-07, -5.58818328e-07,  9.50947552e-08,  1.13133595e-07,
       -2.17893681e-09, -2.20923795e-07,  3.02371745e-07,  1.72557634e-06,
        2.98470553e-07,  1.47123940e-08, -8.53288782e-08, -1.19442205e-07,
        1.61918749e-07,  6.79746535e-08,  1.26499017e-07, -7.60650892e-07,
        7.08876655e-08, -8.57948578e-07,  5.60488786e-07,  1.31472234e-07,
       -1.35581502e-07, -1.53091889e-07, -5.43999334e-09,  1.31034241e-07,
       -3.84336658e-07, -3.06770630e-07, -1.81460109e-07, -8.59075211e-09,
        6.71974334e-08, -1.33693902e-07,  2.88377777e-07,  5.65853156e-07,
       -3.77868439e-08,  1.14585566e-06, -1.92390573e-07, -1.84777321e-07,
        2.57143256e-07, -6.31657073e-08, -9.05696425e-08,  1.62616800e-07,
        5.11932058e-07,  8.54942130e-07,  1.81083990e-07, -3.88105889e-07,
        1.09523391e-07, -2.86763168e-07,  1.44920114e-06,  6.60731274e-08,
       -3.52259462e-07,  3.19866302e-07, -2.07359321e-07,  4.32996146e-08,
       -3.83730054e-07,  2.04251918e-07, -3.16490798e-07, -3.74791455e-07,
        9.46253181e-07, -1.30327553e-08,  2.92616619e-07, -2.48266133e-07,
        1.02249551e-08,  1.02647334e-06,  1.84591869e-07,  3.56919005e-09,
        1.57320663e-08,  6.85378666e-07, -4.02478321e-07,  6.18712264e-08,
        2.58430987e-07, -7.49129924e-07,  7.20021603e-07, -5.05634432e-08,
        5.70314675e-08,  4.01292425e-07, -5.21791890e-07,  5.30305009e-08,
       -4.95378345e-07,  7.72219977e-08, -3.37234383e-07, -5.28767679e-08,
        1.08730944e-07,  1.31315474e-07,  3.65712509e-07, -8.08554262e-07,
        1.91631997e-07, -8.42335282e-07,  4.83950259e-07,  4.59258843e-07,
       -6.15280612e-07, -1.22344500e-06,  2.34015872e-07,  7.07483778e-08,
        5.39801476e-07,  1.39701353e-06,  3.04554675e-07,  2.52107188e-08,
        4.83204232e-08, -4.75948809e-07, -2.52900833e-07,  7.46215250e-08,
       -2.31107133e-09,  1.56125708e-07,  2.60826340e-07,  2.72517777e-08,
       -7.04579008e-07,  1.57311391e-07, -1.15654950e-06, -9.39920085e-07,
       -4.97331825e-07,  6.46740546e-08, -2.65434323e-07,  6.43622897e-08,
        9.37891400e-08,  2.52144559e-07,  9.06175970e-08,  1.44713468e-07,
        1.31762633e-07,  4.81381548e-08,  9.00499764e-09,  1.27348398e-08,
        4.32467040e-08, -1.25784441e-08,  7.66482486e-08,  2.04513384e-07,
       -3.94884552e-07, -1.94403327e-07, -2.98042835e-07,  2.18630575e-07,
        1.69301885e-07,  1.49153067e-07, -3.50019917e-07,  8.77085540e-07,
        4.23077239e-07, -1.09007931e-06, -1.36423535e-06,  4.79422830e-08,
        9.21597064e-07, -1.03429571e-07, -5.79991195e-07, -3.09759798e-07,
        3.58044446e-07, -6.62943904e-08, -1.58884603e-07, -3.58757944e-07,
       -1.61528453e-06, -7.06735534e-07, -7.84457868e-08,  2.55476706e-08,
       -7.91445302e-07,  3.60562851e-08, -4.64593342e-08, -7.16763395e-07,
        4.69918007e-07,  3.78396301e-08,  1.09304949e-07, -5.59533191e-07,
       -3.28639032e-07, -8.60765567e-07,  4.96651126e-07,  6.62748221e-07,
        2.11249969e-07,  1.11245996e-07,  1.35924793e-07, -1.73352049e-07,
       -4.84629993e-07, -1.07275946e-06,  3.53337327e-07,  4.73680934e-08,
       -6.40715143e-07, -3.24083032e-07,  6.44099956e-08, -2.05601381e-07,
        1.65275594e-07,  4.72725148e-08, -6.40937060e-07,  8.60434994e-08,
       -3.91629811e-07,  1.39183157e-07, -9.12502500e-08,  2.04173332e-07,
        2.10377834e-07,  2.19034277e-07, -1.68734374e-07, -1.08304384e-07,
        6.66635287e-07, -1.13561964e-06, -4.51580831e-07, -4.66988524e-07,
        9.00098598e-07,  9.01734012e-08, -2.67860656e-07,  2.83206305e-07,
        1.05244041e-07,  1.10610802e-06, -2.04430139e-09, -1.12441159e-08,
        1.13123171e-07,  1.05943030e-07,  8.35827962e-07, -5.38025688e-07,
       -3.97798289e-07, -1.12537066e-07, -3.02546397e-07,  1.52792026e-07,
       -6.16381897e-07, -3.72609634e-07, -3.07205454e-07,  8.12457301e-07,
       -5.32849413e-07, -1.08672182e-07, -1.15581528e-07, -9.30150037e-08,
       -1.85540880e-08, -1.37492336e-07,  5.40914868e-07,  6.27180867e-08,
       -4.74730456e-07, -1.04523406e-06, -1.33274710e-07,  6.83533514e-08,
        7.06085501e-08,  3.79577614e-07,  1.53265859e-07,  5.01433028e-08,
       -1.67806196e-08, -2.22508689e-07,  1.08804400e-07, -7.36162974e-07],      dtype=float32), {'rms_att_weight': Array([[-3.0143696e-07, -1.2328665e-07, -1.7465823e-08, ...,
        -1.4061717e-07, -1.5792592e-07, -5.4863358e-07],
       [-1.6369802e-06, -8.6754056e-07, -1.8289523e-06, ...,
        -1.0596178e-06, -1.2871564e-06, -1.0761667e-06],
       [-6.9325080e-07, -4.5352172e-07, -8.4269311e-07, ...,
        -8.1122982e-07, -1.5815522e-07,  1.1964835e-07],
       [ 5.1793427e-07, -5.5533786e-07, -2.4150373e-07, ...,
         9.4964332e-07, -1.0185217e-07, -1.0075292e-07],
       [-1.1727195e-06, -1.6750417e-06, -1.1069870e-06, ...,
        -2.1841936e-06, -2.1553085e-06, -1.5655169e-06],
       [-2.2943155e-07, -3.5378143e-08, -6.5307557e-07, ...,
        -7.1474807e-07, -5.4531517e-07,  5.6257981e-08]], dtype=float32), 'rms_ffn_weight': Array([[ 2.95414764e-04,  2.42857262e-04,  2.86784052e-04, ...,
         2.67516210e-04,  2.00823299e-04,  1.17813550e-04],
       [ 7.73553184e-05, -1.25005434e-04, -1.21706857e-04, ...,
        -1.22196643e-05, -3.80375750e-05, -1.78491362e-04],
       [-4.37740673e-04, -3.44874308e-04, -4.67159349e-04, ...,
        -4.63766919e-04, -2.63263530e-04, -3.33167176e-04],
       [ 1.32752233e-04,  9.45400825e-05,  1.38524949e-04, ...,
         1.74323704e-05,  8.29400742e-05, -2.30727273e-05],
       [-9.23259358e-05,  3.04452551e-05, -7.10105960e-05, ...,
        -1.10016546e-04, -1.20125704e-04, -1.12619549e-04],
       [ 9.48957895e-05,  2.31729471e-04,  2.89788470e-04, ...,
         3.06937902e-04,  2.39924557e-04,  1.72888089e-04]],      dtype=float32), 'rms_final_weight': Array([0.3122053 , 0.38303918, 0.8200591 , 0.61894673, 0.12635426,
       0.24344024, 0.8361446 , 0.06651747, 0.896955  , 0.9979907 ,
       0.6266402 , 0.07218794, 0.23386627, 0.44796824, 0.9262833 ,
       0.15019089, 0.7380873 , 0.20100224, 0.183272  , 0.9952421 ,
       0.3053508 , 0.46485856, 0.8713649 , 0.7386524 , 0.45421234,
       0.9742905 , 0.80178434, 0.8113896 , 0.18982358, 0.63831717,
       0.1872204 , 0.10730092, 0.3926591 , 0.6619144 , 0.90564454,
       0.56765217, 0.0778048 , 0.6635671 , 0.4967288 , 0.93859744,
       0.12237269, 0.15338296, 0.58576405, 0.45069534, 0.0977786 ,
       0.00641902, 0.69381803, 0.4580174 , 0.1595322 , 0.48593563,
       0.28240606, 0.5517295 , 0.21862239, 0.91464216, 0.5406124 ,
       0.00819704, 0.30426028, 0.6022607 , 0.96227723, 0.9489469 ,
       0.186835  , 0.34483457, 0.18725023, 0.4927935 , 0.5340233 ,
       0.58169085, 0.07708689, 0.7007939 , 0.55612355, 0.38934347,
       0.9926272 , 0.3682517 , 0.5676963 , 0.17936414, 0.69174665,
       0.7956288 , 0.9634303 , 0.81479746, 0.06194434, 0.2468073 ,
       0.8020321 , 0.38030377, 0.35621735, 0.27904525, 0.25768125,
       0.06319077, 0.99190855, 0.05523045, 0.2980475 , 0.25185248,
       0.38000903, 0.85173017, 0.9537092 , 0.9449313 , 0.38227183,
       0.48736802, 0.89497286, 0.95300776, 0.71275187, 0.6176107 ,
       0.839199  , 0.7585798 , 0.68531674, 0.7475704 , 0.63421494,
       1.0108242 , 0.3633397 , 0.5698798 , 0.5270483 , 0.96704113,
       0.89812386, 0.76888496, 0.8005023 , 0.4232546 , 0.8666324 ,
       0.21481518, 0.91262794, 0.34706104, 0.25790077, 0.60310835,
       0.3188328 , 0.2731171 , 0.9384355 , 0.48072326, 0.98125595,
       0.7326272 , 0.9047383 , 0.5255142 , 0.86498344, 0.96362615,
       0.12110879, 0.7292903 , 0.28708053, 0.09494646, 0.8906763 ,
       0.35737857, 0.96169263, 0.02848402, 0.36005193, 0.78592503,
       0.47747067, 0.14686225, 0.86080545, 0.04425434, 0.90892106,
       0.4249482 , 0.0747368 , 0.7905809 , 0.30792928, 0.46037206,
       0.24123558, 0.42178747, 0.06162682, 0.78396076, 0.34331495,
       0.9420474 , 0.3242471 , 0.8216802 , 0.36618954, 0.3667223 ,
       0.4056759 , 0.7121504 , 0.73845303, 0.96334136, 0.8625689 ,
       0.64736134, 0.9211619 , 0.84406066, 0.34351304, 0.3043062 ,
       0.59214056, 0.49616915, 0.7642584 , 0.7106041 , 0.14271905,
       0.4835507 , 0.7055461 , 0.3586383 , 0.22076729, 0.31910723,
       0.5656433 , 0.71893984, 0.6566055 , 0.47881052, 0.4351634 ,
       0.5286704 , 0.68011975, 0.57306236, 0.87672895, 0.9661281 ,
       0.2449497 , 0.9602345 , 0.22372767, 0.48450968, 0.95286757,
       0.39947754, 0.5368166 , 0.10909911, 0.20554121, 0.611804  ,
       0.44083962, 0.91954535, 0.8771144 , 0.7255645 , 0.3669443 ,
       0.4372653 , 0.65592253, 0.49335885, 0.75596845, 0.00718008,
       0.8201033 , 0.18157919, 0.02211665, 0.5196222 , 0.10101545,
       0.5055483 , 0.57251376, 0.26995882, 0.08761868, 0.99912083,
       0.35904372, 0.7904374 , 0.9225819 , 0.61186445, 0.3830875 ,
       0.07389324, 0.26608193, 0.45918933, 0.20400742, 0.77979153,
       0.6035741 , 0.10805453, 0.6190611 , 0.8768369 , 0.31287163,
       0.80589724, 0.9268372 , 0.90927136, 0.08478162, 0.684047  ,
       0.48910892, 0.22513983, 0.97408646, 0.41005015, 0.22878326,
       0.78730536, 0.59618664, 0.41042367, 0.5120551 , 0.06035588,
       0.36444396, 0.70929223, 0.16675206, 0.75272256, 0.881576  ,
       0.30691344, 0.14660256, 0.26519263, 0.6609811 , 0.55440545,
       0.12501441, 0.3364693 , 0.09852853, 0.8370226 , 0.422585  ,
       0.23525533, 0.77372366, 0.37565795, 0.46482727, 0.15144964,
       0.7651885 , 0.12105324, 0.6146893 , 0.3594155 , 0.8886243 ,
       0.49813882, 0.79519606, 0.8493521 , 0.78389865, 1.004332  ,
       0.09718259, 0.06170881, 0.16077426, 0.9070744 , 0.41441402,
       0.6486587 , 0.717869  , 0.19379644], dtype=float32), 'w1': Array([[[ 3.64633615e-06,  8.72685121e-07,  3.56683722e-06, ...,
          4.34024429e-07,  3.36852531e-06,  1.65969777e-06],
        [ 2.63794050e-06,  6.31343710e-07,  2.58042724e-06, ...,
          3.13994803e-07,  2.43695831e-06,  1.20070774e-06],
        [ 5.99563145e-06,  1.43494674e-06,  5.86491251e-06, ...,
          7.13661734e-07,  5.53883001e-06,  2.72902321e-06],
        ...,
        [-2.69627344e-06, -6.45304681e-07, -2.63748825e-06, ...,
         -3.20938199e-07, -2.49084701e-06, -1.22725908e-06],
        [ 4.82197811e-06,  1.15405385e-06,  4.71684734e-06, ...,
          5.73961415e-07,  4.45459636e-06,  2.19481308e-06],
        [ 2.64609454e-08,  6.33295238e-09,  2.58840345e-08, ...,
          3.14965387e-09,  2.44449119e-08,  1.20441923e-08]],

       [[ 6.17717888e-07,  1.09563098e-06,  1.04882361e-06, ...,
          8.52585856e-07,  1.04958804e-06,  1.77660084e-07],
        [ 2.71790918e-06,  4.82068845e-06,  4.61473974e-06, ...,
          3.75130958e-06,  4.61810305e-06,  7.81690176e-07],
        [ 4.20589959e-06,  7.45990064e-06,  7.14120006e-06, ...,
          5.80506230e-06,  7.14640510e-06,  1.20964694e-06],
        ...,
        [-1.76960100e-06, -3.13869759e-06, -3.00460670e-06, ...,
         -2.44243665e-06, -3.00679653e-06, -5.08949938e-07],
        [ 1.09699340e-06,  1.94571021e-06,  1.86258580e-06, ...,
          1.51409097e-06,  1.86394334e-06,  3.15503172e-07],
        [-8.96390975e-06, -1.58990661e-05, -1.52198290e-05, ...,
         -1.23721575e-05, -1.52309212e-05, -2.57808483e-06]],

       [[-1.93336447e-07, -6.80360870e-08, -4.80970357e-08, ...,
         -8.59555911e-08, -4.33284733e-08, -1.73667392e-08],
        [ 1.00252350e-06,  3.52793137e-07,  2.49401523e-07, ...,
          4.45712629e-07,  2.24674707e-07,  9.00531916e-08],
        [ 8.26453572e-08,  2.90833224e-08,  2.05599946e-08, ...,
          3.67433586e-08,  1.85215825e-08,  7.42374429e-09],
        ...,
        [-4.22935273e-06, -1.48833089e-06, -1.05215202e-06, ...,
         -1.88033107e-06, -9.47836782e-07, -3.79908016e-07],
        [ 1.25773568e-06,  4.42603607e-07,  3.12891615e-07, ...,
          5.59177579e-07,  2.81870086e-07,  1.12978007e-07],
        [-1.98665907e-06, -6.99115446e-07, -4.94228630e-07, ...,
         -8.83250152e-07, -4.45228522e-07, -1.78454655e-07]],

       [[ 1.61405887e-06,  3.88676108e-06,  1.28759098e-06, ...,
          3.81181007e-06,  2.52378095e-06,  4.18733225e-06],
        [ 1.04036246e-06,  2.50526227e-06,  8.29933413e-07, ...,
          2.45695151e-06,  1.62673564e-06,  2.69899897e-06],
        [ 2.18161540e-06,  5.25347514e-06,  1.74035063e-06, ...,
          5.15216880e-06,  3.41122609e-06,  5.65973687e-06],
        ...,
        [-4.35050924e-06, -1.04763167e-05, -3.47055288e-06, ...,
         -1.02742943e-05, -6.80256016e-06, -1.12864700e-05],
        [ 9.54388035e-09,  2.29823005e-08,  7.61348584e-09, ...,
          2.25391155e-08,  1.49230388e-08,  2.47595651e-08],
        [ 2.92645313e-06,  7.04709419e-06,  2.33453375e-06, ...,
          6.91120022e-06,  4.57587203e-06,  7.59205977e-06]],

       [[-1.22685060e-06, -3.18147659e-06, -9.29465898e-07, ...,
         -6.83750841e-07, -3.66203790e-06, -1.67669032e-06],
        [ 3.25078145e-06,  8.42994632e-06,  2.46280206e-06, ...,
          1.81173198e-06,  9.70328801e-06,  4.44272018e-06],
        [ 9.40266034e-07,  2.43830368e-06,  7.12348481e-07, ...,
          5.24030952e-07,  2.80660879e-06,  1.28502597e-06],
        ...,
        [ 2.98411942e-06,  7.73843658e-06,  2.26077805e-06, ...,
          1.66311543e-06,  8.90732554e-06,  4.07828293e-06],
        [ 4.92612116e-07,  1.27744477e-06,  3.73204472e-07, ...,
          2.74543567e-07,  1.47040259e-06,  6.73234354e-07],
        [-2.64967912e-06, -6.87116426e-06, -2.00740510e-06, ...,
         -1.47672449e-06, -7.90905142e-06, -3.62121614e-06]],

       [[ 1.07997903e-05,  1.15772727e-05,  1.09909579e-05, ...,
          2.53739813e-06,  1.04543878e-05,  5.79772086e-06],
        [-1.01716732e-05, -1.09039374e-05, -1.03517223e-05, ...,
         -2.38982284e-06, -9.84635972e-06, -5.46052479e-06],
        [-1.57906391e-06, -1.69274165e-06, -1.60701495e-06, ...,
         -3.70999231e-07, -1.52856171e-06, -8.47699027e-07],
        ...,
        [ 1.05452659e-06,  1.13044246e-06,  1.07319272e-06, ...,
          2.47759772e-07,  1.02080037e-06,  5.66108270e-07],
        [ 1.41518228e-06,  1.51706206e-06,  1.44023250e-06, ...,
          3.32495432e-07,  1.36992151e-06,  7.59721445e-07],
        [-7.34297373e-06, -7.87159843e-06, -7.47295189e-06, ...,
         -1.72522311e-06, -7.10812810e-06, -3.94197605e-06]]],      dtype=float32), 'w2': Array([[[-5.74465557e-05, -5.38376735e-05, -5.58964130e-05, ...,
         -5.03214651e-05, -5.12089464e-05, -5.16573673e-05],
        [-5.19956629e-05, -4.87292164e-05, -5.05926073e-05, ...,
         -4.55466470e-05, -4.63499164e-05, -4.67557875e-05],
        [-7.43175624e-05, -6.96488132e-05, -7.23121630e-05, ...,
         -6.50999646e-05, -6.62480743e-05, -6.68281937e-05],
        ...,
        [ 1.85051264e-04,  1.73426059e-04,  1.80057817e-04, ...,
          1.62099386e-04,  1.64958197e-04,  1.66402679e-04],
        [-1.23381105e-05, -1.15630110e-05, -1.20051773e-05, ...,
         -1.08078157e-05, -1.09984248e-05, -1.10947340e-05],
        [-1.11180387e-04, -1.04195860e-04, -1.08180284e-04, ...,
         -9.73907008e-05, -9.91082998e-05, -9.99761614e-05]],

       [[-5.27832017e-05, -5.08860903e-05, -5.84037953e-05, ...,
         -5.79213447e-05, -5.05039752e-05, -5.37620872e-05],
        [-4.73958862e-05, -4.56924026e-05, -5.24428106e-05, ...,
         -5.20096037e-05, -4.53492867e-05, -4.82748583e-05],
        [-6.78453362e-05, -6.54068645e-05, -7.50698091e-05, ...,
         -7.44496865e-05, -6.49157082e-05, -6.91035530e-05],
        ...,
        [ 1.69247956e-04,  1.63164921e-04,  1.87270241e-04, ...,
          1.85723271e-04,  1.61939679e-04,  1.72386732e-04],
        [-1.13198012e-05, -1.09129496e-05, -1.25251845e-05, ...,
         -1.24217186e-05, -1.08310014e-05, -1.15297307e-05],
        [-1.01789803e-04, -9.81313206e-05, -1.12628841e-04, ...,
         -1.11698457e-04, -9.73944261e-05, -1.03677536e-04]],

       [[-6.29638671e-05, -5.50060577e-05, -5.62734313e-05, ...,
         -5.55980114e-05, -5.51145604e-05, -5.87623581e-05],
        [-5.67860588e-05, -4.96090433e-05, -5.07520672e-05, ...,
         -5.01429167e-05, -4.97069013e-05, -5.29967911e-05],
        [-8.12159269e-05, -7.09512897e-05, -7.25860591e-05, ...,
         -7.17148432e-05, -7.10912500e-05, -7.57964735e-05],
        ...,
        [ 2.02039228e-04,  1.76504109e-04,  1.80570874e-04, ...,
          1.78403585e-04,  1.76852278e-04,  1.88557373e-04],
        [-1.37589886e-05, -1.20200320e-05, -1.22969814e-05, ...,
         -1.21493867e-05, -1.20437426e-05, -1.28408665e-05],
        [-1.21824298e-04, -1.06427295e-04, -1.08879452e-04, ...,
         -1.07572625e-04, -1.06637235e-04, -1.13695096e-04]],

       [[-5.23333874e-05, -4.88984006e-05, -5.27251104e-05, ...,
         -5.09480342e-05, -4.80538692e-05, -4.87045327e-05],
        [-4.72214706e-05, -4.41220109e-05, -4.75749293e-05, ...,
         -4.59714392e-05, -4.33599707e-05, -4.39470787e-05],
        [-6.74981347e-05, -6.30677823e-05, -6.80033700e-05, ...,
         -6.57113487e-05, -6.19785278e-05, -6.28177368e-05],
        ...,
        [ 1.67982813e-04,  1.56956987e-04,  1.69240186e-04, ...,
          1.63536024e-04,  1.54246154e-04,  1.56334689e-04],
        [-1.14248487e-05, -1.06749594e-05, -1.15103649e-05, ...,
         -1.11224135e-05, -1.04905912e-05, -1.06326370e-05],
        [-1.01212929e-04, -9.45696520e-05, -1.01970523e-04, ...,
         -9.85336592e-05, -9.29363232e-05, -9.41947146e-05]],

       [[-5.49333417e-05, -4.95117929e-05, -5.46458905e-05, ...,
         -5.63307367e-05, -6.19458006e-05, -6.00997482e-05],
        [-4.96023931e-05, -4.47069724e-05, -4.93428379e-05, ...,
         -5.08641824e-05, -5.59343389e-05, -5.42674316e-05],
        [-7.08599182e-05, -6.38665224e-05, -7.04891281e-05, ...,
         -7.26624567e-05, -7.99054687e-05, -7.75241933e-05],
        ...,
        [ 1.76331247e-04,  1.58928538e-04,  1.75408553e-04, ...,
          1.80816758e-04,  1.98840644e-04,  1.92914958e-04],
        [-1.19392580e-05, -1.07609339e-05, -1.18767830e-05, ...,
         -1.22429692e-05, -1.34633528e-05, -1.30621293e-05],
        [-1.06232139e-04, -9.57477459e-05, -1.05676263e-04, ...,
         -1.08934481e-04, -1.19793105e-04, -1.16223127e-04]],

       [[-5.45921284e-05, -5.96108148e-05, -5.57108215e-05, ...,
         -5.79298212e-05, -5.71885976e-05, -5.48706012e-05],
        [-4.93639964e-05, -5.39020584e-05, -5.03755582e-05, ...,
         -5.23820490e-05, -5.17118133e-05, -4.96158027e-05],
        [-7.05126222e-05, -7.69948892e-05, -7.19575546e-05, ...,
         -7.48236780e-05, -7.38662930e-05, -7.08723092e-05],
        ...,
        [ 1.75282272e-04,  1.91396073e-04,  1.78874136e-04, ...,
          1.85998812e-04,  1.83618919e-04,  1.76176385e-04],
        [-1.19150491e-05, -1.30104063e-05, -1.21592111e-05, ...,
         -1.26435207e-05, -1.24817443e-05, -1.19758279e-05],
        [-1.05577041e-04, -1.15282797e-04, -1.07740510e-04, ...,
         -1.12031885e-04, -1.10598419e-04, -1.06115585e-04]]],      dtype=float32), 'w3': Array([[[ 3.5490775e-06,  8.4940797e-07,  3.4716991e-06, ...,
          4.2244770e-07,  3.2786768e-06,  1.6154287e-06],
        [ 2.7477595e-06,  6.5762691e-07,  2.6878517e-06, ...,
          3.2706660e-07,  2.5384104e-06,  1.2506939e-06],
        [ 5.3019940e-06,  1.2689371e-06,  5.1863981e-06, ...,
          6.3109786e-07,  4.8980405e-06,  2.4133012e-06],
        ...,
        [-2.5464431e-06, -6.0944546e-07, -2.4909245e-06, ...,
         -3.0310386e-07, -2.3524321e-06, -1.1590610e-06],
        [ 4.5189145e-06,  1.0815211e-06,  4.4203916e-06, ...,
          5.3788767e-07,  4.1746230e-06,  2.0568682e-06],
        [ 2.6770024e-08,  6.4069248e-09,  2.6186374e-08, ...,
          3.1864436e-09,  2.4730442e-08,  1.2184874e-08]],

       [[ 6.6705354e-07,  1.1831364e-06,  1.1325907e-06, ...,
          9.2067989e-07,  1.1334162e-06,  1.9184938e-07],
        [ 2.3132554e-06,  4.1029639e-06,  3.9276779e-06, ...,
          3.1927987e-06,  3.9305405e-06,  6.6530885e-07],
        [ 3.9754595e-06,  7.0511746e-06,  6.7499354e-06, ...,
          5.4870043e-06,  6.7548553e-06,  1.1433707e-06],
        ...,
        [-1.8314257e-06, -3.2483547e-06, -3.1095790e-06, ...,
         -2.5277684e-06, -3.1118454e-06, -5.2673118e-07],
        [ 1.1988182e-06,  2.1263140e-06,  2.0354742e-06, ...,
          1.6546313e-06,  2.0369575e-06,  3.4478867e-07],
        [-9.2805594e-06, -1.6460699e-05, -1.5757467e-05, ...,
         -1.2809203e-05, -1.5768952e-05, -2.6691553e-06]],

       [[-1.9254314e-07, -6.7756915e-08, -4.7899679e-08, ...,
         -8.5602892e-08, -4.3150685e-08, -1.7295479e-08],
        [ 9.8565999e-07,  3.4685877e-07,  2.4520634e-07, ...,
          4.3821527e-07,  2.2089544e-07,  8.8538393e-08],
        [ 8.2081016e-08,  2.8884729e-08,  2.0419604e-08, ...,
          3.6492459e-08,  1.8395109e-08,  7.3730515e-09],
        ...,
        [-4.2611964e-06, -1.4995367e-06, -1.0600738e-06, ...,
         -1.8944883e-06, -9.5497319e-07, -3.8276841e-07],
        [ 1.3054347e-06,  4.5938916e-07,  3.2475791e-07, ...,
          5.8038415e-07,  2.9255989e-07,  1.1726265e-07],
        [-1.9165298e-06, -6.7443659e-07, -4.7678228e-07, ...,
         -8.5207131e-07, -4.2951189e-07, -1.7215518e-07]],

       [[ 1.7120665e-06,  4.1227700e-06,  1.3657750e-06, ...,
          4.0432678e-06,  2.6770281e-06,  4.4415920e-06],
        [ 1.0612503e-06,  2.5555612e-06,  8.4659627e-07, ...,
          2.5062807e-06,  1.6593963e-06,  2.7531878e-06],
        [ 1.9702500e-06,  4.7444933e-06,  1.5717370e-06, ...,
          4.6530017e-06,  3.0807300e-06,  5.1113943e-06],
        ...,
        [-4.3448213e-06, -1.0462620e-05, -3.4660154e-06, ...,
         -1.0260861e-05, -6.7936667e-06, -1.1271714e-05],
        [ 9.7296962e-09,  2.3429756e-08,  7.7617175e-09, ...,
          2.2977945e-08,  1.5213585e-08,  2.5241626e-08],
        [ 3.0418371e-06,  7.3249462e-06,  2.4265794e-06, ...,
          7.1836944e-06,  4.7562889e-06,  7.8913981e-06]],

       [[-1.1891536e-06, -3.0837207e-06, -9.0090657e-07, ...,
         -6.6274151e-07, -3.5495161e-06, -1.6251713e-06],
        [ 3.3974245e-06,  8.8102224e-06,  2.5738996e-06, ...,
          1.8934595e-06,  1.0141005e-05,  4.6431319e-06],
        [ 9.6939925e-07,  2.5138522e-06,  7.3441993e-07, ...,
          5.4026754e-07,  2.8935690e-06,  1.3248413e-06],
        ...,
        [ 3.1900177e-06,  8.2723736e-06,  2.4167673e-06, ...,
          1.7778671e-06,  9.5219139e-06,  4.3596765e-06],
        [ 5.0431458e-07,  1.3077916e-06,  3.8207031e-07, ...,
          2.8106561e-07,  1.5053333e-06,  6.8922765e-07],
        [-2.6572388e-06, -6.8907680e-06, -2.0131324e-06, ...,
         -1.4809377e-06, -7.9316169e-06, -3.6315478e-06]],

       [[ 1.0838507e-05,  1.1618777e-05,  1.1030359e-05, ...,
          2.5464944e-06,  1.0491865e-05,  5.8185051e-06],
        [-9.9084527e-06, -1.0621768e-05, -1.0083842e-05, ...,
         -2.3279795e-06, -9.5915566e-06, -5.3192184e-06],
        [-1.5034987e-06, -1.6117364e-06, -1.5301122e-06, ...,
         -3.5324527e-07, -1.4554133e-06, -8.0713289e-07],
        ...,
        [ 9.9419424e-07,  1.0657668e-06,  1.0117925e-06, ...,
          2.3358479e-07,  9.6239762e-07,  5.3371969e-07],
        [ 1.3294299e-06,  1.4251364e-06,  1.3529623e-06, ...,
          3.1234802e-07,  1.2869117e-06,  7.1368646e-07],
        [-6.6477387e-06, -7.1263130e-06, -6.7654105e-06, ...,
         -1.5618784e-06, -6.4351284e-06, -3.5687483e-06]]], dtype=float32), 'wcls': Array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       ...,
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), 'wk': Array([[[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32), 'wo': Array([[[ 2.5211170e-07,  2.6817838e-07,  2.6122720e-07, ...,
          2.5612391e-07,  2.4704889e-07,  2.6379772e-07],
        [-9.1684535e-07, -9.7527436e-07, -9.4999530e-07, ...,
         -9.3143638e-07, -8.9843360e-07, -9.5934342e-07],
        [-7.3938693e-08, -7.8650679e-08, -7.6612054e-08, ...,
         -7.5115381e-08, -7.2453879e-08, -7.7365932e-08],
        ...,
        [ 1.6139436e-06,  1.7167976e-06,  1.6722981e-06, ...,
          1.6396285e-06,  1.5815330e-06,  1.6887539e-06],
        [ 7.8728803e-08,  8.3746059e-08,  8.1575365e-08, ...,
          7.9981724e-08,  7.7147803e-08,  8.2378079e-08],
        [-1.7059546e-06, -1.8146721e-06, -1.7676358e-06, ...,
         -1.7331038e-06, -1.6716962e-06, -1.7850297e-06]],

       [[-7.4160255e-07, -6.9608097e-07, -7.4464560e-07, ...,
         -7.3699795e-07, -7.0854247e-07, -7.5860277e-07],
        [-6.7127183e-07, -6.3006735e-07, -6.7402630e-07, ...,
         -6.6710396e-07, -6.4134707e-07, -6.8665986e-07],
        [-9.5938697e-07, -9.0049713e-07, -9.6332360e-07, ...,
         -9.5343012e-07, -9.1661821e-07, -9.8137957e-07],
        ...,
        [ 2.3889731e-06,  2.2423314e-06,  2.3987759e-06, ...,
          2.3741402e-06,  2.2824747e-06,  2.4437372e-06],
        [-1.5926791e-07, -1.4949161e-07, -1.5992144e-07, ...,
         -1.5827902e-07, -1.5216789e-07, -1.6291891e-07],
        [-1.4353344e-06, -1.3472296e-06, -1.4412241e-06, ...,
         -1.4264225e-06, -1.3713483e-06, -1.4682374e-06]],

       [[-7.6595751e-07, -7.4974855e-07, -7.9625852e-07, ...,
         -7.5697875e-07, -7.5542829e-07, -7.7191658e-07],
        [-6.8777882e-07, -6.7322424e-07, -7.1498715e-07, ...,
         -6.7971649e-07, -6.7832428e-07, -6.9312966e-07],
        [-9.8451960e-07, -9.6368547e-07, -1.0234669e-06, ...,
         -9.7297880e-07, -9.7098587e-07, -9.9217903e-07],
        ...,
        [ 2.4560272e-06,  2.4040535e-06,  2.5531870e-06, ...,
          2.4272372e-06,  2.4222654e-06,  2.4751350e-06],
        [-1.6426739e-07, -1.6079120e-07, -1.7076576e-07, ...,
         -1.6234181e-07, -1.6200929e-07, -1.6554537e-07],
        [-1.4771156e-06, -1.4458573e-06, -1.5355498e-06, ...,
         -1.4598004e-06, -1.4568103e-06, -1.4886074e-06]],

       [[-7.5559569e-07, -7.3336469e-07, -8.4079471e-07, ...,
         -7.8980622e-07, -8.3589470e-07, -7.9112533e-07],
        [-6.8145312e-07, -6.6140353e-07, -7.5829206e-07, ...,
         -7.1230676e-07, -7.5387283e-07, -7.1349643e-07],
        [-9.7462794e-07, -9.4595254e-07, -1.0845245e-06, ...,
         -1.0187554e-06, -1.0782039e-06, -1.0204569e-06],
        ...,
        [ 2.4245667e-06,  2.3532314e-06,  2.6979546e-06, ...,
          2.5343420e-06,  2.6822313e-06,  2.5385748e-06],
        [-1.6511213e-07, -1.6025422e-07, -1.8372974e-07, ...,
         -1.7258778e-07, -1.8265899e-07, -1.7287603e-07],
        [-1.4619472e-06, -1.4189339e-06, -1.6267926e-06, ...,
         -1.5281387e-06, -1.6173119e-06, -1.5306910e-06]],

       [[-7.1280272e-07, -7.6736785e-07, -7.6152338e-07, ...,
         -7.0458771e-07, -7.8551102e-07, -7.5898555e-07],
        [-6.4317248e-07, -6.9240741e-07, -6.8713382e-07, ...,
         -6.3575993e-07, -7.0877826e-07, -6.8484388e-07],
        [-9.1934436e-07, -9.8972032e-07, -9.8218231e-07, ...,
         -9.0874897e-07, -1.0131206e-06, -9.7890904e-07],
        ...,
        [ 2.2879751e-06,  2.4631199e-06,  2.4443602e-06, ...,
          2.2616064e-06,  2.5213565e-06,  2.4362141e-06],
        [-1.5560721e-07, -1.6751898e-07, -1.6624310e-07, ...,
         -1.5381386e-07, -1.7147968e-07, -1.6568907e-07],
        [-1.3785524e-06, -1.4840807e-06, -1.4727776e-06, ...,
         -1.3626646e-06, -1.5191694e-06, -1.4678693e-06]],

       [[-6.9852308e-07, -6.9982849e-07, -6.9935174e-07, ...,
         -7.2070242e-07, -7.0936545e-07, -7.6198722e-07],
        [-6.3073657e-07, -6.3191533e-07, -6.3148480e-07, ...,
         -6.5076358e-07, -6.4052682e-07, -6.8804201e-07],
        [-9.0104254e-07, -9.0272641e-07, -9.0211142e-07, ...,
         -9.2965223e-07, -9.1502841e-07, -9.8290661e-07],
        ...,
        [ 2.2422000e-06,  2.2463903e-06,  2.2448598e-06, ...,
          2.3133937e-06,  2.2770032e-06,  2.4459146e-06],
        [-1.5181728e-07, -1.5210099e-07, -1.5199737e-07, ...,
         -1.5663774e-07, -1.5417376e-07, -1.6561060e-07],
        [-1.3508316e-06, -1.3533560e-06, -1.3524340e-06, ...,
         -1.3937228e-06, -1.3717989e-06, -1.4735610e-06]]], dtype=float32), 'wq': Array([[[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32), 'wv': Array([[[ 8.33273450e-09,  3.17081650e-08,  1.31639965e-07, ...,
          3.71325157e-08,  1.14651563e-07,  1.41336827e-07],
        [-5.48908363e-10, -2.08873518e-09, -8.67161631e-09, ...,
         -2.44605736e-09, -7.55252660e-09, -9.31038446e-09],
        [-3.52132046e-10, -1.33995148e-09, -5.56295765e-09, ...,
         -1.56917856e-09, -4.84504659e-09, -5.97273608e-09],
        ...,
        [ 3.26836269e-09,  1.24369466e-08,  5.16333678e-08, ...,
          1.45645496e-08,  4.49699762e-08,  5.54367823e-08],
        [-3.82936260e-09, -1.45716932e-08, -6.04960064e-08, ...,
         -1.70644903e-08, -5.26888719e-08, -6.49522605e-08],
        [ 4.67006744e-09,  1.77707875e-08,  7.37774002e-08, ...,
          2.08108570e-08,  6.42562838e-08,  7.92119934e-08]],

       [[ 8.92418299e-08,  2.81410006e-08,  1.04813047e-07, ...,
          2.29284804e-08,  8.77959536e-08,  3.70077409e-08],
        [-8.53703384e-08, -2.69201870e-08, -1.00266043e-07, ...,
         -2.19337952e-08, -8.39871888e-08, -3.54022696e-08],
        [-1.01619402e-09, -3.20440730e-10, -1.19350296e-09, ...,
         -2.61085903e-10, -9.99729854e-10, -4.21406021e-10],
        ...,
        [ 1.55103237e-08,  4.89093566e-09,  1.82166175e-08, ...,
          3.98499411e-09,  1.52590296e-08,  6.43198428e-09],
        [-6.92746340e-08, -2.18446630e-08, -8.13619039e-08, ...,
         -1.77984045e-08, -6.81522678e-08, -2.87275341e-08],
        [-5.58610367e-08, -1.76148962e-08, -6.56078569e-08, ...,
         -1.43521115e-08, -5.49559900e-08, -2.31650432e-08]],

       [[-1.03414184e-08, -2.38561952e-08, -4.19625970e-08, ...,
         -1.85441991e-08, -2.38427145e-08, -2.57376076e-08],
        [ 1.53663073e-08,  3.54479042e-08,  6.23521927e-08, ...,
          2.75548135e-08,  3.54278740e-08,  3.82434919e-08],
        [ 2.45966021e-08,  5.67408946e-08,  9.98061580e-08, ...,
          4.41065495e-08,  5.67088350e-08,  6.12157507e-08],
        ...,
        [-2.26730901e-09, -5.23036192e-09, -9.20010823e-09, ...,
         -4.06573131e-09, -5.22740651e-09, -5.64285330e-09],
        [-1.50363799e-09, -3.46868068e-09, -6.10134387e-09, ...,
         -2.69631872e-09, -3.46672069e-09, -3.74223719e-09],
        [-5.25526289e-09, -1.21231505e-08, -2.13243929e-08, ...,
         -9.42372047e-09, -1.21163000e-08, -1.30792399e-08]],

       [[-3.51821718e-08, -1.46095033e-07, -6.23099528e-09, ...,
         -2.80888859e-08, -8.54807638e-08, -9.40197609e-08],
        [ 2.19111591e-08,  9.09867452e-08,  3.88061094e-09, ...,
          1.74935195e-08,  5.32366933e-08,  5.85547077e-08],
        [-1.05956994e-08, -4.39989627e-08, -1.87656846e-09, ...,
         -8.45943759e-09, -2.57439616e-08, -2.83156218e-08],
        ...,
        [-2.85509654e-11, -1.18558746e-10, -5.05656454e-12, ...,
         -2.27946342e-11, -6.93691701e-11, -7.62987243e-11],
        [-1.16440768e-08, -4.83523799e-08, -2.06224282e-09, ...,
         -9.29644539e-09, -2.82911632e-08, -3.11172741e-08],
        [-2.45677150e-08, -1.02018170e-07, -4.35110437e-09, ...,
         -1.96144700e-08, -5.96912244e-08, -6.56540067e-08]],

       [[-8.10783263e-09, -2.19628369e-08, -5.13717531e-08, ...,
         -2.78277379e-09, -2.97822993e-08, -3.15677724e-08],
        [ 1.68333401e-08,  4.55988598e-08,  1.06657140e-07, ...,
          5.77754644e-09,  6.18334894e-08,  6.55404619e-08],
        [ 2.82066654e-08,  7.64074031e-08,  1.78719276e-07, ...,
          9.68110481e-09,  1.03610844e-07,  1.09822409e-07],
        ...,
        [ 1.28267896e-09,  3.47457485e-09,  8.12713807e-09, ...,
          4.40241676e-10,  4.71163286e-09,  4.99409936e-09],
        [ 7.93404953e-10,  2.14920859e-09,  5.02706543e-09, ...,
          2.72312811e-10,  2.91439473e-09,  3.08911519e-09],
        [ 8.42940917e-09,  2.28339374e-08,  5.34092841e-08, ...,
          2.89314550e-09,  3.09635375e-08,  3.28198304e-08]],

       [[ 1.68797047e-08,  1.01267938e-09,  7.71280018e-09, ...,
          5.63233726e-09,  7.93342014e-09,  1.14855983e-08],
        [-4.72173625e-08, -2.83275381e-09, -2.15749072e-08, ...,
         -1.57552584e-08, -2.21920438e-08, -3.21285043e-08],
        [ 9.21601551e-09,  5.52904778e-10,  4.21105018e-09, ...,
          3.07515480e-09,  4.33150449e-09,  6.27093044e-09],
        ...,
        [ 4.72499515e-08,  2.83470913e-09,  2.15897984e-08, ...,
          1.57661333e-08,  2.22073613e-08,  3.21506803e-08],
        [-1.35296252e-09, -8.11695017e-11, -6.18205653e-10, ...,
         -4.51449905e-10, -6.35889064e-10, -9.20607590e-10],
        [ 7.78290996e-08,  4.66927164e-09,  3.55622518e-08, ...,
          2.59696336e-08,  3.65794861e-08,  5.29579047e-08]]],      dtype=float32)}, Array([[[-0.,  0.,  0., ...,  0.,  0.,  0.]],

       [[-0.,  0.,  0., ...,  0.,  0.,  0.]],

       [[-0.,  0.,  0., ...,  0.,  0.,  0.]],

       [[-0.,  0.,  0., ...,  0.,  0.,  0.]],

       [[-0.,  0.,  0., ...,  0.,  0.,  0.]],

       [[-0.,  0.,  0., ...,  0.,  0.,  0.]]], dtype=float32), Array([[[ 0., -0., -0., ...,  0., -0.,  0.]],

       [[ 0., -0., -0., ...,  0., -0., -0.]],

       [[-0.,  0.,  0., ..., -0., -0., -0.]],

       [[-0.,  0., -0., ..., -0., -0., -0.]],

       [[-0.,  0.,  0., ...,  0.,  0.,  0.]],

       [[ 0., -0.,  0., ...,  0., -0.,  0.]]], dtype=float32))
 tensor<1x288x288xf32>) -> tensor<6x288x288xf32>
Cost: 113466

Operation name: stablehlo.multiply
cache miss
2511610444
%2723 = stablehlo.multiply %1395, %1318 : tensor<288xf32>
Cost: 274

Operation name: stablehlo.reshape
cache miss
2960881163
%2724 = stablehlo.reshape %2723 : (tensor<288xf32>) -> tensor<1x288xf32>
Cost: 413

Operation name: stablehlo.concatenate
cache miss
2777730774
%2725 = stablehlo.concatenate %2655, %2668, %2681, %2694, %2707, %2724, dim = 0 : (tensor<1x288xf32>, tensor<1x288xf32>, tensor<1x288xf32>, tensor<1x288xf32>, tensor<1x288xf32>, tensor<1x288xf32>) -> tensor<6x288xf32>
Cost: 611

Operation name: stablehlo.dot_general
cache miss
1743356395
%2726 = stablehlo.dot_general %1383, %1331, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<768xf32>, tensor<288xf32>) -> tensor<768x288xf32>
Cost: 76402

Operation name: stablehlo.reshape
cache miss
1628429499
%2727 = stablehlo.reshape %2726 : (tensor<768x288xf32>) -> tensor<1x768x288xf32>
Cost: 75716

Operation name: stablehlo.concatenate
cache miss
2658928759
%2728 = stablehlo.concatenate %2657, %2670, %2683, %2696, %2709, %2727, dim = 0 : (tensor<1x768x288xf32>, tensor<1x768x288xf32>, tensor<1x768x288xf32>, tensor<1x768x288xf32>, tensor<1x768x288xf32>, tensor<1x768x288xf32>) -> tensor<6x768x288xf32>
Cost: 265104

Operation name: stablehlo.dot_general
cache miss
9312040
%2729 = stablehlo.dot_general %1370, %1345, contracting_dims = [] x [], precision = [DEFAULT, DEFAULT] : (tensor<288xf32>, tensor<768xf32>) -> tensor<288x768xf32>
Cost: 76564

Operation name: stablehlo.reshape
cache miss
3198799925
%2730 = stablehlo.reshape %2729 : (tensor<288x768xf32>) -> tensor<1x288x768xf32>
Cost: 95366

Operation name: stablehlo.concatenate
cache miss
3159063878
%2731 = stablehlo.concatenate %2659, %2672, %2685, %2698, %2711, %2730, dim = 0 : (tensor<1x288x768xf32>, tensor<1x288x768xf32>, tensor<1x288x768xf32>, tensor<1x288x768xf32>, tensor<1x288x768xf32>, tensor<1x288x768xf32>) -> tensor<6x288x768xf32>
Cost: 285734

Operation name: stablehlo.multiply
cache miss
2511610444
%2732 = stablehlo.multiply %1368, %1349 : tensor<288xf32>
Cost: 311

Operation name: func.return
Cost: 0

Operation name: func.func
Cost: 0

Operation name: builtin.module
Cost: 0

